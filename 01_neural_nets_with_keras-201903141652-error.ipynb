{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01_neural_nets_with_keras.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "qzKmBz4mRPSa",
        "tcDTlaesRPSa",
        "QFcxV-QZRPSg",
        "8SxpWWjLRPSl",
        "a7UfTAiIRPSq",
        "-echLrjrRPSw",
        "FDZdcs90RPS1",
        "tkV3R0f9RPS7",
        "MGRJAGE2RPTB",
        "NbsfVKPDRPTF",
        "NRT_prycRPTJ",
        "HkHovWq6RPTO",
        "sWTzbNH3RPTS",
        "A1Jf4x_URPTW",
        "TeCspcfPRPTc",
        "K76S__jbRPTd",
        "o826oACKRPTg",
        "qtWWGCqsRPTi",
        "y_MwSMbORPTo",
        "zjKiJ4HqRPTs",
        "f8RWZjDkRPTt",
        "_eFqYMAARPTv",
        "5_eewosXRPTw",
        "FQ75RKXbRPTy",
        "0TS0offQRPTz",
        "jmlHeqeWRPT2",
        "4WfPVC2-RPT4",
        "uDOQNtGPRPT6",
        "69UqxP0bRPT8",
        "8dXCkemVRPT-",
        "EdsgSzdSRPUA",
        "bwjFFv2kRPUE",
        "ZkC5WxjXRPUE",
        "EOGYbFicRPUI",
        "4eW2GR0dRPUJ",
        "xhuImJtfRPUJ",
        "MTbUE9DJRPUM",
        "CPv5oiYiRPUP",
        "VSHSbodvRPUS",
        "fnv07ZWURPUV",
        "Dm92YfodRPUV",
        "AKqc9QTnRPUX",
        "S33kKUQURPUb",
        "sUkhb3UHRPUb",
        "iZ3sB1IaRPUb",
        "4xtrYRwlRPUe",
        "azQG_QByRPUh",
        "OJ2r6xLNRPUm",
        "Z4pRXEx1RPUr",
        "Q2tMju8wRPUr",
        "_eV0doc1RPUu",
        "zXVlGI4BRPUx",
        "G3ebysIFRPUx",
        "spC-tgEkRPU3",
        "KXVy4tICRPU3",
        "VHNI4KtiRPU5",
        "GK5cEbagRPU8",
        "aSz_7o9uRPU_",
        "rz9MEwcYRPVB",
        "YxWcFsviRPVD",
        "ZPiVESaMRPVK",
        "aBczbliMRPVL",
        "EgrIJrLSRPVM",
        "HbAPUh1_RPVM",
        "dXZFmIpiRPVO",
        "wccJZgORRPVU",
        "AsmW8IYARPVV",
        "c4kqwt-JRPVW",
        "ILFVy2lSRPVW",
        "pZChwbg_RPVY",
        "uUXS2YMcRPVa",
        "NTdf8BRwRPVc",
        "gHHA0Z1SRPVe",
        "fMFQ195BRPVe",
        "MNI_cZCiRPVh",
        "bG44K4tIRPVk",
        "a_Ae7zSYRPVr",
        "-Mnbyys4RPVw",
        "0qIsJPchRPVz",
        "RnBYQTinRPV0",
        "05BPVXNSRPV1",
        "9aXdqod3RPV3",
        "axwsQ3dPRPV4",
        "sNU-eAdlRPV6",
        "tu9qE9Q7RPV6",
        "XIyRv0_zRPV6",
        "8excpNofRPV7",
        "aIVzBXYjRPV8",
        "5FvV9Un-RPV8"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matchbou/Tensorflow20-20190314/blob/master/01_neural_nets_with_keras-201903141652-error.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "collapsed": true,
        "id": "eWbOASpRRPRu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Neural Nets with Keras"
      ]
    },
    {
      "metadata": {
        "id": "YF9aOsi5RPRw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this notebook you will learn how to implement neural networks using the Keras API. We will use TensorFlow's own implementation, *tf.keras*, which comes bundled with TensorFlow."
      ]
    },
    {
      "metadata": {
        "id": "qyQQijuVRPRx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Don't hesitate to look at the documentation at [keras.io](https://keras.io/). All the code examples should work fine with tf.keras, the only difference is how to import Keras:\n",
        "\n",
        "```python\n",
        "# keras.io code:\n",
        "from keras.layers import Dense\n",
        "output_layer = Dense(10)\n",
        "\n",
        "# corresponding tf.keras code:\n",
        "from tensorflow import keras\n",
        "Dense = keras.layers.Dense\n",
        "output_layer = Dense(10)\n",
        "\n",
        "# or simply:\n",
        "from tensorflow import keras\n",
        "output_layer = keras.layers.Dense(10)\n",
        "```\n",
        "\n",
        "In this notebook, we will not use any TensorFlow-specific code, so everything you see would run just the same way on [keras-team](https://github.com/keras-team/keras) or any other Python implementation of the Keras API (except for the imports)."
      ]
    },
    {
      "metadata": {
        "id": "7xKDGMWdRPRx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ]
    },
    {
      "metadata": {
        "id": "lxj0LL4OWwHy",
        "colab_type": "code",
        "outputId": "5c52ed23-0709-46df-c198-243da28930b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install  tf-nightly-2.0-preview"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tf-nightly-2.0-preview\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/55/c0e4464689cd80fd662339d00b039542383240ea8c005c6ca6272afd7b02/tf_nightly_2.0_preview-2.0.0.dev20190313-cp36-cp36m-manylinux1_x86_64.whl (80.1MB)\n",
            "\u001b[K    100% |████████████████████████████████| 80.1MB 402kB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.0.9)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (3.6.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.11.0)\n",
            "Collecting tb-nightly<1.15.0a0,>=1.14.0a0 (from tf-nightly-2.0-preview)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/ed/81c0a17476a99b81477e3308adaa6129e055ad4a6c35b268cc8b01e54686/tb_nightly-1.14.0a20190313-py3-none-any.whl (3.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 3.0MB 8.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (0.7.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.1.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (0.2.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (0.33.1)\n",
            "Collecting tensorflow-estimator-2.0-preview (from tf-nightly-2.0-preview)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/09/3525d25dba3b8424975ecf3764aca3ac0c27b394bfcf6cc2f61727947511/tensorflow_estimator_2.0_preview-1.14.0.dev2019031300-py2.py3-none-any.whl (351kB)\n",
            "\u001b[K    100% |████████████████████████████████| 358kB 19.2MB/s \n",
            "\u001b[?25hCollecting google-pasta>=0.1.2 (from tf-nightly-2.0-preview)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8c/96/adbd4eafe72ce9b5ca6f168fbf109386e1b601f7c59926a11e9d7b7a5b44/google_pasta-0.1.4-py3-none-any.whl (51kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 15.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (0.7.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.0.7)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.14.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tf-nightly-2.0-preview) (40.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.15.0a0,>=1.14.0a0->tf-nightly-2.0-preview) (0.14.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.15.0a0,>=1.14.0a0->tf-nightly-2.0-preview) (3.0.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tf-nightly-2.0-preview) (2.8.0)\n",
            "Installing collected packages: tb-nightly, tensorflow-estimator-2.0-preview, google-pasta, tf-nightly-2.0-preview\n",
            "Successfully installed google-pasta-0.1.4 tb-nightly-1.14.0a20190313 tensorflow-estimator-2.0-preview-1.14.0.dev2019031300 tf-nightly-2.0-preview-2.0.0.dev20190313\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tc1lT7E0RPRy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZrLAscbXRPR1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xVt5CY4YRPR3",
        "colab_type": "code",
        "outputId": "62757eef-bff6-43b1-b96c-4a89fa2073e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"python\", sys.version)\n",
        "for module in mpl, np, pd, sklearn, tf, keras:\n",
        "    print(module.__name__, module.__version__)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "python 3.6.7 (default, Oct 22 2018, 11:32:17) \n",
            "[GCC 8.2.0]\n",
            "matplotlib 3.0.3\n",
            "numpy 1.14.6\n",
            "pandas 0.22.0\n",
            "sklearn 0.20.3\n",
            "tensorflow 2.0.0-dev20190313\n",
            "tensorflow.python.keras.api._v2.keras 2.2.4-tf\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "b3F0GFvyRPR5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "assert sys.version_info >= (3, 5) # Python ≥3.5 required\n",
        "assert tf.__version__ >= \"2.0\"    # TensorFlow ≥2.0 required"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NtVjR7Q3RPR6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Note**: The preview version of TensorFlow 2.0 shows up as version 1.13. That's okay. To test that this behaves like TF 2.0, we verify that `tf.function()` is present."
      ]
    },
    {
      "metadata": {
        "id": "4A1U5LcXRPR7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![Exercise](https://c1.staticflickr.com/9/8101/8553474140_c50cf08708_b.jpg)"
      ]
    },
    {
      "metadata": {
        "id": "SKkIOGI_RPR8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Exercise 1 – TensorFlow Playground"
      ]
    },
    {
      "metadata": {
        "id": "IuvKcfFIXY1u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "z5qKs0KnRPR8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Visit the [TensorFlow Playground](http://playground.tensorflow.org).\n",
        "* **Layers and patterns**: try training the default neural network by clicking the \"Run\" button (top left). Notice how it quickly finds a good solution for the classification task. Notice that the neurons in the first hidden layer have learned simple patterns, while the neurons in the second hidden layer have learned to combine the simple patterns of the first hidden layer into more complex patterns). In general, the more layers, the more complex the patterns can be.\n",
        "* **Activation function**: try replacing the Tanh activation function with the ReLU activation function, and train the network again. Notice that it finds a solution even faster, but this time the boundaries are linear. This is due to the shape of the ReLU function.\n",
        "* **Local minima**: modify the network architecture to have just one hidden layer with three neurons. Train it multiple times (to reset the network weights, just add and remove a neuron). Notice that the training time varies a lot, and sometimes it even gets stuck in a local minimum.\n",
        "* **Too small**: now remove one neuron to keep just 2. Notice that the neural network is now incapable of finding a good solution, even if you try multiple times. The model has too few parameters and it systematically underfits the training set.\n",
        "* **Large enough**: next, set the number of neurons to 8 and train the network several times. Notice that it is now consistently fast and never gets stuck. This highlights an important finding in neural network theory: large neural networks almost never get stuck in local minima, and even when they do these local optima are almost as good as the global optimum. However, they can still get stuck on long plateaus for a long time.\n",
        "* **Deep net and vanishing gradients**: now change the dataset to be the spiral (bottom right dataset under \"DATA\"). Change the network architecture to have 4 hidden layers with 8 neurons each. Notice that training takes much longer, and often gets stuck on plateaus for long periods of time. Also notice that the neurons in the highest layers (i.e. on the right) tend to evolve faster than the neurons in the lowest layers (i.e. on the left). This problem, called the \"vanishing gradients\" problem, can be alleviated using better weight initialization and other techniques, better optimizers (such as AdaGrad or Adam), or using Batch Normalization.\n",
        "* **More**: go ahead and play with the other parameters to get a feel of what they do. In fact, after this course you should definitely play with this UI for at least one hour, it will grow your intuitions about neural networks significantly."
      ]
    },
    {
      "metadata": {
        "id": "RQQMa8oHRPR9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![Exercise](https://c1.staticflickr.com/9/8101/8553474140_c50cf08708_b.jpg)"
      ]
    },
    {
      "metadata": {
        "id": "1fgfre8-RPR9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Exercise 2 – Image classification with tf.keras"
      ]
    },
    {
      "metadata": {
        "id": "HqgzqeaSRPR-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Load the Fashion MNIST dataset"
      ]
    },
    {
      "metadata": {
        "id": "86klRAC7RPR-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's start by loading the fashion MNIST dataset. Keras has a number of functions to load popular datasets in `keras.datasets`. The dataset is already split for you between a training set and a test set, but it can be useful to split the training set further to have a validation set:"
      ]
    },
    {
      "metadata": {
        "id": "DQ8mYh7DRPR_",
        "colab_type": "code",
        "outputId": "523ffd25-cbd2-48c4-8345-e275b33fb27f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        }
      },
      "cell_type": "code",
      "source": [
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\n",
        "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
        "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CTHm6yUsRPSB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The training set contains 55,000 grayscale images, each 28x28 pixels:"
      ]
    },
    {
      "metadata": {
        "id": "afu-ojQdRPSC",
        "colab_type": "code",
        "outputId": "3811464b-f1d1-4d74-8c7f-1735bdae4748",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(55000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "ItobIk2sRPSE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Each pixel intensity is represented by a uint8 (byte) from 0 to 255:"
      ]
    },
    {
      "metadata": {
        "id": "t75XpEk_RPSE",
        "colab_type": "code",
        "outputId": "9b8c6956-457a-46e9-9452-5a0447835521",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1382
        }
      },
      "cell_type": "code",
      "source": [
        "X_train[0]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   1,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  44, 127,\n",
              "        182, 185, 161, 120,  55,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  42, 198, 251, 255,\n",
              "        251, 249, 247, 255, 252, 214, 100,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   2,   0,   0, 233, 252, 237, 239,\n",
              "        234, 237, 235, 237, 237, 254, 227,   0,   0,   0,   0,   1,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   2,   0,   0,  16, 210, 225, 215, 175,\n",
              "        217, 216, 193, 196, 226, 221, 209,  50,   0,   0,   2,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   2,   0,   0, 199, 229, 232, 230, 245, 204,\n",
              "        219, 253, 245, 207, 194, 223, 231, 236, 235,   0,   0,   3,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   1,   0, 137, 235, 204, 209, 201, 209, 234,\n",
              "        190, 234, 218, 215, 238, 239, 204, 189, 224, 154,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0, 194, 201, 200, 209, 202, 193, 205,\n",
              "        194, 183, 218, 231, 197, 172, 181, 193, 205, 199,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   3, 212, 203, 188, 189, 196, 198, 198,\n",
              "        201, 196, 217, 179, 167, 183, 217, 197, 202, 219,  30,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,  34, 225, 200, 194, 190, 188, 192, 196,\n",
              "        192, 170, 202, 190, 201, 195, 200, 201, 209, 227,  50,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,  68, 225, 210, 211, 198, 192, 196, 204,\n",
              "        196, 181, 212, 197, 195, 192, 206, 220, 210, 229,  93,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0, 111, 223, 227, 253, 209, 196, 204, 211,\n",
              "        206, 183, 216, 206, 210, 203, 215, 244, 224, 227, 150,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0, 139, 225, 224, 255, 202, 206, 212, 209,\n",
              "        211, 190, 213, 202, 207, 206, 222, 255, 230, 220, 190,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0, 180, 226, 224, 255, 199, 204, 207, 214,\n",
              "        214, 190, 216, 206, 203, 205, 219, 243, 224, 214, 234,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0, 225, 223, 228, 254, 209, 206, 208, 213,\n",
              "        210, 191, 215, 207, 204, 208, 211, 249, 226, 214, 255,  38,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0, 250, 232, 240, 239, 211, 203, 209, 205,\n",
              "        211, 197, 215, 208, 208, 214, 213, 239, 231, 219, 255,  81,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0, 248, 236, 247, 240, 203, 200, 208, 206,\n",
              "        214, 193, 213, 212, 208, 212, 211, 243, 242, 225, 254,  66,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0, 247, 230, 252, 226, 199, 211, 202, 211,\n",
              "        213, 182, 213, 212, 206, 202, 219, 207, 247, 222, 237, 104,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,  10, 244, 219, 250, 205, 199, 209, 202, 209,\n",
              "        211, 189, 214, 206, 210, 200, 212, 154, 240, 208, 219, 140,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,  21, 255, 222, 238, 184, 210, 192, 206, 209,\n",
              "        210, 189, 213, 211, 209, 192, 228, 155, 226, 238, 241, 166,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,  37, 245, 226, 241, 150, 197, 189, 204, 209,\n",
              "        210, 183, 213, 213, 201, 184, 215, 146, 216, 236, 225, 154,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,  58, 239, 227, 255, 158, 193, 195, 204, 209,\n",
              "        213, 180, 207, 217, 199, 194, 211, 158, 219, 236, 216, 151,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,  68, 233, 226, 243, 139, 200, 193, 205, 210,\n",
              "        208, 180, 205, 212, 203, 196, 216, 157, 179, 255, 216, 155,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,  81, 225, 224, 211, 138, 219, 185, 201, 213,\n",
              "        207, 197, 226, 212, 200, 190, 215, 183,  90, 255, 211, 147,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,  91, 210, 230, 158, 114, 205, 187, 208, 209,\n",
              "        206, 193, 210, 211, 204, 195, 204, 181,  23, 255, 213, 158,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,  87, 205, 232, 109, 164, 255, 214, 224, 222,\n",
              "        210, 197, 214, 225, 222, 211, 220, 217,   0, 234, 216, 169,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,  92, 213, 232, 146,   5, 134, 151, 162, 170,\n",
              "        183, 182, 164, 166, 178, 162, 156,  98,   0, 240, 225, 210,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,  43, 164, 206, 141,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0, 127, 125,  76,   0,\n",
              "          0,   0]], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "MBUs51RARPSG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You can plot an image using Matplotlib's `imshow()` function, with a `'binary'`\n",
        " color map:"
      ]
    },
    {
      "metadata": {
        "id": "nduLdVaSRPSG",
        "colab_type": "code",
        "outputId": "f6c0e995-7585-483e-f353-cb7ed34effce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "cell_type": "code",
      "source": [
        "plt.imshow(X_train[0], cmap=\"binary\")\n",
        "plt.show()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAD4CAYAAADFJPs2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "NPXbQQXSRPSI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The labels are the class IDs (represented as uint8), from 0 to 9:"
      ]
    },
    {
      "metadata": {
        "id": "PmB-jdgfRPSJ",
        "colab_type": "code",
        "outputId": "d6062b8f-5fda-4e6c-8330-7bda4264d30a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([4, 0, 7, ..., 3, 0, 5], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "0h1CYycgRPSK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here are the corresponding class names:"
      ]
    },
    {
      "metadata": {
        "id": "LfS5xtpsRPSL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
        "               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Cpdrk9MPRPSN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "So the first image in the training set is a coat:"
      ]
    },
    {
      "metadata": {
        "id": "Dj2BZMiiRPSN",
        "colab_type": "code",
        "outputId": "df8e81d4-f823-4cd2-997c-9c7311d0d7dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "cell_type": "code",
      "source": [
        "class_names[y_train[0]]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Coat'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "7yAt34NHRPSQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The validation set contains 5,000 images, and the test set contains 10,000 images:"
      ]
    },
    {
      "metadata": {
        "id": "Vuqgx9BhRPSR",
        "colab_type": "code",
        "outputId": "5a3bc173-82b5-4f8f-a34f-ae16ec30373e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "cell_type": "code",
      "source": [
        "X_valid.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "Dh8llst5RPSU",
        "colab_type": "code",
        "outputId": "4e7af5b8-af90-4251-b765-6ce0e354a1ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "cell_type": "code",
      "source": [
        "X_test.shape"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "kX8157fBRPSX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's take a look at a sample of the images in the dataset:"
      ]
    },
    {
      "metadata": {
        "id": "5rd5-6bwRPSY",
        "colab_type": "code",
        "outputId": "6bb4734a-2311-45ae-f4f0-79c678249b24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        }
      },
      "cell_type": "code",
      "source": [
        "n_rows = 5\n",
        "n_cols = 10\n",
        "plt.figure(figsize=(n_cols*1.4, n_rows * 1.6))\n",
        "for row in range(n_rows):\n",
        "    for col in range(n_cols):\n",
        "        index = n_cols * row + col\n",
        "        plt.subplot(n_rows, n_cols, index + 1)\n",
        "        plt.imshow(X_train[index], cmap=\"binary\", interpolation=\"nearest\")\n",
        "        plt.axis('off')\n",
        "        plt.title(class_names[y_train[index]])\n",
        "plt.show()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAy0AAAHYCAYAAABA0AeFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEARBEASN5v8DiZjCbkLUQpUAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 1008x576 with 50 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "DIptZvjbRPSZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This dataset has the same structure as the famous MNIST dataset (which you can load using `keras.datasets.mnist.load_data()`), except the images represent fashion items rather than handwritten digits, and it is much more challenging. A simple linear model can reach 92% accuracy on MNIST, but only 83% on fashion MNIST."
      ]
    },
    {
      "metadata": {
        "id": "qzKmBz4mRPSa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Build a classification neural network with Keras"
      ]
    },
    {
      "metadata": {
        "id": "tcDTlaesRPSa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.1)\n",
        "Build a `Sequential` model (`keras.models.Sequential`), without any argument, then and add four layers to it by calling its `add()` method:\n",
        "  * a `Flatten` layer (`keras.layers.Flatten`) to convert each 28x28 image to a single row of 784 pixel values. Since it is the first layer in your model, you should specify the `input_shape` argument, leaving out the batch size: `[28, 28]`.\n",
        "  * a `Dense` layer  (`keras.layers.Dense`) with 300 neurons (aka units), and the `\"relu\"` activation function.\n",
        "  * Another `Dense` layer with 100 neurons, also with the `\"relu\"` activation function.\n",
        "  * A final `Dense` layer with 10 neurons (one per class), and with the `\"softmax\"` activation function to ensure that the sum of all the estimated class probabilities for each image is equal to 1."
      ]
    },
    {
      "metadata": {
        "id": "wIUNCrkgRPSb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hYKHUuliRPSc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9cnmBhEcRPSe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QFcxV-QZRPSg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.2)\n",
        "Alternatively, you can pass a list containing the 4 layers to the constructor of the `Sequential` model. The model's `layers` attribute holds the list of layers."
      ]
    },
    {
      "metadata": {
        "id": "XDC2igFJRPSh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2FWvItkERPSi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DFQzJibCRPSj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8SxpWWjLRPSl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.3)\n",
        "Call the model's `summary()` method and examine the output. Also, try using `keras.utils.plot_model()` to save an image of your model's architecture. Alternatively, you can uncomment the following code to display the image within Jupyter."
      ]
    },
    {
      "metadata": {
        "id": "T9fzaSIpRPSm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Warning**: you will need `pydot` and `graphviz` to use `plot_model()`."
      ]
    },
    {
      "metadata": {
        "id": "J48OIWRjRPSm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TFZquj4rRPSn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zm69zOOBRPSp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a7UfTAiIRPSq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.4)\n",
        "After a model is created, you must call its `compile()` method to specify the `loss` function and the `optimizer` to use. In this case, you want to use the `\"sparse_categorical_crossentropy\"` loss, and the `\"sgd\"` optimizer (stochastic gradient descent). Moreover, you can optionally specify a list of additional metrics that should be measured during training. In this case you should specify `metrics=[\"accuracy\"]`. **Note**: you can find more loss functions in `keras.losses`, more metrics in `keras.metrics` and more optimizers in `keras.optimizers`."
      ]
    },
    {
      "metadata": {
        "id": "GG3bHksRRPSs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cRMMrcFKRPSt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y5VWofYnRPSv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-echLrjrRPSw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.5)\n",
        "Now your model is ready to be trained. Call its `fit()` method, passing it the input features (`X_train`) and the target classes (`y_train`). Set `epochs=10` (or else it will just run for a single epoch). You can also (optionally) pass the validation data by setting `validation_data=(X_valid, y_valid)`. If you do, Keras will compute the loss and the additional metrics (the accuracy in this case) on the validation set at the end of each epoch. If the performance on the training set is much better than on the validation set, your model is probably overfitting the training set (or there is a bug, such as a mismatch between the training set and the validation set).\n",
        "**Note**: the `fit()` method will return a `History` object containing training stats. Make sure to preserve it (`history = model.fit(...)`)."
      ]
    },
    {
      "metadata": {
        "id": "ncCeB44fRPSx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KlqpOpiMRPSy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0Lh_lHCjRPSz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FDZdcs90RPS1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.6)\n",
        "Try running `pd.DataFrame(history.history).plot()` to plot the learning curves. To make the graph more readable, you can also set `figsize=(8, 5)`, call `plt.grid(True)` and `plt.gca().set_ylim(0, 1)`."
      ]
    },
    {
      "metadata": {
        "id": "OrG-yjABRPS1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zM_FrNc7RPS3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IogvGR1SRPS6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tkV3R0f9RPS7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.7)\n",
        "Try running `model.fit()` again, and notice that training continues where it left off."
      ]
    },
    {
      "metadata": {
        "id": "nn2w-lAxRPS7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C7hvYgxJRPS9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WalhTz2MRPS_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MGRJAGE2RPTB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.8)\n",
        "call the model's `evaluate()` method, passing it the test set (`X_test` and `y_test`). This will compute the loss (cross-entropy) on the test set, as well as all the additional metrics (in this case, the accuracy). Your model should achieve over 80% accuracy on the test set."
      ]
    },
    {
      "metadata": {
        "id": "aBs9kJxVRPTB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nAXG0qEsRPTD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yrhlOckhRPTE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NbsfVKPDRPTF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.9)\n",
        "Define `X_new` as the first 10 instances of the test set. Call the model's `predict()` method to estimate the probability of each class for each instance (for better readability, you may use the output array's `round()` method):"
      ]
    },
    {
      "metadata": {
        "id": "xQmCnrYtRPTF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0CZJEsnGRPTG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WFu_dp1gRPTI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NRT_prycRPTJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.10)\n",
        "Often, you may only be interested in the most likely class. Use `np.argmax()` to get the class ID of the most likely class for each instance. **Tip**: you want to set `axis=1`."
      ]
    },
    {
      "metadata": {
        "id": "vsCR5ZQGRPTJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mZs-N2WMRPTK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cB516E6xRPTM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HkHovWq6RPTO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.11)\n",
        "Call the model's `predict_classes()` method for `X_new`. You should get the same result as above."
      ]
    },
    {
      "metadata": {
        "id": "XFnGkW8qRPTO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R7LeSftCRPTQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uudcmXWrRPTR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sWTzbNH3RPTS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.12)\n",
        "(Optional) It is often useful to know how confident the model is for each prediction. Try finding the estimated probability for each predicted class using `np.max()`."
      ]
    },
    {
      "metadata": {
        "id": "h4niYJ84RPTS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "P4wprs1tRPTU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_OVL1CvURPTV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A1Jf4x_URPTW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.13)\n",
        "(Optional) It is frequent to want the top k classes and their estimated probabilities rather just the most likely class. You can use `np.argsort()` for this."
      ]
    },
    {
      "metadata": {
        "id": "iaPdI7BsRPTW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r6crm07yRPTZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h3x_UyTnRPTb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H5wrnJsSRPTc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![Exercise solution](https://camo.githubusercontent.com/250388fde3fac9135ead9471733ee28e049f7a37/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f302f30362f46696c6f735f736567756e646f5f6c6f676f5f253238666c69707065642532392e6a7067)"
      ]
    },
    {
      "metadata": {
        "id": "TeCspcfPRPTc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Exercise 2 - Solution"
      ]
    },
    {
      "metadata": {
        "id": "K76S__jbRPTd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.1)\n",
        "Build a `Sequential` model (`keras.models.Sequential`), without any argument, then and add four layers to it by calling its `add()` method:\n",
        "  * a `Flatten` layer (`keras.layers.Flatten`) to convert each 28x28 image to a single row of 784 pixel values. Since it is the first layer in your model, you should specify the `input_shape` argument, leaving out the batch size: `[28, 28]`.\n",
        "  * a `Dense` layer  (`keras.layers.Dense`) with 300 neurons (aka units), and the `\"relu\"` activation function.\n",
        "  * Another `Dense` layer with 100 neurons, also with the `\"relu\"` activation function.\n",
        "  * A final `Dense` layer with 10 neurons (one per class), and with the `\"softmax\"` activation function to ensure that the sum of all the estimated class probabilities for each image is equal to 1."
      ]
    },
    {
      "metadata": {
        "id": "mYlZaehTRPTd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
        "model.add(keras.layers.Dense(300, activation=\"relu\"))\n",
        "model.add(keras.layers.Dense(100, activation=\"relu\"))\n",
        "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o826oACKRPTg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.2)\n",
        "Alternatively, you can pass a list containing the 4 layers to the constructor of the `Sequential` model. The model's `layers` attribute holds the list of layers."
      ]
    },
    {
      "metadata": {
        "id": "PA_-sdlhRPTg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.Dense(300, activation=\"relu\"),\n",
        "    keras.layers.Dense(100, activation=\"relu\"),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J6hLV-iuRPTh",
        "colab_type": "code",
        "outputId": "612ace95-74c2-4ae0-bdc5-5b7868f8dc91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        }
      },
      "cell_type": "code",
      "source": [
        "model.layers"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tensorflow.python.keras.layers.core.Flatten at 0x7f0f76fdd128>,\n",
              " <tensorflow.python.keras.layers.core.Dense at 0x7f0f76fdda90>,\n",
              " <tensorflow.python.keras.layers.core.Dense at 0x7f0f73c939b0>,\n",
              " <tensorflow.python.keras.layers.core.Dense at 0x7f0f73c3a320>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "id": "qtWWGCqsRPTi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.3)\n",
        "Call the model's `summary()` method and examine the output. Also, try using `keras.utils.plot_model()` to save an image of your model's architecture. Alternatively, you can uncomment the following code to display the image within Jupyter."
      ]
    },
    {
      "metadata": {
        "id": "2aRb4-RZRPTj",
        "colab_type": "code",
        "outputId": "de9a97ca-b83b-42e4-e676-614924ce5ba9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        }
      },
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_1 (Flatten)          (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 300)               235500    \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 100)               30100     \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 10)                1010      \n",
            "=================================================================\n",
            "Total params: 266,610\n",
            "Trainable params: 266,610\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nif3rcLaRPTj",
        "colab_type": "code",
        "outputId": "769a789a-215c-4cbd-d724-cb4f2f635796",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 533
        }
      },
      "cell_type": "code",
      "source": [
        "keras.utils.plot_model(model, \"my_mnist_model.png\", show_shapes=True)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdcAAAIECAYAAABLxmTdAAAABmJLR0QA/wD/AP+gvaeTAAAgAElE\nQVR4nOzde1RTZ9Y/8G+AQAATLioX8QZBVBTrWHl/QsvgpWNVRlGBitVerK2orYi3KuCtiCjVIguV\ncUTLmldtFdSCY6XOaIc6jNZlRxmVvrV4BxURud8EYf/+cJIaEzGBAwlxf9ZirfY5zznPPuck2ebk\nPGeLiIjAGGOMMaGkm+g7AsYYY8zYcHJljDHGBMbJlTHGGBMYJ1fGGGNMYGbPNpw5cwYJCQn6iIUx\nxhjrdNLT09Xa1L65FhQU4ODBgx0SEGOs9X788Uf8+OOP+g6jUyksLOTPNyaYll5Pat9cFTRlYsaY\n4QgJCQHA71VdpKWlYdq0aXzMmCAUrydN+DdXxhhjTGCcXBljjDGBcXJljDHGBMbJlTHGGBMYJ1fG\nGGNMYJxcGXvJHTt2DDY2NvjrX/+q71AM0ty5cyESiZR/M2fOVOtz4sQJREZG4tChQ3Bzc1P2feed\nd9T6jh07FlKpFKamphg0aBDOnz/fEbvRajExMfD09IRMJoOFhQXc3d3x6aeforq6Wq3vV199BW9v\nb0ilUvTp0wezZs1CUVGR3sc9cuQI4uPj0dTUpLJeRkaGyrnt1q1bq2LViJ5x4MAB0tDMGDMwwcHB\nFBwc3ObtHD16lGQyGR05ckSAqAxbaz7fwsLCyN7enrKysujKlStUX1+vsnz16tU0ceJEqqysVLbJ\n5XLq2rUrAaCjR4+qbTMrK4sCAwNbtxMdzN/fn7Zv304PHz6kyspKOnDgAInFYho3bpxKv/379xMA\nio+Pp/Lycrpw4QK5ubnR0KFDqbGxUe/jJiYmkr+/P5WVlSnbmpubqbCwkE6dOkUTJkygrl276hRj\nC6+nNE6ujHVSQiVXQ1JbW0s+Pj7ttv3WJlcXFxeNyzZs2EAeHh5UV1en0i6Xy2nfvn1kYmJCLi4u\nVF5errK8MyXXgIAAevz4sUrbW2+9RQDo9u3byrZRo0ZRjx49qLm5Wdm2bds2AkA5OTkGMW54eDj5\n+PhoTPYLFy4UNLnyZWHGmMHYvXs3iouL9R2GVq5evYpVq1bhs88+g0QiUVvu6+uLiIgI3LlzB0uX\nLtVDhMI4evQoTE1NVdoUl09ra2uVbQUFBXB2doZIJFK29erVCwBw69Ytgxh37dq1yM3NRWJios7x\n6IqTK2MvsZycHPTu3RsikQjbtm0DACQnJ8Pa2hpWVlbIzMzE+PHjIZPJ0LNnT3z99dfKdZOSkiCR\nSODg4IC5c+fC2dkZEokEvr6+OHv2rLJfeHg4zM3N4eTkpGz7+OOPYW1tDZFIhJKSEgBAREQElixZ\ngmvXrkEkEsHd3R0A8N1330Emk2H9+vUdcUi0lpSUBCLCpEmTntsnNjYWHh4e2LVrF06cONHi9ogI\nCQkJGDhwICwsLGBnZ4fJkyfjl19+UfbR9twAQFNTE1avXo3evXvD0tISQ4YMwYEDB9q20/91584d\nWFpawtXVVdnm5uam9g8jxe+ebm5uBjGunZ0d/P39kZiYCCISJKbn0uFrLmPMgAh1WbigoIAA0Nat\nW5Vt0dHRBIBOnjxJFRUVVFxcTH5+fmRtbU0NDQ3KfmFhYWRtbU0///wz1dfXU15eHnl7e5NUKlW5\ndDdjxgxydHRUGXfTpk0EgB48eKBsCwoKIrlcrtLv6NGjJJVKKSYmps37KuRlYTc3N/L09NS4jlwu\npxs3bhAR0enTp8nExIT69u1L1dXVRKT5svDq1avJ3Nyc9uzZQ+Xl5XTx4kUaNmwYdevWjYqKipT9\ntD03S5cuJQsLCzp48CCVlZVRVFQUmZiY0Llz53Ta/2fV1NSQVCql8PBwlfbs7GwSi8WUlJRElZWV\ndPnyZRo4cCC9+eabbRpP6HEjIyMJAF24cEGlnS8LM8Y6jK+vL2QyGbp3747Q0FDU1NTg9u3bKn3M\nzMyU37Y8PT2RnJyMqqoqpKamChJDQEAAKisrsWrVKkG2J4SamhrcuHEDcrn8hX19fHywaNEi3Lx5\nEytWrNDYp66uDgkJCZg6dSpmzpwJGxsbeHl5YceOHSgpKcHOnTvV1mnp3NTX1yM5ORlTpkxBUFAQ\nbG1tsXLlSojF4jafl7i4ODg7OyM2Nlal3d/fH8uXL0d4eDhkMhkGDx6Mqqoq7Nq1q03jCT1uv379\nAACXLl0SJK7n4eTKGNOKubk5AKCxsbHFfsOHD4eVlZXK5UxjU1xcDCKClZWVVv1jY2PRv39/bN++\nHTk5OWrL8/LyUF1djeHDh6u0e3t7w9zcXOUyuybPnpsrV66gtrYWgwcPVvaxtLSEk5NTm87L4cOH\nkZaWhuPHj0Mqlaosi46Oxs6dO3Hy5ElUV1fj+vXr8PX1hY+PDwoKClo9ptDjKs7Z/fv32xTTi3By\nZYwJzsLCAg8ePNB3GO2mvr4ewJP91IZEIkFqaipEIhE++OAD1NXVqSwvLy8HAHTp0kVtXVtbW1RV\nVekUX01NDQBg5cqVKvM4b926pXIzkC7279+PjRs3Ijs7G3379lVZdu/ePcTHx2POnDkYPXo0rK2t\n4erqipSUFNy9exebNm1q1ZjtMa6lpSWA385he+HkyhgTVGNjI8rLy9GzZ099h9JuFB/Qzz6UoCU+\nPj5YvHgx8vPzsW7dOpVltra2AKAxibbmWHbv3h0AsGXLFhCRyt+ZM2d02hYAbN26FXv37sX333+P\nHj16qC3Pz89HU1OT2jKZTAZ7e3vk5eXpPGZ7jdvQ0ADgt3PYXp5bz5UxxlojOzsbRIQRI0Yo28zM\nzF54ObkzcXBwgEgkQkVFhU7rrVu3DkePHsWFCxfQu3dvZfvgwYPRpUsX/PTTTyr9z549i4aGBrz6\n6qs6jdOrVy9IJBLk5ubqtN6ziAgrVqxAWVkZMjIyYGamOWUokv+9e/dU2quqqlBaWqqcGmMI4yrO\nmaOjo04x6Yq/uTLG2qS5uRllZWV4/PgxLl68iIiICPTu3Rvvv/++so+7uztKS0uRkZGBxsZGPHjw\nQOPcR3t7e9y9exc3b95EVVUVGhsbkZWVZXBTcaysrODm5obCwkKd1lNcHn52/qZEIsGSJUtw+PBh\n7N27F5WVlbh06RLmzZsHZ2dnhIWF6TzOrFmz8PXXXyM5ORmVlZVoampCYWGhMhGFhobC0dGxxccv\n/vzzz/j888+RkpICsViscolZJBJh8+bNAABXV1eMGjUKKSkpOHXqFOrq6lBQUKCMe/bs2cpt6mtc\nBcU58/Ly0uWQ6oyTK2MvsW3btsHb2xsAsHz5cgQGBiI5ORlbtmwBAAwZMgTXr19HSkoKlixZAgAY\nN24c8vPzlduor6+Hl5cXLC0t4efnBw8PD/zjH/9Q+T1y/vz5GDVqFKZPn47+/ftj3bp1ystyT994\nMm/ePDg4OMDT0xMTJkxAaWlphxyH1ggICEBeXp7K76fffPMN3N3dce3aNXh7e2PBggVq640YMQKL\nFy9Wa1+zZg3i4uIQExODbt26wd/fH3379kV2djasra0BQKdzk5iYiEWLFiE+Ph5du3aFs7MzIiIi\nUFZWBuDJ5dHi4mJkZmY+dx9Jy7mgIpEI6enpCA0NxezZs2FnZwdPT0/cvn0bhw4dgp+fn7KvvsZV\nOHfuHFxcXDBkyBCtxmg1HebtMMYMiCE8/lDx3N3OQsh5rvn5+WRmZkZ79uwRKrwO1dTURH5+frR7\n9+6XYlwiopKSEpJIJLR582a1ZTzPlTFmUHS5qaezqqurw/Hjx5Gfn6+8Icbd3R0xMTGIiYnRWKnF\nkDU1NSEjIwNVVVUIDQ01+nEV1q5di6FDhyI8PBzAk2/Id+/eRU5ODq5evSroWJxcGWPsBUpLSzFu\n3Dh4eHjggw8+ULZHRkYiJCQEoaGhOt/cpE/Z2dk4dOgQsrKytJ6r25nHBYCEhATk5ubi2LFjEIvF\nAIDMzEy4uLjAz88P3377raDjCZZcHz16hIULF8LJyQlWVlZ44403lHfU7dixQ6hh9K65uRlbtmyB\nr69vm7ZjDDU0f/zxRwwcOBAmJiYQiURwdHRUe3qKvj1bX9PJyUljPU6mu6ioKKSmpqKiogKurq44\nePCgvkNqFzt27FCZyrJ3716V5evXr0d4eDg2bNigpwh1N2bMGOzbt0/lec/GPG5mZiYePXqE7Oxs\n2NnZKdsnT56scm4Vz7kWgmBTcb744gt89913+OWXX5CWlgZ7e3sMHTpU+agpY5Cfn49Zs2bhX//6\nF1555ZU2bYva+6HRHWDEiBH4v//7P4wbNw7Hjx/HlStXlPP1DEVQUBCCgoLg7u6OkpKSVhduZuri\n4uIQFxen7zAMwtixYzF27Fh9h8GeIzAwEIGBgR06pmDfXDMyMjB8+HDY2tpizpw5CA4ObtV26urq\n1L4VamrraP/5z3+wYsUKzJs3D0OHDm3z9gICAlBRUYGJEycKEF3bGMLxFYox7QtjrPMSLLkWFhYq\nr2O3haZ6joZQ4/GVV17BoUOHMGPGDK0fedZZGMLxFYox7QtjrPNqc3L9+9//Dnd3d9y7dw9/+ctf\nIBKJND4fU+Gf//wnPD09YWNjA4lEAi8vLxw/fhyA5nqOz6vx2FKtQl1qHuqDsdfQNLR90VVLr9EP\nP/xQ+futXC7HhQsXAACzZs2ClZUVbGxscOTIEQAtv0Y///xzWFlZQSqVori4GEuWLIGLiwuuXLnS\nqpgZYwZGh3k7LXJ0dKT33ntPpS0/P58A0J/+9CdlW3p6Oq1du5ZKS0vp4cOHNGLECJW5RZrqOWpq\ne1GtQm1rHrbG//t//49eeeWVNm3DmGpovvnmmwSAysrKDHJfiJ7U17SxsXnhvhBp9xo1NTWlO3fu\nqKz39ttv05EjR5T/r+1rdOHChbR161aaOnUq/d///Z9WMRIZxjzXzobn8TMhGdQ81+DgYKxZswZ2\ndnawt7fHpEmT8PDhQ50qaOhSq1CbepSGxphqaBrCvujqRa/RefPmoampSSW+yspKnDt3DhMmTACg\n22t048aN+OSTT3Do0CEMGDCg43aUMdZu9P7gfsXvtLpMRG9trUJt61EaEmOqodlZ9+XZ1+jo0aPh\n4eGBL7/8ElFRURCJRNi/fz9CQ0OVz4xtr3qazzp48CBEIpFg23tZ8DFj7a3Dk+u3336LTZs2IS8v\nD5WVla1KdE/XKly5cqXKMmdnZ0Hi7IyMqYamPvflRa9RkUiEuXPnYvHixTh58iTeeOMN/O///i/2\n7dun7NNRr9ERI0Zg0aJFgm3P2J05cwaJiYnK374ZawvF60mTDk2ut2/fxpQpUzB16lR8+eWX6NGj\nB7Zu3YpPP/1Up+08XaswIiKiPULtdIyphmZH78upU6fw73//G4sWLdL6Nfr+++8jKioKu3btQq9e\nvSCTydCnTx/l8o56jfbs2RNvvfVWu23fGCUmJvIxY4IxiOR66dIlNDY2Yv78+XBzcwPQusszQtUq\nNCbGVEOzo/fl3//+t7LqiLavUTs7O0ybNg379++HVCrFRx99pLKcX6OMvdw69IYmRXHgEydOoL6+\nHvn5+SpTLgDN9RyfbTM1NX1hrUJjZ0w1NNt7X56nsbER9+/fVynppc1rVGHevHl49OgRjh49qvYw\nEG3qaTLGjJgOtxZrdPPmTfrd735HAMjMzIyGDRtGBw8epC+++IIcHR0JAFlbW9PUqVOJiGj58uVk\nb29Ptra2FBISQtu2bSMAJJfL6fbt23T+/Hnq06cPWVpa0uuvv05FRUUa2x49ekTLly+n3r17k5mZ\nGXXv3p2CgoIoLy+Ptm/fTlZWVgSA+vXrR9euXaOdO3eSTCYjANSnTx/69ddfdbrl+syZM/Taa6+R\ns7MzASAA5OTkRL6+vvTDDz/otK2tW7eSk5MTASArKyuaNGmSTjGHhYWRWCwmFxcXMjMzI5lMRpMn\nT6Zr166pjPPw4UMaNWoUSSQScnV1pQULFtCyZcsIALm7uyunumg6vseOHSOpVEqxsbHP3Y8ff/yR\nBg0aRCYmJsrjsX79eoPalz/96U8kl8uV5+x5f4cPH1aO9aLX6NN+97vfUWRkpMbj09JrND4+niwt\nLQkA9erVq1Vly3gqju54Kg4TUktTcUREqg+5TUtLw7Rp04zi2bfGau7cuUhPT8fDhw/1HUqbdfZ9\nCQgIwLZt2+Dq6trhY4eEhAAA0tPTO3zszoo/35iQWng9pXPJuU7KmGpodqZ9efoy88WLFyGRSPSS\nWBljhu2lTa6//PKL8jF2Lf1pW9BX6O0xw7R8+XLk5+fj119/xaxZs7Bu3Tp9h8Ta2dy5c1Xew5pK\nFp44cQKRkZFqJQ7feecdtb5jx46FVCqFqakpBg0ahPPnz3fEbrRaTEwMPD09IZPJYGFhAXd3d3z6\n6acaC8R/9dVX8Pb2hlQqRZ8+fTBr1qxWV6ISctwjR44gPj5e7R/yGRkZKue2W7durYpVIx2uITMD\nEBkZSebm5gSA+vbtS+np6foOqdU6475ER0eTiYkJ9erVS+VRh/rAv7nqrjWfb2FhYWRvb09ZWVl0\n5coVqq+vV1m+evVqmjhxIlVWVirb5HI5de3alQDQ0aNH1baZlZVFgYGBrduJDubv70/bt2+nhw8f\nUmVlJR04cIDEYjGNGzdOpd/+/fsJAMXHx1N5eTlduHCB3NzcaOjQodTY2Kj3cRMTE8nf31/lMa3N\nzc1UWFhIp06dogkTJqg85lQbLf3mysmVsU7KEJJrbW0t+fj4dJoxWptcXVxcNC7bsGEDeXh4UF1d\nnUq7XC6nffv2kYmJCbm4uFB5ebnK8s6UXAMCAujx48cqbW+99RYBULnBb9SoUdSjRw9qbm5Wtilu\nBszJyTGIccPDw8nHx0djsl+4cKGgyfWlvSzMGGu7jijxZ6hlBK9evYpVq1bhs88+g0QiUVvu6+uL\niIgI3LlzB0uXLtVDhMI4evSo8rGeCorLp7W1tcq2goICODs7q8wL79WrFwBonDanj3HXrl2L3Nzc\n5z74QUicXBl7iRAREhISlIUS7OzsMHnyZJXnHbelxF9nKIkolKSkJBARJk2a9Nw+sbGx8PDwwK5d\nu3DixIkWt6fNudGlnGZLJQ/b6s6dO7C0tFS5mc/NzU3tH0GK3z0VD2TR97h2dnbw9/dHYmJi+98x\nrsPXXMaYAWnNZeHVq1eTubk57dmzh8rLy+nixYs0bNgw6tatGxUVFSn7taXEn6GVRHyakJeF3dzc\nyNPTU+M6crmcbty4QUREp0+fJhMTE+rbty9VV1cTkebLwtqeG23LOb6o5GFr1dTUkFQqpfDwcJX2\n7OxsEovFlJSURJWVlXT58mUaOHAgvfnmm20aT+hxIyMjCQBduHBBpZ0vCzPGWqWurg4JCQmYOnUq\nZs6cCRsbG3h5eWHHjh0oKSnBzp07BRurs5REbK2amhrcuHEDcrn8hX19fHywaNEi3Lx5EytWrNDY\npzXnpqVyjrqUPNRVXFwcnJ2dERsbq9Lu7++P5cuXIzw8HDKZDIMHD0ZVVRV27drVpvGEHrdfv34A\nnjzqtD1xcmXsJZGXl4fq6moMHz5cpd3b2xvm5ubPfcyjEAytjGBbFRcXg4hgZWWlVf/Y2Fj0798f\n27dvR05Ojtrytp6bZ8s5tlfJw8OHDyMtLQ3Hjx+HVCpVWRYdHY2dO3fi5MmTqK6uxvXr1+Hr6wsf\nHx8UFBS0ekyhx1Wcs/v377cpphfh5MrYS6K8vBwA0KVLF7Vltra2qKqqatfxjakkYn19PYAn+6QN\niUSC1NRUiEQifPDBB6irq1NZLvS5ebrk4dPzOG/duqVyM5Au9u/fj40bNyI7Oxt9+/ZVWXbv3j3E\nx8djzpw5GD16NKytreHq6oqUlBTcvXsXmzZtatWY7TGupaUlgN/OYXvh5MrYS8LW1hYANH5Qt3eJ\nP2MqiQj89gGty9PFfHx8sHjxYuTn56s9fEToc/N0yUMiUvk7c+aMTtsCgK1bt2Lv3r34/vvv0aNH\nD7Xl+fn5aGpqUlsmk8lgb2+PvLw8ncdsr3EbGhoA/HYO20uHF0tnjOnH4MGD0aVLF/z0008q7WfP\nnkVDQwNeffVVZZvQJf6MqSQiADg4OEAkEqGiokKn9datW4ejR4/iwoULygpMgG7nRhtClTwkIqxY\nsQJlZWXIyMiAmZnmlKFI/s9WfKqqqkJpaalyaowhjKs4Z46OjjrFpCv+5srYS0IikWDJkiU4fPgw\n9u7di8rKSly6dAnz5s2Ds7MzwsLClH3bWuLPmEoiamJlZQU3NzcUFhbqtJ7i8vCz8zd1OTfajvOi\nkoehoaFwdHRs8fGLP//8Mz7//HOkpKRALBarPc518+bNAABXV1eMGjUKKSkpOHXqFOrq6lBQUKCM\ne/bs2cpt6mtcBcU58/Ly0uWQ6oyTK2MvkTVr1iAuLg4xMTHo1q0b/P390bdvX5WatgAwf/58jBo1\nCtOnT0f//v2xbt065WW0p28UmTdvHhwcHODp6YkJEyagtLQUwJPfs7y8vGBpaQk/Pz94eHjgH//4\nh8pvlG0dQ98CAgKQl5en8vvpN998A3d3d1y7dg3e3t5YsGCB2nojRozA4sWL1dq1OTfJycnYsmUL\nAGDIkCG4fv06UlJSsGTJEgDAuHHjkJ+fDwBITEzEokWLEB8fj65du8LZ2RkREREoKysD8OTyaHFx\nMTIzM5+7j6TlXFCRSIT09HSEhoZi9uzZsLOzg6enJ27fvo1Dhw7Bz89P2Vdf4yqcO3cOLi4uGDJk\niFZjtJoO83YYYwbEEB5/qIniWbyGSMh5rvn5+WRmZtaqWryGoKmpifz8/Gj37t0vxbhERCUlJSSR\nSGjz5s1qy3ieK2PM4HWmMoLaqKurw/Hjx5Gfn6+8Icbd3R0xMTGIiYnRWKnFkDU1NSEjIwNVVVUd\nWqlLX+MqrF27FkOHDkV4eDiAJ9+Q7969i5ycHFy9elXQsTi5MsbYC5SWlmLcuHHw8PDABx98oGyP\njIxESEgIQkNDdb65SZ+ys7Nx6NAhZGVlaT1XtzOPCwAJCQnIzc3FsWPHIBaLAQCZmZlwcXGBn58f\nvv32W0HH4+TKGBNMVFQUUlNTUVFRAVdXVxw8eFDfIbXZjh07VKay7N27V2X5+vXrER4ejg0bNugp\nQt2NGTMG+/btU3m2szGPm5mZiUePHiE7Oxt2dnbK9smTJ6ucW8UzrYXAU3EYY4KJi4tDXFycvsPo\ncGPHjsXYsWP1HQZ7jsDAQAQGBnbomPzNlTHGGBMYJ1fGGGNMYJxcGWOMMYFxcmWMMcYE9twbmtLS\n0joyDsaYjhSPceP3qvYUD63nY8aE0FIRBBGR6nOm0tLSMG3atHYPijHGGDMGpP64xnS15MoYMzyK\nf/Ty25WxTiGdf3NljDHGBMbJlTHGGBMYJ1fGGGNMYJxcGWOMMYFxcmWMMcYExsmVMcYYExgnV8YY\nY0xgnFwZY4wxgXFyZYwxxgTGyZUxxhgTGCdXxhhjTGCcXBljjDGBcXJljDHGBMbJlTHGGBMYJ1fG\nGGNMYJxcGWOMMYFxcmWMMcYExsmVMcYYExgnV8YYY0xgnFwZY4wxgXFyZYwxxgTGyZUxxhgTGCdX\nxhhjTGCcXBljjDGBcXJljDHGBMbJlTHGGBMYJ1fGGGNMYJxcGWOMMYFxcmWMMcYExsmVMcYYExgn\nV8YYY0xgnFwZY4wxgXFyZYwxxgRmpu8AGGOqCgsL8d5776GpqUnZVlZWBqlUipEjR6r07d+/P/78\n5z93cISMsRfh5MqYgenZsydu3bqFa9euqS374YcfVP7/97//fUeFxRjTAV8WZswAvfvuuxCLxS/s\nFxoa2gHRMMZ0xcmVMQM0Y8YMPH78uMU+gwYNgqenZwdFxBjTBSdXxgyQXC7HkCFDIBKJNC4Xi8V4\n7733Ojgqxpi2OLkyZqDeffddmJqaalz2+PFjhISEdHBEjDFtcXJlzEBNnz4dzc3Nau0mJiYYMWIE\n+vbt2/FBMca0wsmVMQPl7OyM1157DSYmqm9TExMTvPvuu3qKijGmDU6ujBmwd955R62NiDB16lQ9\nRMMY0xYnV8YMWHBwsMrvrqampnjjjTfg4OCgx6gYYy/CyZUxA2ZnZ4c//OEPygRLRJg5c6aeo2KM\nvQgnV8YM3MyZM5U3NonFYkyePFnPETHGXoSTK2MGbtKkSbCwsAAATJw4EV26dNFzRIyxF+HkypiB\ns7a2Vn5b5UvCjHUOIiIifQchpLS0NEybNk3fYTDGGNOSkaUhAEg32qo4Bw4c0HcIzMhs2bIFALBo\n0aIOH7upqQkHDhzA22+/3eFjt8WZM2eQmJjI70emkeL1YYyMNrm+9dZb+g6BGZn09HQA+nttTZky\nBRKJRC9jt0ViYiK/H9lzGWty5d9cGeskOmNiZexlxcmVMcYYExgnV8YYY0xgnFwZY4wxgXFyZYwx\nxgTGyZWxDnbs2DHY2Njgr3/9q75DMXgnTpxAZGQkDh06BDc3N4hEIohEIo3VgsaOHQupVApTU1MM\nGjQI58+f10PE2ouJiYGnpydkMhksLCzg7u6OTz/9FNXV1Wp9v/rqK3h7e0MqlaJPnz6YNWsWioqK\n9D7ukSNHEB8fj6amplbFYsw4uTLWwYxwwny7WLNmDZKSkhAVFYWgoCBcv34dcrkcXbt2xd69e/Ht\nt9+q9P/b3/6G9PR0TJw4EXl5eRg2bJieItfO999/j08++QQ3b95ESUkJ4uLikJiYiJCQEJV+Bw4c\nwIwZMxASEoLCwkJkZmbi1KlTGD9+PB4/fqzXcSdNmgSJRIIxY8agvLy89QfDGJGROXDgABnhbjED\nEBwcTMHBwfoOQ1C1tbXk4+PTbttv7ftxw4YN5OHhQXV1dSrtcrmc9u3bRyYmJuTi4kLl5eUqy7Oy\nsigwMLBNMXeUgIAAevz4sUrbW2+9RQDo9u3byrZRo0ZRjx49qLm5Wdm2bds2AkA5OTkGMW54eDj5\n+PhQY2OjTrEY8ed1Gn9zZewltnv3bhQXF+s7DBVXr17FqlWr8Nlnn2mc2xNVxmcAACAASURBVOvr\n64uIiAjcuXMHS5cu1UOEwjh69KhKrV4A6NatGwCgtrZW2VZQUABnZ2eIRCJlW69evQAAt27dMohx\n165di9zcXKN9IERrcHJlrAPl5OSgd+/eEIlE2LZtGwAgOTkZ1tbWsLKyQmZmJsaPHw+ZTIaePXvi\n66+/Vq6blJQEiUQCBwcHzJ07F87OzpBIJPD19cXZs2eV/cLDw2Fubg4nJydl28cffwxra2uIRCKU\nlJQAACIiIrBkyRJcu3YNIpEI7u7uAIDvvvsOMpkM69ev74hDoiYpKQlEhEmTJj23T2xsLDw8PLBr\n1y6cOHGixe0RERISEjBw4EBYWFjAzs4OkydPxi+//KLso+05AJ48inL16tXo3bs3LC0tMWTIEMEe\n73jnzh1YWlrC1dVV2ebm5qb2DyDF755ubm4GMa6dnR38/f2RmJjIP3so6Pmrs+CM+DID0zOhLgsX\nFBQQANq6dauyLTo6mgDQyZMnqaKigoqLi8nPz4+sra2poaFB2S8sLIysra3p559/pvr6esrLyyNv\nb2+SSqUql/RmzJhBjo6OKuNu2rSJANCDBw+UbUFBQSSXy1X6HT16lKRSKcXExLR5X1vzfnRzcyNP\nT0+Ny+RyOd24cYOIiE6fPk0mJibUt29fqq6uJiLNl4VXr15N5ubmtGfPHiovL6eLFy/SsGHDqFu3\nblRUVKTsp+05WLp0KVlYWNDBgweprKyMoqKiyMTEhM6dO6fTfj6rpqaGpFIphYeHq7RnZ2eTWCym\npKQkqqyspMuXL9PAgQPpzTffbNN4Qo8bGRlJAOjChQtaj23En9d8WZgxQ+Lr6wuZTIbu3bsjNDQU\nNTU1uH37tkofMzMz5bcwT09PJCcno6qqCqmpqYLEEBAQgMrKSqxatUqQ7emipqYGN27cgFwuf2Ff\nHx8fLFq0CDdv3sSKFSs09qmrq0NCQgKmTp2KmTNnwsbGBl5eXtixYwdKSkqwc+dOtXVaOgf19fVI\nTk7GlClTEBQUBFtbW6xcuRJisbjNxz8uLg7Ozs6IjY1Vaff398fy5csRHh4OmUyGwYMHo6qqCrt2\n7WrTeEKP269fPwDApUuXBImrs+PkypiBMjc3BwA0Nja22G/48OGwsrJSuczZWRUXF4OIYGVlpVX/\n2NhY9O/fH9u3b0dOTo7a8ry8PFRXV2P48OEq7d7e3jA3N1e5nK7Js+fgypUrqK2txeDBg5V9LC0t\n4eTk1Kbjf/jwYaSlpeH48eOQSqUqy6Kjo7Fz506cPHkS1dXVuH79Onx9feHj44OCgoJWjyn0uIpz\ndv/+/TbFZCw4uTJmBCwsLPDgwQN9h9Fm9fX1AJ7sjzYkEglSU1MhEonwwQcfoK6uTmW5YnpIly5d\n1Na1tbVFVVWVTvHV1NQAAFauXKmccysSiXDr1i2Vm4F0sX//fmzcuBHZ2dno27evyrJ79+4hPj4e\nc+bMwejRo2FtbQ1XV1ekpKTg7t272LRpU6vGbI9xLS0tAfx2Dl92nFwZ6+QaGxtRXl6Onj176juU\nNlN8QOvyUAIfHx8sXrwY+fn5WLduncoyW1tbANCYRFtzzLp37w7gSW1fIlL5O3PmjE7bAoCtW7di\n7969+P7779GjRw+15fn5+WhqalJbJpPJYG9vj7y8PJ3HbK9xGxoaAPx2Dl92RlvPlbGXRXZ2NogI\nI0aMULaZmZm98HKyIXJwcIBIJEJFRYVO661btw5Hjx7FhQsX0Lt3b2X74MGD0aVLF/z0008q/c+e\nPYuGhga8+uqrOo3Tq1cvSCQS5Obm6rTes4gIK1asQFlZGTIyMmBmpvmjWJH87927p9JeVVWF0tJS\n5dQYQxhXcc4cHR11islY8TdXxjqZ5uZmlJWV4fHjx7h48SIiIiLQu3dvvP/++8o+7u7uKC0tRUZG\nBhobG/HgwQONcyLt7e1x9+5d3Lx5E1VVVWhsbERWVpbepuJYWVnBzc0NhYWFOq2nuDz87PxNiUSC\nJUuW4PDhw9i7dy8qKytx6dIlzJs3D87OzggLC9N5nFmzZuHrr79GcnIyKisr0dTUhMLCQmUiCg0N\nhaOjY4uPX/z555/x+eefIyUlBWKxWOUSs0gkwubNmwEArq6uGDVqFFJSUnDq1CnU1dWhoKBAGffs\n2bOV29TXuAqKc+bl5aXLITVanFwZ60Dbtm2Dt7c3AGD58uUIDAxEcnIytmzZAgAYMmQIrl+/jpSU\nFCxZsgQAMG7cOOTn5yu3UV9fDy8vL1haWsLPzw8eHh74xz/+ofI75fz58zFq1ChMnz4d/fv3x7p1\n65SX656+IWXevHlwcHCAp6cnJkyYgNLS0g45Di0JCAhAXl6eyu+n33zzDdzd3XHt2jV4e3tjwYIF\nauuNGDECixcvVmtfs2YN4uLiEBMTg27dusHf3x99+/ZFdnY2rK2tAUCnc5CYmIhFixYhPj4eXbt2\nhbOzMyIiIlBWVgbgyeXR4uJiZGZmPncfScu5oCKRCOnp6QgNDcXs2bNhZ2cHT09P3L59G4cOHYKf\nn5+yr77GVTh37hxcXFwwZMgQrcYwevqaBNRejHjeFNMzQ3j8YVhYGNnb2+s1Bl205v2Yn59PZmZm\ntGfPnnaKqn01NTWRn58f7d69+6UYl4iopKSEJBIJbd68Waf1jPjzmue5MtbZGHsFEnd3d8TExCAm\nJkZjpRZD1tTUhIyMDFRVVSE0NNTox1VYu3Ythg4divDw8A4f21Bxcv2vR48eYeHChXBycoKVlRXe\neOMN5c0VO3bs0Hd4gmlubsaWLVvg6+vb6m08W/5L05/i1v7Nmzcb5XFk7SsyMhIhISEIDQ3V+eYm\nfcrOzsahQ4eQlZWl9VzdzjwuACQkJCA3NxfHjh2DWCzu0LENGSfX//riiy/w3Xff4ZdffkFiYiLm\nzp2L06dP6zssQeXn5+P3v/89Fi9e3Oo5eQBUyn/Z2NgopyI8fvwYtbW1uH//vvINvnTpUqM7jvoS\nFRWF1NRUVFRUwNXVFQcPHtR3SO1q/fr1CA8Px4YNG/QditbGjBmDffv2qTzX2ZjHzczMxKNHj5Cd\nnQ07O7sOHdvQcXL9r4yMDAwfPhy2traYM2cOgoODW7Wduro6tW+Fmto62n/+8x+sWLEC8+bNw9Ch\nQ9tlDFNTU1haWsLBwQEeHh5t2pahHkd9iouLw6NHj0BEuHHjRqtfo53J2LFjsXHjRn2HwZ4jMDAQ\nkZGRandpM06uSoWFhYJc0tBUwssQynq98sorOHToEGbMmKH102/aIiMjo03rG+pxZIwxbbz0yfXv\nf/873N3dce/ePfzlL3+BSCTS+Kg0hX/+85/w9PSEjY0NJBIJvLy8cPz4cQCaS3g9r6xXS2WrdCl/\nJbSOKjdm7MeRMfZye+mT6x/+8AdcvXoVjo6OeO+990BELd6heP/+fUybNg03b97E3bt30aVLF8yY\nMQPAk/lvEydOhFwuBxHh6tWrGtsAYMWKFfj888+xZcsW3Lt3DxMnTsTbb7+Nn376CfPnz8eiRYtQ\nV1cHqVSKAwcO4Nq1a3Bzc8NHH33Urk/eUdyJ2tzc3Kr1v//+e+VE9JYY+3FkjL3cXvrkqqvg4GCs\nWbMGdnZ2sLe3x6RJk/Dw4UOdHpquS9kqbUqQCUnXcmMVFRUqdwmPGTNGq/WM/Tgyxl5u/GzhNlL8\nTqvL3MPWlq3StgRZR7KxsVFWHgGeTAl49jmu2ugsx7GwsBBpaWk6r/eyUjzMno8Z06Q1xQ46C06u\nOvr222+xadMm5OXlobKyslUf0E+XrVq5cqXKMmdnZ0Hi1JeRI0di5MiRL+zXWY/jjz/+iGnTprXL\nto0ZHzP2suHLwjq4ffs2pkyZAicnJ5w9exYVFRWIj4/XeTtCl63qbDrzcQwODlYbi/+e/6e4uUzf\ncfCfYf4pXh/GiL+56uDSpUtobGzE/Pnz4ebmBuDJA651JVTZqs6KjyNjzNjxN1cdKOpEnjhxAvX1\n9cjPz8fZs2dV+mgq4fVsm6mp6QvLVulLR5QbexmOI2PsJUdGRtcqCzdv3qTf/e53BIDMzMxo2LBh\ndPDgQfriiy/I0dGRAJC1tTVNnTqViIiWL19O9vb2ZGtrSyEhIbRt2zYCQHK5nG7fvk3nz5+nPn36\nkKWlJb3++utUVFSkse3Ro0e0fPly6t27N5mZmVH37t0pKCiI8vLyaPv27WRlZUUAqF+/fnTt2jXa\nuXMnyWQyAkB9+vShX3/9VafjcubMGXrttdfI2dmZABAAcnJyIl9fX/rhhx+U/Y4dO0ZSqZRiY2Of\nu61//etf5OHhobKdMWPGaOxrTMfREKridDZGXPWECcCIXx9pIiLSrsBfJ5GWloZp06bByHaLGYCQ\nkBAAQHp6up4j6Tz4/chaYsSvj3S+LMwYY4wJjJNrJ/XLL7+0WPJN8aeP2o6MMfay4+TaSQ0YMECr\nW93379+v71AZa3cnTpxAZGSkWq3hd955R63v2LFjIZVKYWpqikGDBuH8+fN6iFh7I0eOfO4/np99\nDvpXX30Fb29vSKVS9OnTB7NmzUJRUVGL26+vr8eAAQNU5oofOXIE8fHxOj3Uhani5MoY69TWrFmD\npKQkREVFqdQa7tq1K/bu3Ytvv/1Wpf/f/vY3pKenY+LEicjLy8OwYcP0FHnbvf7668r/PnDgAGbM\nmIGQkBAUFhYiMzMTp06dwvjx4/H48ePnbiM6OhpXrlxRaZs0aRIkEgnGjBmj8gQ2pj1Orox1Ih1R\n07Yz1c3duHEj9u/fj7S0NEilUpVlSUlJMDExQVhYGCoqKvQUYdtJJBJUVlaqXZUKCwvDp59+quz3\n5z//GT169MCyZctgY2ODoUOHYvHixcjNzVWb6qZw+vRpXL58WeOyhQsX4pVXXsGECRNaTM5MM06u\njHUiHVHTtrPUzb169SpWrVqFzz77DBKJRG25r68vIiIicOfOHSxdulQPEQrju+++U/uHQ0FBAS5f\nvozRo0ertDk7O6s8kKVXr14AgFu3bqltt66uDsuWLUNiYuJzx167di1yc3Nb7MM04+TKWDsiIiQk\nJGDgwIGwsLCAnZ0dJk+erFJYIDw8HObm5nByclK2ffzxx7C2toZIJEJJSQkAzXVuk5KSIJFI4ODg\ngLlz58LZ2RkSiQS+vr4q31baMgbQcXV+dZGUlAQiwqRJk57bJzY2Fh4eHti1axdOnDjR4va0OVe6\n1AhuqdZwW23cuBELFy5UaXNzc1P7R5Hi91bFk9CeFh0djY8//lj5GFFN7Ozs4O/vj8TERGOcLtO+\nOnJWbUcw4knJTM9a8xCJ1atXk7m5Oe3Zs4fKy8vp4sWLNGzYMOrWrRsVFRUp+82YMYMcHR1V1t20\naRMBoAcPHijbgoKCSC6Xq/QLCwsja2tr+vnnn6m+vp7y8vLI29ubpFIp3b59W5Axjh49SlKplGJi\nYnTa//Z8P7q5uZGnp6fGZXK5nG7cuEFERKdPnyYTExPq27cvVVdXExFRVlYWBQYGqqyj7bmKjo4m\nAHTy5EmqqKig4uJi8vPzI2tra2poaFD2W7p0KVlYWNDBgweprKyMoqKiyMTEhM6dO9em/S4sLCRP\nT09qampSac/OziaxWExJSUlUWVlJly9fpoEDB9Kbb76pto2cnByaNGkSERE9ePCAAFB0dLTG8SIj\nIwkAXbhwoU1xa2LEn9dp/M2VsXZSV1eHhIQETJ06FTNnzoSNjQ28vLywY8cOlJSUYOfOnYKNZWZm\npvzG5enpieTkZFRVVanVtW0tXev8treamhrcuHEDcrn8hX19fHywaNEi3Lx5EytWrNDYpzXnqqUa\nwbrUGtbVxo0bsWDBApiYqH58+/v7Y/ny5QgPD4dMJsPgwYNRVVWFXbt2qe1rREQEkpOTtRqvX79+\nAJ48E5xpj5MrY+0kLy8P1dXVGD58uEq7t7c3zM3Nn3uTiRCGDx8OKyurFuvadmbFxcUgIlhZWWnV\nPzY2Fv3798f27duRk5Ojtryt5+rZGsGtrTX8Infv3sWRI0fw/vvvqy2Ljo7Gzp07cfLkSVRXV+P6\n9evw9fWFj48PCgoKlP2ioqIwZ84cuLi4aDWm4hjfv3+/1XG/jDi5MtZOFFMYnp2LCAC2traoqqpq\n1/EtLCzw4MGDdh1DX+rr6wE82UdtSCQSpKamQiQS4YMPPkBdXZ3KcqHP1dO1hp+el3rr1i3U1tbq\ntK2nxcfH46OPPlK7gevevXuIj4/HnDlzMHr0aFhbW8PV1RUpKSm4e/cuNm3aBADIycnBpUuX8OGH\nH2o9pqWlJYDfjjnTDidXxtqJra0tAGj8YC4vL0fPnj3bbezGxsZ2H0OfFB/4ujzkwMfHB4sXL0Z+\nfj7WrVunskzoc9UetYaLiorw1VdfYf78+WrL8vPz0dTUhB49eqi0y2Qy2NvbIy8vD8CTO8FPnjwJ\nExMTZcJXxLp+/XqIRCL89NNPKttoaGgA8NsxZ9rh5MpYOxk8eDC6dOmi9mF19uxZNDQ04NVXX1W2\nmZmZKS8pCiE7OxtEhBEjRrTbGPrk4OAAkUik8/zVdevWYcCAAbhw4YJKuy7nShvtUWs4Pj4eM2fO\nhL29vdoyRfJ/ttRiVVUVSktLlVNyUlNT1ZK94upGdHQ0iEjt0rjiGDs6Ogq2Ly8DTq6MtROJRIIl\nS5bg8OHD2Lt3LyorK3Hp0iXMmzcPzs7OCAsLU/Z1d3dHaWkpMjIy0NjYiAcPHmicm6ipzi0ANDc3\no6ysDI8fP8bFixcRERGB3r17q/w215YxOqLOry6srKzg5uaGwsJCndZTXB42NTVVa9f2XGk7zotq\nDYeGhsLR0VGrxy/ev38fX375JRYtWqRxuaurK0aNGoWUlBScOnUKdXV1KCgoUMY9e/ZsneJ/muIY\ne3l5tXobLyU93abcboz41m6mZ62ZitPc3EybNm2ifv36kVgsJjs7O5oyZQpduXJFpd/Dhw9p1KhR\nJJFIyNXVlRYsWEDLli0jAOTu7q6cUqOppm1YWBiJxWJycXEhMzMzkslkNHnyZLp27ZpgY2hT51eT\n9nw/hoeHk1gsptraWmXb4cOHSS6XEwDq1q0bffLJJxrXXbZsmdpUHG3OlS41gluqNUxENGXKFAJA\nq1evfuG+Ll68mGbOnNlin5KSEoqIiCB3d3eysLCgLl260GuvvUbffPNNi+u9aCpOQEAAubi4UHNz\n8wvj1JURf16nGd1eGfHJYnpmqMXSw8LCyN7eXt9haNSe78f8/HwyMzOjPXv2tMv221tTUxP5+fnR\n7t279R3Kc5WUlJBEIqHNmze3y/aN+POa57kyZgxexuol7u7uiImJQUxMDKqrq/Udjk6ampqQkZGB\nqqoqgy4LuXbtWgwdOhTh4eH6DqXT4eTKGOu0IiMjERISgtDQ0E71cP7s7GwcOnQIWVlZWs/V7WgJ\nCQnIzc3FsWPHIBaL9R1Op8PJlbFOLCoqCqmpqaioqICrqysOHjyo75A63Pr16xEeHo4NGzboOxSt\njRkzBvv27VN51rMhyczMxKNHj5CdnQ07Ozt9h9Mpmek7AMZY68XFxSEuLk7fYejd2LFjMXbsWH2H\nYTQCAwMRGBio7zA6Nf7myhhjjAmMkytjjDEmME6ujDHGmMA4uTLGGGMCM9obmkJCQvQdAjMyP/74\nIwB+belC8eg8PmZME10fX9mZiIiI9B2EkM6cOYOEhAR9h8GYoIqKinDhwgWMHz9e36EwJrj09HR9\nhyC0dKNLrowZo7S0NEybNg38dmWsU0jn31wZY4wxgXFyZYwxxgTGyZUxxhgTGCdXxhhjTGCcXBlj\njDGBcXJljDHGBMbJlTHGGBMYJ1fGGGNMYJxcGWOMMYFxcmWMMcYExsmVMcYYExgnV8YYY0xgnFwZ\nY4wxgXFyZYwxxgTGyZUxxhgTGCdXxhhjTGCcXBljjDGBcXJljDHGBMbJlTHGGBMYJ1fGGGNMYJxc\nGWOMMYFxcmWMMcYExsmVMcYYExgnV8YYY0xgnFwZY4wxgXFyZYwxxgTGyZUxxhgTGCdXxhhjTGCc\nXBljjDGBcXJljDHGBMbJlTHGGBMYJ1fGGGNMYGb6DoAxpqqxsRHV1dUqbTU1NQCAsrIylXaRSARb\nW9sOi40xph1OrowZmNLSUri4uKCpqUltmb29vcr/jxo1Ct9//31HhcYY0xJfFmbMwDg6OuL3v/89\nTExafnuKRCJMnz69g6JijOmCkytjBuidd955YR9TU1NMnTq1A6JhjOmKkytjBigoKAhmZs//1cbU\n1BTjxo1D165dOzAqxpi2OLkyZoBkMhnGjx//3ARLRJg5c2YHR8UY0xYnV8YM1MyZMzXe1AQA5ubm\n+OMf/9jBETHGtMXJlTED9cc//hFWVlZq7WKxGFOmTIG1tbUeomKMaYOTK2MGSiKRYOrUqRCLxSrt\njY2NmDFjhp6iYoxpg5MrYwbs7bffRmNjo0qbTCbDH/7wBz1FxBjTBidXxgzYG2+8ofLgCLFYjOnT\np8Pc3FyPUTHGXoSTK2MGzMzMDNOnT1deGm5sbMTbb7+t56gYYy/CyZUxAzd9+nTlpWFHR0e8/vrr\neo6IMfYinFwZM3C+vr5wcXEBALz77rsvfCwiY0z/+MH9/1VYWIjTp0/rOwzGNPL29sadO3fQtWtX\npKWl6TscxjR666239B2CwRAREek7CEOQlpaGadOm6TsMxhjrtDidKKXzN9dn8IuD6SokJAQAkJ6e\n3q7jHDx4EMHBwe06RkdR/GOW32/Ggb+cqOMfbxjrJIwlsTL2MuDkyhhjjAmMkytjjDEmME6ujDHG\nmMA4uTLGGGMC4+TKGGOMCYyTK2MG4tixY7CxscFf//pXfYdi8E6cOIHIyEgcOnQIbm5uEIlEEIlE\neOedd9T6jh07FlKpFKamphg0aBDOnz+vh4i1N3LkSOX+PPvXpUsXlb5fffUVvL29IZVK0adPH8ya\nNQtFRUUtbr++vh4DBgzAypUrlW1HjhxBfHw8mpqa2mWfXkacXBkzEDznUztr1qxBUlISoqKiEBQU\nhOvXr0Mul6Nr167Yu3cvvv32W5X+f/vb35Ceno6JEyciLy8Pw4YN01Pkbff0c6UPHDiAGTNmICQk\nBIWFhcjMzMSpU6cwfvx4PH78+LnbiI6OxpUrV1TaJk2aBIlEgjFjxqC8vLzd4n+ZcHJlzEAEBASg\noqICEydO1HcoqKurg6+vr77DULNx40bs378faWlpkEqlKsuSkpJgYmKCsLAwVFRU6CnCtpNIJKis\nrAQRqfyFhYXh008/Vfb785//jB49emDZsmWwsbHB0KFDsXjxYuTm5uLs2bMat3369GlcvnxZ47KF\nCxfilVdewYQJE1pMzkw7nFwZY2p2796N4uJifYeh4urVq1i1ahU+++wzSCQSteW+vr6IiIjAnTt3\nsHTpUj1EKIzvvvtO7R8OBQUFuHz5MkaPHq3S5uzsDJFIpGzr1asXAODWrVtq262rq8OyZcuQmJj4\n3LHXrl2L3NzcFvsw7XByZcwA5OTkoHfv3hCJRNi2bRsAIDk5GdbW1rCyskJmZibGjx8PmUyGnj17\n4uuvv1aum5SUBIlEAgcHB8ydOxfOzs6QSCTw9fVV+QYTHh4Oc3NzODk5Kds+/vhjWFtbQyQSoaSk\nBAAQERGBJUuW4Nq1axCJRHB3dwfw5ENfJpNh/fr1HXFI1CQlJYGIMGnSpOf2iY2NhYeHB3bt2oUT\nJ060uD0iQkJCAgYOHAgLCwvY2dlh8uTJ+OWXX5R9tD0HANDU1ITVq1ejd+/esLS0xJAhQ3DgwIG2\n7fR/bdy4EQsXLlRpc3NzU/sHkOL3Vjc3N7VtREdH4+OPP0b37t2fO46dnR38/f2RmJjIP1O0FTEi\nIjpw4ADx4WCtERwcTMHBwW3eTkFBAQGgrVu3Ktuio6MJAJ08eZIqKiqouLiY/Pz8yNramhoaGpT9\nwsLCyNramn7++Weqr6+nvLw88vb2JqlUSrdv31b2mzFjBjk6OqqMu2nTJgJADx48ULYFBQWRXC5X\n6Xf06FGSSqUUExPT5n1tzfvNzc2NPD09NS6Ty+V048YNIiI6ffo0mZiYUN++fam6upqIiLKysigw\nMFBlndWrV5O5uTnt2bOHysvL6eLFizRs2DDq1q0bFRUVKftpew6WLl1KFhYWdPDgQSorK6OoqCgy\nMTGhc+fO6bSfzyosLCRPT09qampSac/OziaxWExJSUlUWVlJly9fpoEDB9Kbb76pto2cnByaNGkS\nERE9ePCAAFB0dLTG8SIjIwkAXbhwQesY+fNTTRp/c2WsE/D19YVMJkP37t0RGhqKmpoa3L59W6WP\nmZmZ8luYp6cnkpOTUVVVhdTUVEFiCAgIQGVlJVatWiXI9nRRU1ODGzduQC6Xv7Cvj48PFi1ahJs3\nb2LFihUa+9TV1SEhIQFTp07FzJkzYWNjAy8vL+zYsQMlJSXYuXOn2jotnYP6+nokJydjypQpCAoK\ngq2tLVauXAmxWNzm479x40YsWLBArY6vv78/li9fjvDwcMhkMgwePBhVVVXYtWuX2r5GREQgOTlZ\nq/H69esHALh06VKb4n7ZcXJlrJMxNzcHADQ2NrbYb/jw4bCyslK5zNlZFRcXg4hgZWWlVf/Y2Fj0\n798f27dvR05OjtryvLw8VFdXY/jw4Srt3t7eMDc3f+4NQQrPnoMrV66gtrYWgwcPVvaxtLSEk5NT\nm47/3bt3ceTIEbz//vtqy6Kjo7Fz506cPHkS1dXVuH79Onx9feHj44OCggJlv6ioKMyZMwcuLi5a\njak4xvfv32913IyTK2NGzcLCAg8ePNB3GG1WX18P4Mn+aEMikSA1NRUikQgffPAB6urqVJYrpps8\nO28UAGxtbVFVVaVTfDU1NQCAlStXqsxLvXXrFmpra3Xa1tPi4+Px0Ucfqd3Ade/ePcTHx2POnDkY\nPXo0rK2t4erqipSUFNy9exebNm0C8OS3/EuXLuHDDz/UekxLS0sAEvfIHwAAIABJREFUvx1z1jqc\nXBkzUo2NjSgvL0fPnj31HUqbKT7wdXnIgY+PDxYvXoz8/HysW7dOZZmtrS0AaEyirTlmipuEtmzZ\nojaF5syZMzptS6GoqAhfffUV5s+fr7YsPz8fTU1N6NGjh0q7TCaDvb098vLyADy56/vkyZMwMTFR\nJnxFrOvXr4dIJMJPP/2kso2GhgYAvx1z1jqcXBkzUtnZ2SAijBgxQtlmZmb2wsvJhsjBwQEikUjn\n+avr1q3DgAEDcOHCBZX2wYMHo0uXLmqJ5ezZs2hoaMCrr76q0zi9evWCRCJBbm6uTuu1JD4+HjNn\nzoS9vb3aMkXyv3fvnkp7VVUVSktLlVNyUlNT1ZK94kpGdHQ0iEjt0rjiGDs6Ogq2Ly8jTq6MGYnm\n5maUlZXh8ePHuHjxIiIiItC7d2+V3+vc3d1RWlqKjIwMNDY24sGDBxrnRNrb2+Pu3bu4efMmqqqq\n0NjYiKysLL1NxbGysoKbmxsKCwt1Wk9xedjU1FStfcmSJTh8+DD27t2LyspKXLp0CfPmzYOzszPC\nwsJ0HmfWrFn4+uuvkZycjMrKSjQ1NaGwsFCZAENDQ+Ho6KjV4xfv37+PL7/8EosWLdK43NXVFaNG\njUJKSgpOnTqFuro6FBQUKOOePXu2TvE/TXGMvby8Wr0NBr53WoFvJWetJcRUnK1bt5KTkxMBICsr\nK5o0aRJt376drKysCAD169ePrl27Rjt37iSZTEYAqE+fPvTrr78S0ZOpOGKxmFxcXMjMzIxkMhlN\nnjyZrl27pjLOw4cPadSoUSSRSMjV1ZUWLFhAy5YtIwDk7u6unLZz/vx56tOnD1laWtLrr79ORUVF\ndOzYMZJKpRQbG9umfSVq3fstPDycxGIx1dbWKtsOHz5McrmcAFC3bt3ok08+0bjusmXL1KbiNDc3\n06ZNm6hfv34kFovJzs6OpkyZQleuXFH20eUcPHr0iJYvX069e/cmMzMz6t69OwUFBVFeXh4REU2Z\nMoUA0OrVq1+4r4sXL6aZM2e22KekpIQiIiLI3d2dLCwsqEuXLvTaa6/RN9980+J6L5qKExAQQC4u\nLtTc3PzCOBX481NNGh+N/+IXB2stoea5tkVYWBjZ29vrNQZdtOb9lp+fT2ZmZrRnz552iqp9NTU1\nkZ+fH+3evVvfoTxXSUkJSSQS2rx5s07r8eenGp7nypixMPaKJu7u7oiJiUFMTAyqq6v1HY5Ompqa\nkJGRgaqqKoSGhuo7nOdau3Ythg4divDwcH2H0ulxchXQhx9+CKlUCpFIJOiNDR0pPj4eAwYMgKWl\nJaytrTFgwACsWrUKlZWVOm/r2XJgij9zc3M4ODhg5MiR2LRpE8rKytphT5gxioyMREhICEJDQzvV\nw/mzs7Nx6NAhZGVlaT1Xt6MlJCQgNzcXx44dg1gs1nc4nR4nVwHt2rULKSkp+g6jTf75z3/io48+\nwu3bt3H//n2sW7cO8fHxCA4O1nlbT5cDs7GxARGhubkZxcXFSEtLg6urK5YvX45Bgwap3bXJtBcV\nFYXU1FRUVFTA1dUVBw8e1HdI7Wr9+vUIDw/Hhg0b9B2K1saMGYN9+/apPNfZkGRmZuLRo0fIzs6G\nnZ2dvsMxCmb6DoAZFnNzc3z88cfKSeshISFIT09Heno67t27B2dn5zZtXyQSwdbWFiNHjsTIkSMR\nEBCAadOmISAgAL/++itsbGyE2I2XSlxcHOLi4vQdRocaO3Ysxo4dq+8wjEZgYCACAwP1HYZR4W+u\nAnu6/FNndPjwYbWnwSgem9Yev3MFBwfj/fffR3FxMXbs2CH49hljTB84ubYBEWHTpk3o378/LCws\nYGNjg2XLlqn1a6kUlS4lrX744Qf8z//8D6ysrCCTyeDl5aX8LbQ9y13l5+fD1tYWffr0UbYJWX5M\nMQ8zKytL2dbZjxlj7CWn7/uVDUVrbiWPjo4mkUhEX3zxBZWVlVFtbS1t375drVzTi0pRaVPSqrq6\nmmQyGcXHx1NdXR0VFRXR1KlTlWXChC531dDQQIWFhbR161aysLBQm/6gS/kxuVxONjY2z11eWVlJ\nAKhXr17Kts50zAxhKk5nw1M3jAufTzU8z1VB1xdH7f9v796DmjrX/YF/AwmEcBFULqmI5aJQ77Xq\nFtTaHo6cUY8oBStVW9lO/aG9IGoZREURvJYOcthbxuOW4h7tVkE5aqt0PO4OeNylTjuoKG4toHij\n3FTkfs3z+8MmmxDEJCwIic9nhj+68qy8T95l8nQla71PYyPJZDKaPXu22vajR4+qFdempiaSyWQU\nGhqqtq+lpSV98sknRPSvQtHU1KSKURbp4uJiIiK6ceMGAaDvvvtOIxdtxtCVs7MzAaAhQ4bQf/3X\nf6n1rdTVy4orEZFIJCJ7e3siMr454+KqO/4wNi18PDVk8AVNeiouLkZjYyP8/f17jNO3FVXXllYe\nHh5wcnLCsmXLsGbNGoSFheH111/v1Rg9efDgAWpqanDlyhXExMTgwIED+OGHH+Dk5KTX8/WkoaEB\nRAQ7OzsAxjlnP/30ExYtWqTzfq8q5RJ7PGemQddlKV8F/JurnpT/mJQdJl5EqFZUVlZW+OGHHzBj\nxgzs2LEDHh4eCA0NRVNTU5+0u5JIJHB0dERAQACOHTuGwsLCPrsi9ddffwUA+Pj4ADDeOWOMMSU+\nc9WT8oralpaWHuM6t6KKjIzs1ZhjxozBt99+i6qqKiQlJWH37t0YM2aMasUXIcbojpeXF8zNzVVt\nrIT2/fffAwDmzJkDwDjnbNq0acjMzOz187wqMjIysHjxYp4zE6E8nuxf+MxVT2PHjoWZmRlyc3N7\njBOqFVVZWRlu3rwJ4Hnx2bVrFyZNmoSbN28KNsbjx4+xZMkSje3K3pHKNlZCKi8vx969e+Hq6ooV\nK1YAMK45Y4yx7nBx1ZOjoyOCg4Nx4sQJpKWloba2FgUFBThw4IBanDatqLRRVlaGVatW4datW2ht\nbcWVK1dw7949TJs2TbAxrK2tcf78efzwww+ora1FW1sbrly5guXLl8Pa2hrr1q1TxerafoyIUF9f\nD4VCoeopefz4cUyfPh3m5uY4deqU6jdXY5ozxhjrloGvqBow9Lnara6ujj7++GMaMmQI2djY0IwZ\nM2jLli0EgFxdXenatWtE1HMrKm1bWpWWlpKfnx85ODiQubk5vfbaa7Rp0yZqb29/6Ri6CAwMJHd3\nd7KxsSFLS0vy9PSk0NBQun79ulqcNu3Hzpw5Q+PHjyeZTEYWFhZkZmZGAFRXBk+dOpXi4+Pp8ePH\nGvsa05zx1cK646tLTQsfTw0ZIiIig1X2AUT5mwFPB9OV8opX/v1Qe/x+My18PDVk8tfCjDHGmMC4\nuJq4W7duabR86+5vIPeYZKyrCxcuICYmRqOt4YcffqgRGxAQAFtbW5ibm2PMmDHIz883QMba06Xt\n46VLlzB9+nTIZDLI5XJER0d3ewfDy+LOnDmDPXv2mHxP4P7ExdXE+fj4gIhe+nfs2DFDp8qYVrZu\n3YqUlBRs3LhRra3hkCFDcOTIEZw9e1Yt/vz588jMzMT8+fNRWFiISZMmGShz7Wjb9rGwsBABAQHw\n9/dHVVUVsrKy8PXXX2P16tU6xwUGBkIqlcLf3x81NTX98jpNHRdXxkxAU1MT/Pz8jH6Ml9m9ezeO\nHTuGjIwM2Nraqj2WkpICMzMzhIeHG1Uj9a6UbR8dHR1hY2ODRYsWYeHChfjf//1ftSvZExIS4OLi\ngm3btsHa2hq+vr6Ijo7GoUOH1FYZ0zZuzZo1mDBhAubOnYv29vZ+fc2miIsrYyYgLS0NlZWVRj9G\nT4qLixEbG4tt27ZptEUEAD8/P0RGRuLRo0f44osvDJChMLRp+9je3o6zZ89i1qxZam0u58yZAyLC\n6dOndYpTiouLw9WrV5GcnNwnr+1VwsWVMQMgIiQlJeGNN96ApaUlHBwcsHDhQrUziYiICFhYWMDF\nxUW17dNPP4W1tTVEIhGqq6sBAJGRkVi/fj1KSkogEong5eWFlJQUSKVSODk5YdWqVZDL5ZBKpfDz\n88Ply5cFGQMQtvXgy6SkpICIEBgY+MKY7du3Y9SoUTh48CAuXLjQ4/Npcwx0aW/Yn20f79y5g/r6\neri5uanFeXp6AgAKCgp0ilNycHDArFmzkJyczFf+9lb/3vozcPF9Wkxf+tznumXLFrKwsKDDhw9T\nTU0NFRQU0KRJk2jo0KFUXl6uilu6dCk5Ozur7ZuYmEgAVK3ziIiCg4PJ09NTLS48PJysra3p5s2b\n1NzcTIWFhTRlyhSytbWl+/fvCzKGLq0HO9Pn/ebh4UGjR4/u9jFPT0+6e/cuERH9+OOPZGZmRq+/\n/jrV19cTEVF2djYtWLBAbR9tj4E27Q2J+rftY25uLgGgxMREjf2srKzI399fp7jOYmJiNNpmvgx/\nfmrI4DNXxvpZU1MTkpKS8N5772HZsmUYNGgQxo0bh/3796O6ulpjla/eEIvFqjOz0aNHIzU1FXV1\ndUhPTxfk+efNm4fa2lrExsYK8nwv0tDQgLt376rOuHri6+uLtWvXorS0FBs2bOg2Rp9j4OfnBzs7\nOzg6OiI0NBQNDQ24f/8+AKC5uRmpqakICgpCcHAw7O3tsXnzZkgkEr3nevjw4XB1dUVcXBy+/PJL\ntbV7lVf6mpuba+wnkUjQ1NSkU1xnI0eOBABcv35dr7zZc1xcGetnhYWFqK+vx+TJk9W2T5kyBRYW\nFmpf2wpt8uTJkMlkerciNJTKykoQEWQymVbx27dvh7e3N/bt24dLly5pPN7bY9C1vWFftX2srKzE\n3/72N/z1r3/Fm2++qfrNW/mbbHcXHrW2tsLKykqnuM6Uc1xRUaFX3uw5Lq6M9TPlrQ42NjYaj9nb\n26Ourq5Px7e0tERVVVWfjiG05uZmAM9z14ZUKkV6ejpEIhFWrFihcYYm9DHo77aPyt/Iu9772tjY\niObmZsjlcp3iOlMWXOWcM/1wcWWsn9nb2wNAtx/gNTU1cHV17bOx29ra+nyMvqD8wNdlkQNfX1+s\nW7cORUVFSEhIUHtM6GPQuU0idbmHPC8vT6fn6k7Xto/u7u6wtbXFvXv31OKKi4sBAOPHj9cprrPW\n1lYA6PaslmmPiytj/Wzs2LGwsbHBL7/8orb98uXLaG1txVtvvaXaJhaLVV89CiEnJwdEhGnTpvXZ\nGH3ByckJIpFI5/tXExIS4OPjgytXrqht1+UYaKO/2z6KxWLMnTsXFy9ehEKhUMVlZ2dDJBKprqjW\nNq4z5Rw7Ozv36rW86ri4MtbPpFIp1q9fj6ysLBw5cgS1tbW4fv06Vq9eDblcjvDwcFWsl5cXnjx5\nglOnTqGtrQ1VVVUaZyEAMHjwYJSVlaG0tBR1dXWqYqlQKPD06VO0t7ejoKAAkZGRcHNzQ1hYmCBj\n6Np6UF8ymQweHh54+PChTvspvx7uekGPLsdA23Fe1sIwNDQUzs7OPS6/qEvbx9jYWFRUVGDr1q1o\naGhAXl4eEhMTERYWBm9vb53jlJRzPG7cOJ3mgHVhyGuVBxK+lJzpS59bcRQKBSUmJtLIkSNJIpGQ\ng4MDBQUF0e3bt9XiHj9+TO+++y5JpVJyd3enzz//nKKioggAeXl5qW6pyc/PpxEjRpCVlRXNmDGD\nysvLKTw8nCQSCQ0bNozEYjHZ2dnRwoULqaSkRLAxtGk92B193m8REREkkUiosbFRtS0rK4s8PT0J\nAA0dOpQ+++yzbveNiorSuBVHm2OgbXtDope3MAwKCiIAtGXLlh5fp7ZtH4me32ozdepUsrS0JLlc\nTlFRUdTc3Kx3HBHRvHnzaNiwYaRQKHrMszP+/NSQwbPxO/7HwfQ1UPu5hoeH0+DBgw2dRrf0eb8V\nFRWRWCxWu9/TmHR0dNDMmTMpLS3N0Km8UHV1NUmlUvrqq6902o8/PzXwfa6MmTJT6nLi5eWF+Ph4\nxMfHq5YBNBYdHR04deoU6urqBnQHqri4OEycOBERERGGTsXocXFljBmNmJgYLFq0CKGhoUa1OH9O\nTg5OnjyJ7Oxsre/V7W9JSUm4evUqzp07B4lEYuh0jB4XV8ZM0MaNG5Geno5nz57B3d0dJ06cMHRK\ngtmxYwciIiKwa9cuQ6eiNX9/f3zzzTdqazgPJKdPn0ZLSwtycnLg4OBg6HRMgtjQCTDGhLdz507V\nggOmKCAgAAEBAYZOw2QsWLAACxYsMHQaJoXPXBljjDGBcXFljDHGBMbFlTHGGBMYF1fGGGNMYFxc\nGWOMMYHx1cJdiEQiQ6fAjBT/29EdzxkzVVxcf+fn54fjx48bOg3GupWXl4fk5GT+N8qYkRARERk6\nCcZYzzIyMrB48WLw25Uxo5DJv7kyxhhjAuPiyhhjjAmMiytjjDEmMC6ujDHGmMC4uDLGGGMC4+LK\nGGOMCYyLK2OMMSYwLq6MMcaYwLi4MsYYYwLj4soYY4wJjIsrY4wxJjAurowxxpjAuLgyxhhjAuPi\nyhhjjAmMiytjjDEmMC6ujDHGmMC4uDLGGGMC4+LKGGOMCYyLK2OMMSYwLq6MMcaYwLi4MsYYYwLj\n4soYY4wJjIsrY4wxJjAurowxxpjAuLgyxhhjAuPiyhhjjAmMiytjjDEmMC6ujDHGmMC4uDLGGGMC\n4+LKGGOMCYyLK2OMMSYwLq6MMcaYwMSGToAxpq6qqgr/8z//o7btl19+AQAcOHBAbbutrS0++OCD\nfsuNMaYdERGRoZNgjP1LS0sLnJycUF9fD3NzcwCA8m0qEolUcW1tbVi+fDkOHTpkiDQZYy+WyV8L\nMzbAWFpaIiQkBGKxGG1tbWhra0N7ezva29tV/93W1gYAWLJkiYGzZYx1h4srYwPQkiVL0Nra2mOM\nvb09/u3f/q2fMmKM6YKLK2MD0LvvvgtHR8cXPi6RSLBs2TKIxXzZBGMDERdXxgYgMzMzLF26FBKJ\npNvH29ra+EImxgYwLq6MDVAffPCB6rfVrl577TX4+vr2c0aMMW1xcWVsgJo6dSpGjBihsd3CwgLL\nly9Xu3KYMTawcHFlbAD78MMPNb4abm1t5a+EGRvguLgyNoAtXbpU46thLy8vjBs3zkAZMca0wcWV\nsQHMx8cHo0ePVn0FLJFI8Mc//tHAWTHGXoaLK2MD3EcffaRaqam9vZ2/EmbMCHBxZWyA++CDD9DR\n0QEAmDRpEtzd3Q2cEWPsZbi4MjbAubm54Q9/+AMAYPny5QbOhjGmDV7e5Xd5eXlISkoydBqMdaul\npQUikQjnz5/HxYsXDZ0OY93KzMw0dAoDBp+5/u7Bgwc4ceKEodNgRuinn37CTz/91KdjuLq6wtnZ\nGVKptE/H6S8PHz7k95sJ4eOpic9cu+D/82K6WrRoEYC+/7dTXFwMLy+vPh2jv2RkZGDx4sX8fjMR\nyuPJ/oXPXBkzEqZSWBl7FXBxZYwxxgTGxZUxxhgTGBdXxhhjTGBcXBljjDGBcXFlbIA4d+4cBg0a\nhG+//dbQqQx4Fy5cQExMDE6ePAkPDw+IRCKIRCJ8+OGHGrEBAQGwtbWFubk5xowZg/z8fANkrL09\ne/bAx8cHVlZWsLa2ho+PD2JjY1FbW6sRe+nSJUyfPh0ymQxyuRzR0dFoaWnROe7MmTPYs2ePaiUw\n1ntcXBkbIIjI0CkYha1btyIlJQUbN25EcHAw7ty5A09PTwwZMgRHjhzB2bNn1eLPnz+PzMxMzJ8/\nH4WFhZg0aZKBMtfO//3f/2HlypW4f/8+KioqkJCQgD179iAkJEQtrrCwEAEBAfD390dVVRWysrLw\n9ddfY/Xq1TrHBQYGQiqVwt/fHzU1Nf3yOk0eMSIiOn78OPF0MH2EhIRQSEiIodMQVGNjI/n6+vbZ\n8+v7ftu1axeNGjWKmpqa1LZ7enrSN998Q2ZmZjRs2DCqqalRezw7O5sWLFjQq5z7S1BQkMbrW7Ro\nEQGgsrIy1bbFixeTu7s7KRQK1bbExEQSiUT0z3/+U+c4IqKIiAjy9fWltrY2nXLmz08NGXzmyhjT\nkJaWhsrKSkOnoaa4uBixsbHYtm1btytV+fn5ITIyEo8ePcIXX3xhgAyFkZWVpfH6hg0bBgCor68H\n8Lw70tmzZzFr1ixVO0IAmDNnDogIp0+f1ilOKS4uDlevXkVycnKfvLZXCRdXxgaAS5cuwc3NDSKR\nCH/+858BAKmpqbC2toZMJsPp06cxZ84c2NnZwdXVFUePHlXtm5KSAqlUCicnJ6xatQpyuRxSqRR+\nfn64fPmyKi4iIgIWFhZwcXFRbfv0009hbW0NkUiE6upqAEBkZCTWr1+PkpISiEQi1eIV33//Pezs\n7LBjx47+mBINKSkpICIEBga+MGb79u0YNWoUDh48iAsXLvT4fESEpKQkvPHGG7C0tISDgwMWLlyI\nW7duqWK0PQYA0NHRgS1btsDNzQ1WVlYYP348jh8/3rsX/buioiLY29tjxIgRAIA7d+6gvr4ebm5u\nanGenp4AgIKCAp3ilBwcHDBr1iwkJyfzzxS9xMWVsQFgxowZ+PHHH9W2ffLJJ1i7di2amppga2uL\n48ePo6SkBB4eHli5ciXa2toAPC+aYWFhaGxsxJo1a1BaWor8/Hy0t7dj9uzZePDgAYDnxen9999X\nG2Pfvn3Ytm2b2rbk5GTMnz8fnp6eICIUFxcDgOpiF4VC0Sdz8DJnz56Ft7c3ZDLZC2OsrKxw6NAh\nmJmZYeXKlWhoaHhhbFxcHGJiYrBp0yZUVlbi4sWLePDgAWbOnImKigoA2h8DANiwYQO+/PJL7N27\nF7/99hvmz5+PJUuW4JdfftHr9ba1teHRo0f485//jAsXLuBPf/oTLCwsAADl5eUAAFtbW7V9pFIp\nrKysVPlrG9fZm2++iUePHuHatWt65c2e4+LKmBHw8/ODnZ0dHB0dERoaioaGBty/f18tRiwWq87C\nRo8ejdTUVNTV1SE9PV2QHObNm4fa2lrExsYK8ny6aGhowN27d1VnXD3x9fXF2rVrUVpaig0bNnQb\n09TUhKSkJLz33ntYtmwZBg0ahHHjxmH//v2orq7GgQMHNPbp6Rg0NzcjNTUVQUFBCA4Ohr29PTZv\n3gyJRKL3/A8fPhyurq6Ii4vDl19+qbZ2r/JKX3Nzc439JBIJmpqadIrrbOTIkQCA69ev65U3e46L\nK2NGRnn20vmsqTuTJ0+GTCZT+5rTWFVWVoKIejxr7Wz79u3w9vbGvn37cOnSJY3HCwsLUV9fj8mT\nJ6ttnzJlCiwsLNS+Tu9O12Nw+/ZtNDY2YuzYsaoYKysruLi46D3/Dx48QGVlJf72t7/hr3/9K958\n803V7+DK32Tb29s19mttbYWVlZVOcZ0p57i7s1qmPS6ujJkwS0tLVFVVGTqNXmtubgbw/PVoQyqV\nIj09HSKRCCtWrNA4Q1PebmJjY6Oxr729Perq6nTKT/n18+bNm1X33IpEIty7dw+NjY06PZeSRCKB\no6MjAgICcOzYMRQWFmLnzp0AoPrdvOu9r42NjWhuboZcLtcprjNlwVXOOdMPF1fGTFRbWxtqamrg\n6upq6FR6TfmBr8siB76+vli3bh2KioqQkJCg9pi9vT0AdFtE9ZkzR0dHAMDevXtBRGp/eXl5Oj1X\nd7y8vGBubo7CwkIAgLu7O2xtbXHv3j21OOXv4+PHj9cprrPW1lYA6PaslmmPiytjJionJwdEhGnT\npqm2icXil36dPBA5OTlBJBLh2bNnOu2XkJAAHx8fXLlyRW372LFjYWNjo3Gx0eXLl9Ha2oq33npL\np3GGDx8OqVSKq1ev6rRfV48fP8aSJUs0thcVFaGjowPDhw8H8Pw4zp07FxcvXlS7wCw7OxsikUh1\nRbW2cZ0p59jZ2blXr+VVx8WVMROhUCjw9OlTtLe3o6CgAJGRkXBzc0NYWJgqxsvLC0+ePMGpU6fQ\n1taGqqoqjbMaABg8eDDKyspQWlqKuro6tLW1ITs722C34shkMnh4eODhw4c67af8erjrBT1SqRTr\n169HVlYWjhw5gtraWly/fh2rV6+GXC5HeHi4zuP88Y9/xNGjR5Gamora2lp0dHTg4cOH+O233wAA\noaGhcHZ27nH5RWtra5w/fx4//PADamtr0dbWhitXrmD58uWwtrbGunXrVLGxsbGoqKjA1q1b0dDQ\ngLy8PCQmJiIsLAze3t46xykp53jcuHE6zQHrwjCLVww8vMII05cQKzT96U9/IhcXFwJAMpmMAgMD\nad++fSSTyQgAjRw5kkpKSujAgQNkZ2dHAGjEiBH066+/EhFReHg4SSQSGjZsGInFYrKzs6OFCxdS\nSUmJ2jiPHz+md999l6RSKbm7u9Pnn39OUVFRBIC8vLzo/v37RESUn59PI0aMICsrK5oxYwaVl5fT\nuXPnyNbWlrZv396r10qk3/stIiKCJBIJNTY2qrZlZWWRp6cnAaChQ4fSZ5991u2+UVFRGis0KRQK\nSkxMpJEjR5JEIiEHBwcKCgqi27dvq2J0OQYtLS0UHR1Nbm5uJBaLydHRkYKDg6mwsJCInq+8BIC2\nbNnS4+sMDAwkd3d3srGxIUtLS/L09KTQ0FC6fv26Rmxubi5NnTqVLC0tSS6XU1RUFDU3N+sdR0Q0\nb948GjZsmNqKTi/Dn58aMng2fsf/OJi+BsLyh+Hh4TR48GCD5qALfd5vRUVFJBaL6fDhw32UVd/q\n6OigmTNnUlpamqFTeaHq6mqSSqX01Vdf6bQff35q4OUPGTMVpt7RxMvLC/Hx8YiPj1ctA2gsOjo6\ncOrUKdTV1SE0NNTQ6bxQXFwcJk6ciIiICEOnYvS4uDLGjEZMTAwWLVqE0NBQnS9uMqScnBycPHkS\n2dnZWt+r29+SkpJw9epVnDt3DhKJxNDpGD0urgL6+OOPYWtrC5FI1OurBgeK5uZm+Pj4YPPmzTrv\n27XXpvLPwsICTk5OeOedd5CYmIinT5/2Qeavjo0bNyI9PR03FsBnAAAgAElEQVTPnj2Du7s7Tpw4\nYeiU+tSOHTsQERGBXbt2GToVrfn7++Obb75RW9d5IDl9+jRaWlqQk5MDBwcHQ6djEri4CujgwYP4\ny1/+Yug0BLVp0ybcvn1br30799ocNGgQiAgKhQKVlZXIyMiAu7s7oqOjMWbMGL3XX2XAzp070dLS\nAiLC3bt3Nfp+mqKAgADs3r3b0GmYjAULFiAmJqbbZRKZfri4shf68ccfcePGDUGfUyQSwd7eHu+8\n8w7S09ORkZGBiooKzJs3z6i+5mOMsZ5wcRVY556JxqypqQlRUVF93tcxJCQEYWFhqKysxP79+/t0\nLMYY6y9cXHuBiJCYmAhvb29YWlpi0KBBiIqK0ojrqc+jLv0ic3NzMXXqVMhkMtjZ2WHcuHGqNUOF\n7iW5adMmfPrpp6pl3boSsrencpGD7Oxs1TZjnDPGGFPi4toLsbGxiI6ORnh4OCoqKlBeXt5ti6ue\n+jxq2y+yoaEBgYGBCAkJwZMnT1BUVIRRo0ap1gEVspfkP/7xD5SUlHS7DJuSkL09J06cCOB5Y2cl\nY5szxhhTY+AbbQcMXW+CbmxsJJlMRrNnz1bbfvToUQJAV65cISKipqYmkslkFBoaqravpaUlffLJ\nJ0REtGnTJgJATU1Nqph9+/YRACouLiYiohs3bhAA+u677zRy0WYMXV7X5MmT6eHDh0REVFVVRQBo\n06ZNOj1PZ56enjRo0KAeY0QiEdnb2xOR8c3ZQFhEwtjwogOmhY+nhgyxgWq60SsuLkZjYyP8/f17\njNO3z2PXfpEeHh5wcnLCsmXLsGbNGoSFheH111/v1Rjd2bhxI/7f//t/GDZsmE779UZDQwOICHZ2\ndgCMb84A4MSJEybze3t/4jljpoqLq56Ui1u/6DdJpc59HrveK9pdL8UXsbKywg8//IANGzZgx44d\niI+Px/vvv4/09HTBxrh06RKuX7+OpKQkrfcRwq+//goA8PHxAWBcc6Y0bdo0rF27Vuf9XlV5eXlI\nTk7m37hNhPJ4sn/h4qonqVQKAGhpaekxrnOfx8jIyF6NOWbMGHz77beoqqpCUlISdu/ejTFjxqiW\nU+vtGGlpafj73/8OMzPNn+J37NiBHTt24Oeff8bkyZP1HqM733//PQBgzpw5AIxrzpRcXV3x/vvv\n9/p5XiXJyck8ZyaEi6s6vqBJT2PHjoWZmRlyc3N7jBOqz2NZWRlu3rwJ4Hnx2bVrFyZNmoSbN28K\nNkZ6erpGo+eqqioAz68eJiLBC2t5eTn27t0LV1dXrFixAoBxzRljjHWHi6ueHB0dERwcjBMnTiAt\nLQ21tbUoKCjAgQMH1OK06fOojbKyMqxatQq3bt1Ca2srrly5gnv37mHatGmCjaELXXt7EhHq6+uh\nUChURfv48eOYPn06zM3NcerUKdVvrqY6Z4yxV4jhLqYaWPS52q2uro4+/vhjGjJkCNnY2NCMGTNo\ny5YtBIBcXV3p2rVrRNRzn0dt+0WWlpaSn58fOTg4kLm5Ob322mu0adMmam9vf+kYvfGiq4W16e15\n5swZGj9+PMlkMrKwsCAzMzMCoLoyeOrUqRQfH0+PHz/W2NeY5oyvFtYdX11qWvh4asgQEREZqrAP\nJBkZGVi8eDF4OpiuFi1aBADIzMw0cCbGg99vpoWPp4ZM/lqYMcYYExgXVxN369YtjZZv3f0N5AbO\njHV14cIFxMTEaLQ1/PDDDzViAwICYGtrC3Nzc4wZMwb5+fkGyFh3CoUCe/fuhZ+f3wtjLl26hOnT\np0Mmk0EulyM6OrrbOxheFnfmzBns2bNHtfIa6z0uribOx8dH4wrg7v6OHTtm6FQZ08rWrVuRkpKC\njRs3qrU1HDJkCI4cOYKzZ8+qxZ8/fx6ZmZmYP38+CgsLMWnSJANlrr2ioiK8/fbbWLduHRobG7uN\nKSwsREBAAPz9/VFVVYWsrCx8/fXXWL16tc5xgYGBkEql8Pf3R01NTZ++tlcFF1fGTEBTU1OPZzjG\nMsbL7N69G8eOHUNGRgZsbW3VHktJSYGZmRnCw8ONun3htWvXsGHDBqxevVq17nZ3EhIS4OLigm3b\ntsHa2hq+vr6Ijo7GoUOH1FYZ0zZuzZo1mDBhAubOnYv29vY+fY2vAi6ujJmAtLQ0VFZWGv0YPSku\nLkZsbCy2bdumWsSlMz8/P0RGRuLRo0f44osvDJChMCZMmICTJ09i6dKlsLS07Damvb0dZ8+exaxZ\ns9SWkJwzZw6ICKdPn9YpTikuLg5Xr17lBSEEwMWVMQMgIiQlJeGNN96ApaUlHBwcsHDhQrUziYiI\nCFhYWMDFxUW17dNPP4W1tTVEIhGqq6sBAJGRkVi/fj1KSkogEong5eWFlJQUSKVSODk5YdWqVZDL\n5ZBKpfDz88Ply5cFGQMQtvXgy6SkpICIEBgY+MKY7du3Y9SoUTh48CAuXLjQ4/Npcwx0aW/Yny0M\n79y5g/r6eri5ualt9/T0BAAUFBToFKfk4OCAWbNmITk5ma/87SUurowZQFxcHGJiYrBp0yZUVlbi\n4sWLePDgAWbOnImKigoAz4tJ1+UB9+3bh23btqltS05Oxvz58+Hp6QkiQnFxMSIiIhAWFobGxkas\nWbMGpaWlyM/PR3t7O2bPno0HDx70egxA2NaDL3P27Fl4e3tDJpO9MMbKygqHDh2CmZkZVq5cqVpD\nujvaHANt2xsC/dvCsLy8HAA0vhqXSqWwsrJS5a9tXGdvvvkmHj16hGvXrgme96uEiytj/aypqQlJ\nSUl47733sGzZMgwaNAjjxo3D/v37UV1drbHKV2+IxWLVmdno0aORmpqKuro6pKenC/L88+bNQ21t\nLWJjYwV5vhdpaGjA3bt3VWdcPfH19cXatWtRWlrabX9lQL9j4OfnBzs7Ozg6OiI0NBQNDQ24f/8+\nAKC5uRmpqakICgpCcHAw7O3tsXnzZkgkEsHmujPllb7m5uYaj0kkEjQ1NekU19nIkSMBANevXxcs\n31cRF1fG+llhYSHq6+s11mmeMmUKLCws1L62FdrkyZMhk8n0aqtnSJWVlSCiHs9aO9u+fTu8vb2x\nb98+XLp0SePx3h6Dru0NhW5h+DLK35y7u/CotbUVVlZWOsV1ppzj7s5qmfa4uDLWz5S3OtjY2Gg8\nZm9vj7q6uj4d39LSUtWQwVg0NzcDwAsv8OlKKpUiPT0dIpEIK1as0DhDE/oYdG5h2Pn+8Xv37r3w\nVpreUP5GXltbq7a9sbERzc3NqraJ2sZ1piy4yjln+uHiylg/s7e3B4BuP8Bramrg6uraZ2O3tbX1\n+Rh9QfmBr8siB76+vli3bh2KioqQkJCg9pjQx6Bzm8Su95Dn5eXp9FzacHd3h62tLe7du6e2Xflb\n+Pjx43WK66y1tRUAuj2rZdrj4spYPxs7dixsbGw0LnS5fPkyWltb8dZbb6m2icVitYtmeisnJwdE\nhGnTpvXZGH3ByckJIpFI5/tXExIS4OPjgytXrqht1+UYaKO/WxiKxWLMnTsXFy9eVLuYLDs7GyKR\nSHVFtbZxnSnn2NnZuY9fhWnj4spYP5NKpVi/fj2ysrJw5MgR1NbW4vr161i9ejXkcjnCw8NVsV5e\nXnjy5AlOnTqFtrY2VFVVaZyFAMDgwYNRVlaG0tJS1NXVqYqlQqHA06dP0d7ejoKCAkRGRsLNzQ1h\nYWGCjKFr60F9yWQyeHh44OHDhzrtp/x6uOsFPbocA23HeVkLw9DQUDg7Owu2/GJsbCwqKiqwdetW\nNDQ0IC8vD4mJiQgLC4O3t7fOcUrKOR43bpwgeb6y+qf7zsDHLZOYvvRpOadQKCgxMZFGjhxJEomE\nHBwcKCgoiG7fvq0W9/jxY3r33XdJKpWSu7s7ff755xQVFUUAyMvLi+7fv09ERPn5+TRixAiysrKi\nGTNmUHl5OYWHh5NEIqFhw4aRWCwmOzs7WrhwIZWUlAg2hjatB7ujz/stIiKCJBIJNTY2qrZlZWWR\np6cnAaChQ4fSZ5991u2+UVFRtGDBArVt2hwDbdsbEr28hWFQUBABoC1btvT4OvPy8mj69Okkl8sJ\nAAEgFxcX8vPzo9zcXLXY3Nxcmjp1KllaWpJcLqeoqChqbm7WeE5t44iI5s2bR8OGDSOFQtFjnp3x\n56eGDJ6N3/E/DqavgdrPNTw8nAYPHmzoNLqlz/utqKiIxGIxHT58uI+y6lsdHR00c+ZMSktLM3Qq\nL1RdXU1SqZS++uornfbjz08NGfy1MGMmzJS6nHh5eSE+Ph7x8fGor683dDo66ejowKlTp1BXVzeg\nO1DFxcVh4sSJiIiIMHQqRo+LK2PMaMTExGDRokUIDQ01qsX5c3JycPLkSWRnZ2t9r25/S0pKwtWr\nV3Hu3DlIJBJDp2P0uLgyZoI2btyI9PR0PHv2DO7u7jhx4oShUxLMjh07EBERgV27dhk6Fa35+/vj\nm2++UVvDeSA5ffo0WlpakJOTAwcHB0OnYxLEhk6AMSa8nTt3YufOnYZOo88EBAQgICDA0GmYjAUL\nFmDBggWGTsOk8JkrY4wxJjAurowxxpjAuLgyxhhjAuPiyhhjjAmML2jqIiMjw9ApMCOjXC6O/+1o\nT7mYPc+ZaeiL5gTGTkREZOgkBoKMjAwsXrzY0GkwxpjR4nKiksnFlTEjoPyfP367MmYUMvk3V8YY\nY0xgXFwZY4wxgXFxZYwxxgTGxZUxxhgTGBdXxhhjTGBcXBljjDGBcXFljDHGBMbFlTHGGBMYF1fG\nGGNMYFxcGWOMMYFxcWWMMcYExsWVMcYYExgXV8YYY0xgXFwZY4wxgXFxZYwxxgTGxZUxxhgTGBdX\nxhhjTGBcXBljjDGBcXFljDHGBMbFlTHGGBMYF1fGGGNMYFxcGWOMMYFxcWWMMcYExsWVMcYYExgX\nV8YYY0xgXFwZY4wxgXFxZYwxxgTGxZUxxhgTGBdXxhhjTGBcXBljjDGBcXFljDHGBMbFlTHGGBMY\nF1fGGGNMYGJDJ8AYU/fw4UMsX74cHR0dqm1Pnz6Fra0t3nnnHbVYb29v/Pd//3c/Z8gYexkurowN\nMK6urrh37x5KSko0HsvNzVX777fffru/0mKM6YC/FmZsAProo48gkUheGhcaGtoP2TDGdMXFlbEB\naOnSpWhvb+8xZsyYMRg9enQ/ZcQY0wUXV8YGIE9PT4wfPx4ikajbxyUSCZYvX97PWTHGtMXFlbEB\n6qOPPoK5uXm3j7W3t2PRokX9nBFjTFtcXBkboD744AMoFAqN7WZmZpg2bRpef/31/k+KMaYVLq6M\nDVByuRzTp0+HmZn629TMzAwfffSRgbJijGmDiytjA9iHH36osY2I8N577xkgG8aYtri4MjaAhYSE\nqP3uam5ujn//93+Hk5OTAbNijL0MF1fGBjAHBwfMnj1bVWCJCMuWLTNwVoyxl+HiytgAt2zZMtWF\nTRKJBAsXLjRwRoyxl+HiytgAFxgYCEtLSwDA/PnzYWNjY+CMGGMvw8WVsQHO2tpadbbKXwkzZhxE\nRESGTmIgyMjIwOLFiw2dBmOMGS0uJyqZ3BWni+PHjxs6BWZk9u7dCwBYu3Ztn43R0dGB48ePY8mS\nJX02Rn/Ky8tDcnIyv99MhPJ4sn/h4trF+++/b+gUmJHJzMwE0Pf/doKCgiCVSvt0jP6UnJzM7zcT\nwsVVHf/mypiRMKXCypip4+LKGGOMCYyLK2OMMSYwLq6MMcaYwLi4MsYYYwLj4srYAHHu3DkMGjQI\n3377raFTGfAuXLiAmJgYnDx5Eh4eHhCJRBCJRN12EQoICICtrS3Mzc0xZswY5OfnGyBj3SkUCuzd\nuxd+fn4vjLl06RKmT58OmUwGuVyO6OhotLS06Bx35swZ7NmzBx0dHX3yWl5FXFwZGyD4BnztbN26\nFSkpKdi4cSOCg4Nx584deHp6YsiQIThy5AjOnj2rFn/+/HlkZmZi/vz5KCwsxKRJkwyUufaKiorw\n9ttvY926dWhsbOw2prCwEAEBAfD390dVVRWysrLw9ddfY/Xq1TrHBQYGQiqVwt/fHzU1NX362l4V\nXFwZGyDmzZuHZ8+eYf78+YZOBU1NTT2eMRnK7t27cezYMWRkZMDW1lbtsZSUFJiZmSE8PBzPnj0z\nUIa9d+3aNWzYsAGrV6/GxIkTXxiXkJAAFxcXbNu2DdbW1vD19UV0dDQOHTqEW7du6Ry3Zs0aTJgw\nAXPnzkV7e3ufvsZXARdXxpiGtLQ0VFZWGjoNNcXFxYiNjcW2bdu6vefXz88PkZGRePToEb744gsD\nZCiMCRMm4OTJk1i6dKmqYUNX7e3tOHv2LGbNmgWRSKTaPmfOHBARTp8+rVOcUlxcHK5evcoLQgiA\niytjA8ClS5fg5uYGkUiEP//5zwCA1NRUWFtbQyaT4fTp05gzZw7s7Ozg6uqKo0ePqvZNSUmBVCqF\nk5MTVq1aBblcDqlUCj8/P1y+fFkVFxERAQsLC7i4uKi2ffrpp7C2toZIJEJ1dTUAIDIyEuvXr0dJ\nSQlEIhG8vLwAAN9//z3s7OywY8eO/pgSDSkpKSAiBAYGvjBm+/btGDVqFA4ePIgLFy70+HxEhKSk\nJLzxxhuwtLSEg4MDFi5cqHY2p+0xAJ4vUbllyxa4ubnBysoK48eP77PlHe/cuYP6+nq4ubmpbff0\n9AQAFBQU6BSn5ODggFmzZiE5OZl/puglLq6MDQAzZszAjz/+qLbtk08+wdq1a9HU1ARbW1scP34c\nJSUl8PDwwMqVK9HW1gbgedEMCwtDY2Mj1qxZg9LSUuTn56O9vR2zZ8/GgwcPADwvTl2XG9y3bx+2\nbdumti05ORnz58+Hp6cniAjFxcUAoLrYRdlbtr+dPXsW3t7ekMlkL4yxsrLCoUOHYGZmhpUrV6Kh\noeGFsXFxcYiJicGmTZtQWVmJixcv4sGDB5g5cyYqKioAaH8MAGDDhg348ssvsXfvXvz222+YP38+\nlixZgl9++UW4SfhdeXk5AGh8NS6VSmFlZaXKX9u4zt588008evQI165dEzzvVwkXV8aMgJ+fH+zs\n7ODo6IjQ0FA0NDTg/v37ajFisVh1FjZ69Gikpqairq4O6enpguQwb9481NbWIjY2VpDn00VDQwPu\n3r2rOuPqia+vL9auXYvS0lJs2LCh25impiYkJSXhvffew7JlyzBo0CCMGzcO+/fvR3V1NQ4cOKCx\nT0/HoLm5GampqQgKCkJwcDDs7e2xefNmSCQSwea/M+WVvubm5hqPSSQSNDU16RTX2ciRIwEA169f\nFyzfVxEXV8aMjIWFBQConTV1Z/LkyZDJZGpfcxqryspKEFGPZ62dbd++Hd7e3ti3bx8uXbqk8Xhh\nYSHq6+sxefJkte1TpkyBhYWF2tfp3el6DG7fvo3GxkaMHTtWFWNlZQUXF5c+mX/lb87dXXjU2toK\nKysrneI6U85xd2e1THtcXBkzYZaWlqiqqjJ0Gr3W3NwMAC+8wKcrqVSK9PR0iEQirFixQuMMTXm7\niY2Njca+9vb2qKur0yk/5dfPmzdvVt1zKxKJcO/evRfeStMbyt/Na2tr1bY3NjaiubkZcrlcp7jO\nlAVXOedMP1xcGTNRbW1tqKmpgaurq6FT6TXlB74uixz4+vpi3bp1KCoqQkJCgtpj9vb2ANBtEdVn\nzhwdHQE87+1LRGp/eXl5Oj2XNtzd3WFra4t79+6pbVf+Pj5+/Hid4jprbW0FgG7Papn2uLgyZqJy\ncnJARJg2bZpqm1gsfunXyQORk5MTRCKRzvevJiQkwMfHB1euXFHbPnbsWNjY2GhcbHT58mW0trbi\nrbfe0mmc4cOHQyqV4urVqzrtpy+xWIy5c+fi4sWLaheYZWdnQyQSqa6o1jauM+UcOzs79/GrMG1c\nXBkzEQqFAk+fPkV7ezsKCgoQGRkJNzc3hIWFqWK8vLzw5MkTnDp1Cm1tbaiqqtI4qwGAwYMHo6ys\nDKWlpairq0NbWxuys7MNdiuOTCaDh4cHHj58qNN+yq+Hu17QI5VKsX79emRlZeHIkSOora3F9evX\nsXr1asjlcoSHh+s8zh//+EccPXoUqampqK2tRUdHBx4+fIjffvsNABAaGgpnZ2fBll+MjY1FRUUF\ntm7dioaGBuTl5SExMRFhYWHw9vbWOU5JOcfjxo0TJM9XFjEiIjp+/DjxdDB9hISEUEhISK+e409/\n+hO5uLgQAJLJZBQYGEj79u0jmUxGAGjkyJFUUlJCBw4cIDs7OwJAI0aMoF9//ZWIiMLDw0kikdCw\nYcNILBaTnZ0dLVy4kEpKStTGefz4Mb377rsklUrJ3d2dPv/8c4qKiiIA5OXlRffv3yciovz8fBox\nYgRZWVnRjBkzqLy8nM6dO0e2tra0ffv2Xr1WIv3ebxERESSRSKixsVG1LSsrizw9PQkADR06lD77\n7LNu942KiqIFCxaobVMoFJSYmEgjR44kiURCDg4OFBQURLdv31bF6HIMWlpaKDo6mtzc3EgsFpOj\noyMFBwdTYWEhEREFBQURANqyZUuPrzMvL4+mT59OcrmcABAAcnFxIT8/P8rNzVWLzc3NpalTp5Kl\npSXJ5XKKioqi5uZmjefUNo6IaN68eTRs2DBSKBQ95tkZf35qyODZ+B3/42D6EqK49lZ4eDgNHjzY\noDnoQp/3W1FREYnFYjp8+HAfZdW3Ojo6aObMmZSWlmboVF6ourqapFIpffXVVzrtx5+fGjL4a2HG\nTISpdzTx8vJCfHw84uPjUV9fb+h0dNLR0YFTp06hrq4OoaGhhk7nheLi4jBx4kREREQYOhWjx8VV\nQB9//DFsbW0hEon67cIGoW3fvl3tVgLlX+f797TVtR2Y8s/CwgJOTk545513kJiYiKdPn/bBK2Gm\nKCYmBosWLUJoaKhRLc6fk5ODkydPIjs7W+t7dftbUlISrl69inPnzkEikRg6HaPHxVVABw8exF/+\n8hdDpzFgdG4HNmjQIBARFAoFKisrkZGRAXd3d0RHR2PMmDF9skTcq2Ljxo1IT0/Hs2fP4O7ujhMn\nThg6pT61Y8cOREREYNeuXYZORWv+/v745ptv1NZ1HkhOnz6NlpYW5OTkwMHBwdDpmAQurkzD4cOH\nNe7Vu3HjhiDPLRKJYG9vj3feeQfp6enIyMhARUWFqt0a093OnTvR0tICIsLdu3cREhJi6JT6XEBA\nAHbv3m3oNEzGggULEBMT0+0yiUw/XFwF1rmtE3u5kJAQhIWFobKyEvv37zd0OowxJggurr1AREhM\nTIS3tzcsLS0xaNAgREVFacT11IpKl5ZWubm5mDp1KmQyGezs7DBu3DjVsmb92e4KELb9mPI+zOzs\nbNU2U5wzxtirg4trL8TGxiI6Ohrh4eGoqKhAeXl5t104empFpW1Lq4aGBgQGBiIkJARPnjxBUVER\nRo0apVqqTMh2VzExMXBwcICFhQXc3d2xcOFC/Pzzz2oxQrYfmzhxIoDnvSeVjG3OGGNMjQHvAxpQ\ndL1Pq7GxkWQyGc2ePVtt+9GjRwkAXblyhYiImpqaSCaTUWhoqNq+lpaW9MknnxAR0aZNmwgANTU1\nqWL27dtHAKi4uJiIiG7cuEEA6LvvvtPIRZsxtHX//n3Kz8+nuro6amlpoby8PHrzzTfJysqKbty4\nodNzKXl6etKgQYN6jBGJRGRvb09ExjdnA+E+V2PD90WaFj6eGjLEhirqxq64uBiNjY3w9/fvMU7f\nVlRdW1p5eHjAyckJy5Ytw5o1axAWFobXX3+9V2N0Z/jw4Rg+fLjqv6dNm4b09HRMnDgR+/btQ2pq\nqk7Pp42GhgYQEezs7AAY35wBz5eMy8jI0Hm/V5VyMXueM9PQF80JjJ6hy/tAoev/eZ07d44AaKy2\n0vXM9R//+IdqCbOuf9OmTSOi7s/C/vKXvxAA+uc//6naduPGDfrP//xPEovFJBKJaPHixdTY2KjV\nGL3R0dFB5ubm5O/vr9f+Lztzzc/PJwAUEBBARMY3ZyEhIS98Lv7jv1fpj6nwCk36UjYhbmlp6TFO\nyFZUY8aMwbfffouysjJER0fj+PHj+Oqrr/q83ZVCoYBCodC6l6auvv/+ewDAnDlzABjnnIWEhGg8\nD/+9+E954Zih8+A/YY8n+xcurnoaO3YszMzMkJub22OcUK2oysrKcPPmTQDPi8+uXbswadIk3Lx5\nU9B2V//xH/+hse3nn38GEcHX17fXz99VeXk59u7dC1dXV6xYsQKA8c0ZY4x1xcVVT46OjggODsaJ\nEyeQlpaG2tpaFBQU4MCBA2px2rSi0kZZWRlWrVqFW7duobW1FVeuXMG9e/cwbdo0wcYAgEePHuHY\nsWOoqalBW1sb8vLy8PHHH8PNzQ2rV69WxenafoyIUF9fD4VCASJCVVUVjh8/junTp8Pc3BynTp1S\n/eZqbHPGGGMaiBGRfle71dXV0ccff0xDhgwhGxsbmjFjBm3ZsoUAkKurK127do2Iem5FpW1Lq9LS\nUvLz8yMHBwcyNzen1157jTZt2kTt7e0vHUMX69evJ09PT7K2tiaxWEyurq60cuVKKisrU4vTpv3Y\nmTNnaPz48SSTycjCwoLMzMwIgOrK4KlTp1J8fDw9fvxYY19jmjO+Wlh3fHWpaeHjqSFDRERksMo+\ngGRkZGDx4sXg6WC6WrRoEQAgMzPTwJkYD36/mRY+nhoy+WthxhhjTGBcXE3crVu3um0h1/VvIPeY\nZIwxY8PF1cT5+PhodSn9sWPHDJ0qY71y4cIFxMTEaPQR/vDDDzViAwICYGtrC3Nzc4wZMwb5+fkG\nyFh3CoUCe/fuhZ+f3wtjLl26hOnTp0Mmk0EulyM6OlrtlsEzZ85gz549qiVMWd/g4soYM3pbt25F\nSkoKNm7cqNZHeMiQIThy5AjOnj2rFn/+/HlkZmZi/vz5KCwsxKRJkwyUufaKiorw9ttvY926dWhs\nbOw2prCwEAEBAfD390dVVRWysrLw9ddfq13pHxgYCMxuKo4AAAhMSURBVKlUCn9/f9TU1PRX+q8c\nLq6MmYCmpqYez2aMZQx97N69G8eOHUNGRgZsbW3VHktJSYGZmRnCw8ONul/wtWvXsGHDBqxevVrV\n6KI7CQkJcHFxwbZt22BtbQ1fX19ER0fj0KFDast6rlmzBhMmTMDcuXPR3t7eHy/hlcPFlTETkJaW\nhsrKSqMfQ1fFxcWIjY3Ftm3bVKumdebn54fIyEg8evQIX3zxhQEyFMaECRNw8uRJLF269IUrpbW3\nt+Ps2bOYNWuWWl/pOXPmgIhw+vRptfi4uDhcvXoVycnJfZr7q4qLK2MGQERISkrCG2+8AUtLSzg4\nOGDhwoVqZxcRERGwsLCAi4uLatunn34Ka2triEQiVFdXAwAiIyOxfv16lJSUQCQSwcvLCykpKZBK\npXBycsKqVasgl8shlUrh5+eHy5cvCzIGIGxfX32kpKSAiBAYGPjCmO3bt2PUqFE4ePAgLly40OPz\naXNcdOkn3J89g+/cuYP6+nq4ubmpbff09AQAFBQUqG13cHDArFmzkJyczLfQ9IV+vrF2wOKboJm+\n9FlEYsuWLWRhYUGHDx+mmpoaKigooEmTJtHQoUOpvLxcFbd06VJydnZW2zcxMZEAUFVVlWpbcHAw\neXp6qsWFh4eTtbU13bx5k5qbm6mwsJCmTJlCtra2dP/+fUHG+O6778jW1pbi4+N1ev1Cvd88PDxo\n9OjR3T7m6elJd+/eJSKiH3/8kczMzOj111+n+vp6IiLKzs6mBQsWqO2j7XFRNo74+9//Ts+ePaPK\nykqaOXMmWVtbU2trqyruiy++IEtLSzpx4gQ9ffqUNm7cSGZmZvTzzz/r/Zr/8Ic/0IQJEzS25+bm\nEgBKTEzUeMzKyqrbxhsxMTEE/KvRiL7481MDL9zPWH9rampCUlIS3nvvPSxbtgyDBg3CuHHjsH//\nflRXV2ssodkbYrFYdRY2evRopKamoq6uDunp6YI8/7x581BbW4vY2FhBnk8XDQ0NuHv3rurMrCe+\nvr5Yu3YtSktLsWHDhm5j9Dkufn5+sLOzg6OjI0JDQ9HQ0ID79+8DAJqbm5GamoqgoCAEBwfD3t4e\nmzdvhkQiEWz+O1NeEWxubq7xmEQiQVNTk8b2kSNHAgCuX78ueD6vOi6ujPWzwsJC1NfXY/LkyWrb\np0yZAgsLC7WvbYU2efJkyGQyvXrWDjSVlZUgIshkMq3it2/fDm9vb+zbtw+XLl3SeLy3x6VrP2Gh\newa/jPI35+4uUGptbYWVlZXGduXcVVRUCJ7Pq46LK2P9THn7g42NjcZj9vb2qKur69PxLS0tUVVV\n1adj9Ifm5mYA0LoVolQqRXp6OkQiEVasWKFxJif0cWloaAAAbN68WW3Blnv37r3wVpreUP5uXltb\nq7a9sbERzc3NkMvlGvsoC65yLplwuLgy1s/s7e0BoNsP65qaGri6uvbZ2G1tbX0+Rn9RFgZdFkPw\n9fXFunXrUFRUhISEBLXHhD4ufd1nuSt3d3fY2tri3r17atuLi4sBAOPHj9fYp7W1FQC6PatlvcPF\nlbF+NnbsWNjY2OCXX35R23758mW0trbirbfeUm0Ti8WqrxmFkJOTAyLCtGnT+myM/uLk5ASRSKTz\n/asJCQnw8fHBlStX1Lbrcly00d89g8ViMebOnYuLFy9CoVCotmdnZ0MkEnV7RbVy7pydnfslx1cJ\nF1fG+plUKsX69euRlZWFI0eOoLa2FtevX8fq1ashl8sRHh6uivXy8sKTJ09w6tQptLW1oaqqSuPM\nBAAGDx6MsrIylJaWoq6uTlUsFQoFnj59ivb2dhQUFCAyMhJubm4ICwsTZAxd+/oKSSaTwcPDAw8f\nPtRpP+XXw10v/NHluGg7zst6BoeGhsLZ2Vmw5RdjY2NRUVGBrVu3oqGhAXl5eUhMTERYWBi8vb01\n4pVzN27cOEHGZ50Y8lrlgYQvJWf60udWHIVCQYmJiTRy5EiSSCTk4OBAQUFBdPv2bbW4x48f07vv\nvktSqZTc3d3p888/p6ioKAJAXl5eqltq8vPzacSIEWRlZUUzZsyg8vJyCg8PJ4lEQsOGDSOxWEx2\ndna0cOFCKikpEWwMbfr6dkeo91tERARJJBJqbGxUbcvKyiJPT08CQEOHDqXPPvus232joqI0bsXR\n5rho20+Y6OU9g4OCgggAbdmypcfXmZeXR9OnTye5XE4ACAC5uLiQn58f5ebmqsXm5ubS1KlTydLS\nkuRyOUVFRVFzc3O3zztv3jwaNmwYKRSKHsd/Gf781JDBs/E7/sfB9DVQm6WHh4fT4MGDDZ1Gt4R6\nvxUVFZFYLKbDhw8LkFX/6+jooJkzZ1JaWlq/j11dXU1SqZS++uqrXj8Xf35q4PtcGTNlpt75xMvL\nC/Hx8YiPj0d9fb2h09FJR0cHTp06hbq6OoO0fIyLi8PEiRMRERHR72O/Cri4MsaMWkxMDBYtWoTQ\n0FCjWpw/JycHJ0+eRHZ2ttb36golKSkJV69exblz5yCRSPp17FcFF1fGTNDGjRuRnp6OZ8+ewd3d\nHSdOnDB0Sn1qx44diIiIwK5duwyditb8/f3xzTffqK3r3B9Onz6NlpYW5OTkwMHBoV/HfpWIDZ0A\nY0x4O3fuxM6dOw2dRr8KCAhAQECAodMY8BYsWIAFCxYYOg2Tx2eujDHGmMC4uDLGGGMC4+LKGGOM\nCYyLK2OMMSYwvqCpi0WLFhk6BWZkfvrpJwD8b0cXymX3eM5Mg65LUL4KREREhk5iIMjLy0NSUpKh\n02CMMaOVmZlp6BQGikwurowxxpiwMvk3V8YYY0xgXFwZY4wxgXFxZYwxxgTGxZUxxhgT2P8HhHJV\nlhCT6XoAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "metadata": {
        "id": "wFHmZr5DRPTl",
        "colab_type": "code",
        "outputId": "439a2336-dc58-4d85-a058-cd3999a10a31",
        "colab": {
          "resources": {
            "http://localhost:8080/my_mnist_model.png": {
              "data": "CjwhRE9DVFlQRSBodG1sPgo8aHRtbCBsYW5nPWVuPgogIDxtZXRhIGNoYXJzZXQ9dXRmLTg+CiAgPG1ldGEgbmFtZT12aWV3cG9ydCBjb250ZW50PSJpbml0aWFsLXNjYWxlPTEsIG1pbmltdW0tc2NhbGU9MSwgd2lkdGg9ZGV2aWNlLXdpZHRoIj4KICA8dGl0bGU+RXJyb3IgNDA0IChOb3QgRm91bmQpISExPC90aXRsZT4KICA8c3R5bGU+CiAgICAqe21hcmdpbjowO3BhZGRpbmc6MH1odG1sLGNvZGV7Zm9udDoxNXB4LzIycHggYXJpYWwsc2Fucy1zZXJpZn1odG1se2JhY2tncm91bmQ6I2ZmZjtjb2xvcjojMjIyO3BhZGRpbmc6MTVweH1ib2R5e21hcmdpbjo3JSBhdXRvIDA7bWF4LXdpZHRoOjM5MHB4O21pbi1oZWlnaHQ6MTgwcHg7cGFkZGluZzozMHB4IDAgMTVweH0qID4gYm9keXtiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9lcnJvcnMvcm9ib3QucG5nKSAxMDAlIDVweCBuby1yZXBlYXQ7cGFkZGluZy1yaWdodDoyMDVweH1we21hcmdpbjoxMXB4IDAgMjJweDtvdmVyZmxvdzpoaWRkZW59aW5ze2NvbG9yOiM3Nzc7dGV4dC1kZWNvcmF0aW9uOm5vbmV9YSBpbWd7Ym9yZGVyOjB9QG1lZGlhIHNjcmVlbiBhbmQgKG1heC13aWR0aDo3NzJweCl7Ym9keXtiYWNrZ3JvdW5kOm5vbmU7bWFyZ2luLXRvcDowO21heC13aWR0aDpub25lO3BhZGRpbmctcmlnaHQ6MH19I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LnBuZykgbm8tcmVwZWF0O21hcmdpbi1sZWZ0Oi01cHh9QG1lZGlhIG9ubHkgc2NyZWVuIGFuZCAobWluLXJlc29sdXRpb246MTkyZHBpKXsjbG9nb3tiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSBuby1yZXBlYXQgMCUgMCUvMTAwJSAxMDAlOy1tb3otYm9yZGVyLWltYWdlOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSAwfX1AbWVkaWEgb25seSBzY3JlZW4gYW5kICgtd2Via2l0LW1pbi1kZXZpY2UtcGl4ZWwtcmF0aW86Mil7I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LTJ4LnBuZykgbm8tcmVwZWF0Oy13ZWJraXQtYmFja2dyb3VuZC1zaXplOjEwMCUgMTAwJX19I2xvZ297ZGlzcGxheTppbmxpbmUtYmxvY2s7aGVpZ2h0OjU0cHg7d2lkdGg6MTUwcHh9CiAgPC9zdHlsZT4KICA8YSBocmVmPS8vd3d3Lmdvb2dsZS5jb20vPjxzcGFuIGlkPWxvZ28gYXJpYS1sYWJlbD1Hb29nbGU+PC9zcGFuPjwvYT4KICA8cD48Yj40MDQuPC9iPiA8aW5zPlRoYXTigJlzIGFuIGVycm9yLjwvaW5zPgogIDxwPiAgPGlucz5UaGF04oCZcyBhbGwgd2Uga25vdy48L2lucz4K",
              "ok": false,
              "headers": [
                [
                  "content-length",
                  "1449"
                ],
                [
                  "content-type",
                  "text/html; charset=utf-8"
                ]
              ],
              "status": 404,
              "status_text": "Not Found"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "cell_type": "code",
      "source": [
        "%%html\n",
        "<img src=\"my_mnist_model.png\" />"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"my_mnist_model.png\" />"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "THHCfHWqRPTm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Warning**: at the present, you need `from tensorflow.python.keras.utils.vis_utils import model_to_dot`, instead of simply `keras.utils.model_to_dot`. See [TensorFlow issue 24639](https://github.com/tensorflow/tensorflow/issues/24639)."
      ]
    },
    {
      "metadata": {
        "id": "huHWYxxeRPTn",
        "colab_type": "code",
        "outputId": "76150314-413d-454b-8a70-e4738998b6a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 536
        }
      },
      "cell_type": "code",
      "source": [
        "from IPython.display import SVG\n",
        "from tensorflow.python.keras.utils.vis_utils import model_to_dot\n",
        "SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.SVG object>"
            ],
            "image/svg+xml": "<svg height=\"387pt\" viewBox=\"0.00 0.00 353.00 387.00\" width=\"353pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 383)\">\n<title>G</title>\n<polygon fill=\"#ffffff\" points=\"-4,4 -4,-383 349,-383 349,4 -4,4\" stroke=\"transparent\"/>\n<!-- 139704343440856 -->\n<g class=\"node\" id=\"node1\">\n<title>139704343440856</title>\n<polygon fill=\"none\" points=\"0,-332.5 0,-378.5 345,-378.5 345,-332.5 0,-332.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"88\" y=\"-351.8\">flatten_1_input: InputLayer</text>\n<polyline fill=\"none\" points=\"176,-332.5 176,-378.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"205\" y=\"-363.3\">input:</text>\n<polyline fill=\"none\" points=\"176,-355.5 234,-355.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"205\" y=\"-340.3\">output:</text>\n<polyline fill=\"none\" points=\"234,-332.5 234,-378.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"289.5\" y=\"-363.3\">[(None, 28, 28)]</text>\n<polyline fill=\"none\" points=\"234,-355.5 345,-355.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"289.5\" y=\"-340.3\">[(None, 28, 28)]</text>\n</g>\n<!-- 139704397582632 -->\n<g class=\"node\" id=\"node2\">\n<title>139704397582632</title>\n<polygon fill=\"none\" points=\"36,-249.5 36,-295.5 309,-295.5 309,-249.5 36,-249.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"92.5\" y=\"-268.8\">flatten_1: Flatten</text>\n<polyline fill=\"none\" points=\"149,-249.5 149,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"178\" y=\"-280.3\">input:</text>\n<polyline fill=\"none\" points=\"149,-272.5 207,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"178\" y=\"-257.3\">output:</text>\n<polyline fill=\"none\" points=\"207,-249.5 207,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"258\" y=\"-280.3\">(None, 28, 28)</text>\n<polyline fill=\"none\" points=\"207,-272.5 309,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"258\" y=\"-257.3\">(None, 784)</text>\n</g>\n<!-- 139704343440856&#45;&gt;139704397582632 -->\n<g class=\"edge\" id=\"edge1\">\n<title>139704343440856-&gt;139704397582632</title>\n<path d=\"M172.5,-332.3799C172.5,-324.1745 172.5,-314.7679 172.5,-305.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"176.0001,-305.784 172.5,-295.784 169.0001,-305.784 176.0001,-305.784\" stroke=\"#000000\"/>\n</g>\n<!-- 139704397585040 -->\n<g class=\"node\" id=\"node3\">\n<title>139704397585040</title>\n<polygon fill=\"none\" points=\"46.5,-166.5 46.5,-212.5 298.5,-212.5 298.5,-166.5 46.5,-166.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"100\" y=\"-185.8\">dense_3: Dense</text>\n<polyline fill=\"none\" points=\"153.5,-166.5 153.5,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"182.5\" y=\"-197.3\">input:</text>\n<polyline fill=\"none\" points=\"153.5,-189.5 211.5,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"182.5\" y=\"-174.3\">output:</text>\n<polyline fill=\"none\" points=\"211.5,-166.5 211.5,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"255\" y=\"-197.3\">(None, 784)</text>\n<polyline fill=\"none\" points=\"211.5,-189.5 298.5,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"255\" y=\"-174.3\">(None, 300)</text>\n</g>\n<!-- 139704397582632&#45;&gt;139704397585040 -->\n<g class=\"edge\" id=\"edge2\">\n<title>139704397582632-&gt;139704397585040</title>\n<path d=\"M172.5,-249.3799C172.5,-241.1745 172.5,-231.7679 172.5,-222.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"176.0001,-222.784 172.5,-212.784 169.0001,-222.784 176.0001,-222.784\" stroke=\"#000000\"/>\n</g>\n<!-- 139704343804336 -->\n<g class=\"node\" id=\"node4\">\n<title>139704343804336</title>\n<polygon fill=\"none\" points=\"46.5,-83.5 46.5,-129.5 298.5,-129.5 298.5,-83.5 46.5,-83.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"100\" y=\"-102.8\">dense_4: Dense</text>\n<polyline fill=\"none\" points=\"153.5,-83.5 153.5,-129.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"182.5\" y=\"-114.3\">input:</text>\n<polyline fill=\"none\" points=\"153.5,-106.5 211.5,-106.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"182.5\" y=\"-91.3\">output:</text>\n<polyline fill=\"none\" points=\"211.5,-83.5 211.5,-129.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"255\" y=\"-114.3\">(None, 300)</text>\n<polyline fill=\"none\" points=\"211.5,-106.5 298.5,-106.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"255\" y=\"-91.3\">(None, 100)</text>\n</g>\n<!-- 139704397585040&#45;&gt;139704343804336 -->\n<g class=\"edge\" id=\"edge3\">\n<title>139704397585040-&gt;139704343804336</title>\n<path d=\"M172.5,-166.3799C172.5,-158.1745 172.5,-148.7679 172.5,-139.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"176.0001,-139.784 172.5,-129.784 169.0001,-139.784 176.0001,-139.784\" stroke=\"#000000\"/>\n</g>\n<!-- 139704343438112 -->\n<g class=\"node\" id=\"node5\">\n<title>139704343438112</title>\n<polygon fill=\"none\" points=\"46.5,-.5 46.5,-46.5 298.5,-46.5 298.5,-.5 46.5,-.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"100\" y=\"-19.8\">dense_5: Dense</text>\n<polyline fill=\"none\" points=\"153.5,-.5 153.5,-46.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"182.5\" y=\"-31.3\">input:</text>\n<polyline fill=\"none\" points=\"153.5,-23.5 211.5,-23.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"182.5\" y=\"-8.3\">output:</text>\n<polyline fill=\"none\" points=\"211.5,-.5 211.5,-46.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"255\" y=\"-31.3\">(None, 100)</text>\n<polyline fill=\"none\" points=\"211.5,-23.5 298.5,-23.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"255\" y=\"-8.3\">(None, 10)</text>\n</g>\n<!-- 139704343804336&#45;&gt;139704343438112 -->\n<g class=\"edge\" id=\"edge4\">\n<title>139704343804336-&gt;139704343438112</title>\n<path d=\"M172.5,-83.3799C172.5,-75.1745 172.5,-65.7679 172.5,-56.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"176.0001,-56.784 172.5,-46.784 169.0001,-56.784 176.0001,-56.784\" stroke=\"#000000\"/>\n</g>\n</g>\n</svg>"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "metadata": {
        "id": "y_MwSMbORPTo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.4)\n",
        "After a model is created, you must call its `compile()` method to specify the `loss` function and the `optimizer` to use. In this case, you want to use the `\"sparse_categorical_crossentropy\"` loss, and the `\"sgd\"` optimizer (stochastic gradient descent). Moreover, you can optionally specify a list of additional metrics that should be measured during training. In this case you should specify `metrics=[\"accuracy\"]`. **Note**: you can find more loss functions in `keras.losses`, more metrics in `keras.metrics` and more optimizers in `keras.optimizers`."
      ]
    },
    {
      "metadata": {
        "id": "NMV_ag8YRPTp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=\"sgd\", metrics=[\"accuracy\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zjKiJ4HqRPTs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.5)\n",
        "Now your model is ready to be trained. Call its `fit()` method, passing it the input features (`X_train`) and the target classes (`y_train`). Set `epochs=10` (or else it will just run for a single epoch). You can also (optionally) pass the validation data by setting `validation_data=(X_valid, y_valid)`. If you do, Keras will compute the loss and the additional metrics (the accuracy in this case) on the validation set at the end of each epoch. If the performance on the training set is much better than on the validation set, your model is probably overfitting the training set (or there is a bug, such as a mismatch between the training set and the validation set).\n",
        "**Note**: the `fit()` method will return a `History` object containing training stats. Make sure to preserve it (`history = model.fit(...)`)."
      ]
    },
    {
      "metadata": {
        "id": "F1X4npqFRPTs",
        "colab_type": "code",
        "outputId": "1f1866c4-969c-4038-d164-f4fb06dc2888",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        }
      },
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train, y_train, epochs=10,\n",
        "                    validation_data=(X_valid, y_valid))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 55000 samples, validate on 5000 samples\n",
            "Epoch 1/10\n",
            "55000/55000 [==============================] - 7s 136us/sample - loss: 3.1418 - accuracy: 0.6447 - val_loss: 0.7972 - val_accuracy: 0.6984\n",
            "Epoch 2/10\n",
            "55000/55000 [==============================] - 7s 128us/sample - loss: 0.7410 - accuracy: 0.7047 - val_loss: 0.7100 - val_accuracy: 0.7260\n",
            "Epoch 3/10\n",
            "55000/55000 [==============================] - 7s 131us/sample - loss: 0.6764 - accuracy: 0.7204 - val_loss: 0.7126 - val_accuracy: 0.7294\n",
            "Epoch 4/10\n",
            "55000/55000 [==============================] - 7s 131us/sample - loss: 0.6409 - accuracy: 0.7299 - val_loss: 0.7127 - val_accuracy: 0.7338\n",
            "Epoch 5/10\n",
            "55000/55000 [==============================] - 7s 127us/sample - loss: 0.6118 - accuracy: 0.7382 - val_loss: 0.7025 - val_accuracy: 0.7160\n",
            "Epoch 6/10\n",
            "55000/55000 [==============================] - 7s 131us/sample - loss: 0.5909 - accuracy: 0.7439 - val_loss: 0.6374 - val_accuracy: 0.7478\n",
            "Epoch 7/10\n",
            "55000/55000 [==============================] - 7s 134us/sample - loss: 0.5633 - accuracy: 0.7676 - val_loss: 0.5995 - val_accuracy: 0.7916\n",
            "Epoch 8/10\n",
            "55000/55000 [==============================] - 7s 132us/sample - loss: 0.5269 - accuracy: 0.7981 - val_loss: 0.5834 - val_accuracy: 0.8044\n",
            "Epoch 9/10\n",
            "55000/55000 [==============================] - 7s 132us/sample - loss: 0.5063 - accuracy: 0.8059 - val_loss: 0.5690 - val_accuracy: 0.8052\n",
            "Epoch 10/10\n",
            "55000/55000 [==============================] - 7s 133us/sample - loss: 0.4783 - accuracy: 0.8232 - val_loss: 0.5509 - val_accuracy: 0.8224\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "f8RWZjDkRPTt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.6)\n",
        "Try running `pd.DataFrame(history.history).plot()` to plot the learning curves. To make the graph more readable, you can also set `figsize=(8, 5)`, call `plt.grid(True)` and `plt.gca().set_ylim(0, 1)`."
      ]
    },
    {
      "metadata": {
        "id": "yho2UR-bRPTt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plot_learning_curves(history):\n",
        "    pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
        "    plt.grid(True)\n",
        "    plt.gca().set_ylim(0, 1)\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fWuNoI8ZRPTu",
        "colab_type": "code",
        "outputId": "886f2ae7-2c8f-4098-ed9a-ea96e2164ad1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        }
      },
      "cell_type": "code",
      "source": [
        "plot_learning_curves(history)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeEAAAEzCAYAAAAGvLsqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzs3XeYJFdh7/1vdc4zPTlvVu3sSlpl\naQVoEWCMjbEwQjLXYL+84AcT5AdkGxuuwVyCsQky4eolgzEYBLYxXO4F27oIWwiUVqu8oVbaNLOT\nc+iemU71/lE9PXl3VjvTPeH3eZ5+uruquvrs2Z7+9Tl16pRh2zYiIiJSfK5SF0BERGSzUgiLiIiU\niEJYRESkRBTCIiIiJaIQFhERKRGFsIiISIl4lrORaZqXAv8L+IxlWXfPW/cK4ONAFvipZVkfXfFS\nioiIbEDnbQmbphkG/idw3xKbfB64FXgR8ErTNPesXPFEREQ2ruV0R08Bvwl0zl9hmuZ2YNCyrHbL\nsnLAT4GXr2wRRURENqbzhrBlWRnLsiaWWF0H9M163gvUr0TBRERENrplHRO+AMb5NshksrbH417h\nt12besf7ueMnH+Smrddzx/VvLnVxRESkdBbNx4sN4U6c1vC0Rhbptp5taCh5kW85V3V1lL6+sRXd\n50qZzOQAGBofXbNlvBBrua43EtVzcaiei0P17Kiuji66/KJOUbIs6zQQM01zq2maHuC3gHsvZp8b\nScAdwGW4SKSX6s0XEZHN7LwtYdM0rwbuArYCadM0Xw/8GDhlWdYPgXcA9+Q3/75lWcdXqazrjmEY\nBD0BkumVbf2LiMjGcN4QtizrEPDSc6z/BbB/Bcu0oYQ9IZIZtYRFRGQhzZi1ykLeEMl0El23WURE\n5lMIr7KQJ0jGzpLKpUtdFBERWWMUwqss5A0C6LiwiIgsoBBeZWFvCEDHhUVEZAGF8CoLeZyWcEIt\nYRERmUchvMpCagmLiMgSFMKrLOzJh7BawiIiMo9CeJVND8xSd7SIiMynEF5lIY+6o0VEZHEK4VUW\n1ilKIiKyBIXwKgvmW8IJtYRFRGQehfAq02QdIiKyFIXwKvO6PPjcPh0TFhGRBRTCRRD2hNQSFhGR\nBRTCRRDyBkmk1RIWEZG5FMJFEPIEmcxOks1lS10UERFZQxTCRTB9EYeJzGSJSyIiImuJQrgIChdx\nyOi4sIiIzFAIF0HhIg4anCUiIrMohIsgrKkrRURkEQrhIgjqIg4iIrIIhXARhAvd0WoJi4jIDIVw\nEUwPzEpqYJaIiMyiEC6Cmfmj1RIWEZEZCuEiCBeupKSWsIiIzFAIF0FIx4RFRGQRCuEiCLj9uAyX\njgmLiMgcCuEiMAyDkEcXcRARkbkUwkUS8gY1Y5aIiMyhEC6SkCdEMjOBbdulLoqIiKwRCuEiCXmD\nZO0sU9lUqYsiIiJrhEK4SGbmj1aXtIiIOBTCRaLTlEREZD6FcJFo6koREZlPIVwk0xdx0GlKIiIy\nTSFcJGoJi4jIfArhItFFHEREZD6FcJHMdEerJSwiIg6FcJGEdIqSiIjMoxAuEnVHi4jIfArhIpke\nmJXIKIRFRMShEC4Sj8uD3+3TRRxERKTAU+oCbCbTF3EQEZHSyWRzjIynGBpJMtLdz3hfP1MDg6SH\nhrBHh8Hj4xV/8hYCfu+ql0UhXEQhb5CBicFSF0NEZEOybZvEZIbB0UlG+oYY6+ljom+Q1NAg2ZFh\njLERvBNjBKYSRDJJwtlJokB03n6yhovM+O3gj696mRXCRRT2hOjIdpHNZXG73KUujojIupFKZxke\nn2JocIzR7n4S+dZrZmQYRkdwJ0bxTY4TSSeJZJP47Rz+JfaVcXlIBSMkwjUQK8NTHidQWUG4uopY\nXRXBhgY8sVhR/l0K4SIqjJDOTBD1RUpcGhGR0svlbMaSKQZHkoz0DDDe289E/yCZoSFyo8O4xkfx\nJscIphNEM0kCuTRlQNli+8Ig5Q8zGavFjpThLivHX1lBsKqCWG01oZpKvPE4rmAIwzCK/U9d1LJC\n2DTNzwA3ADbwbsuyDs5a9y7gTUAWeMyyrPesRkE3gsK5wumkQlhENizbtkmlc0yms0xkRzl1/Cxj\nPf0k+wZIDQyRHR2GsRE8yTECk+OE813DIWxCS+wz5fGTjpQxFo7hKivHG48TqKogUl1FtK4Kf0Ul\n7lgMw7W+xhufN4RN0zwA7LIsa79pmq3AN4D9+XUx4L3ATsuyMqZp3mua5g2WZT28qqVepwqzZmlw\nlojMYts29tQk2bFxsuNjZBPjiz7OTU2BbedvADZ24bk9vbOZZQB2DmznPQDI5QrbOa8HO5cjl3Ne\nZ+dscrmc8zj/3LZz2Dm7sB979j7zNzu/P2xn34ZTQDx2Fq+dJQ4sdoQ163IzFYiQCFdBtAxPeTn+\nigrCNVXE6qoJVlXiKS/H5fOtVvWX1HJawi8HfgRgWdZR0zTjpmnGLMsaBVL5W8Q0zXEgBGjk0RJm\nJuzQaUoiG5UTqFNOgI6Pz7rPPx4bd4J1fJzsmLMulxjHzmRW5v0xwJj9eDoOnef29HIoPJ6z3phZ\nx+ztjdnLZr3ecGEYBobLeS9j+uZy7nNeL5PhGK6yMnwVFQSrKonWVhGprcJXHscVDq+ZruFSWE4I\n1wGHZj3vyy8btSxr0jTNDwMngQnge5ZlHT/XzuLxEB7PygxKGhlK8oNvH6L18npaL6t3PgRrWO2o\n8zvQHbSprp4/Hm99WK/lXm9Uz8Vxvnq2bZvc5CTpsTEyo2OkR0dJj46RGXMez9yPO/f5bZYbqO5w\nCG80hqe2Gm80iicWwxuL4o5EGcdLf9pNZxLaxnKcHEzTk7DnBOJMGOa/+84TZi6XQdDvIehzE/B7\nCPg9BH0eAn43QZ+HYMBDYNbzgN9D0O/OL5u1rX9mO7/XvalD9GK9kIFZhdrOd0f/d+ASYBT4uWma\n+yzLemqpFw8NrVwrsLdrlKNPd3H4yU6qaiNcf2A7zdvia/YDkZ10jlV0Dw7SFx4rcWkuXHV1lL6+\n9Vfu9Ub1vPrSA/0EJ0YY7Oid1TIdm+n2ndVSXW6guoJB3JEI/uYW3JEIrkgEdySKu3Afzt/nl4XD\nGB4Pyck07b3jc24dVoJ0JgfMvHdFLMTOrUGCfg9+n5uA143f54RgwDd3WcDnnvfcg9/rxuM2Vub7\nMZcjPZkiPQnn+6Tq8+xY6gffckK4E6flO60B6Mo/bgVOWpbVD2Ca5gPA1cCSIbySaupjvOPPX8p/\n/K/DPH+0l5/809PUN5dx/YHt1DctNnautMKzBmaJSPFkx8ZIWkdJHj1C8sgR0n2959x+OlB9Tc1O\naEbzQRoO447ODtZIPlAjGJ5zf53mbJu+4Qnae8ZpPzFAe+8Z2nvHGRidnLOdx23QUBWmpSZKc02E\n5poITTURIsHVnzhCim85IXwv8GHgy6ZpXgV0WpY1/bPmNNBqmmbQsqwJ4Brgp6tS0iVUVkf4tVv2\ncOUNzTz6i1OcOTHIj/7xCbbsqOC6m7ZTVbt2RiHPPkVJRFZPbmqKieeOO6F79AhT7W2FgUquYJDw\nFVdSucdk0uWbCdNoFHfYCdXzBer5TKWynO2b27pt7xtnKpWds10s7GPvtopC2DbXRKirCOFxr68R\nvvLCnfeTZlnWg6ZpHjJN80EgB7zLNM03AyOWZf3QNM1PAf9pmmYGeNCyrAdWt8iLq6qN8pu3XU7X\n2REeuf8kZ04McubEIDtba7j2JVspr1hq4HvxTJ+ilNCVlERWlJ3NMnn6VCF0J088X+hGNjweguZu\nQrtbCe3ZS2DLVgy3e0W6SW3bZmhsirbecdp7xgqB2zs0URjsBOAyDOorQzNhWxuhuSZKWXhjjviV\n5VvWzz3Lst43b9FTs9Z9GfjyShbqYtQ3lXHL711B+6khHrn/JM8f7eXEsV52X17PNS/aQiQWKFnZ\nwoWWsLqjRS6GbdukujoLoTthHSM3kf9xaxj4m1sI7dlLqHUPwZ27cPmXmjtp+dKZLJ39Sdp6nbA9\nmw/cxOTcY8bhgAezpZymfOC21ERpqArhXaEBqbKxbMgZswzDoGV7Bc3b4py0+nj0F6c4+lQXx5/t\n5tKrGrlyfwvBUPF/gfrdflyGS9cUFnkB0oOD+dA9TPLoUbIjw4V13ppaotfdQKh1D6HdrbgjF3cY\naiSRoj0ftu09Tth2DSTJ2TPtWwOoiQdp3RLPt3CjtNRGiEf9a3ZwqKw9GzKEpxmGwY7dNWy7pArr\nmR4e+9Vpnjp4liNPdbHvumb2XduEz1+8KjAMg5AnqJawyDJkEwmS1rFC8Ka7uwvr3NGYE7p79hBq\n3YO3suoFvUcmm6N7MLlgdPJoIjVnO7/XzfaG2Jxjt43VYQK+Df0VKkWwKT5BLpeL1n31XLK3lsNP\ndHLooTM89svTPHvoLFft38LeKxvweIvTVRT2hkhodLTIArl0isnnn585rnv6VGEwleH3E758H6Hd\newjt2YOvsemCWpvJyTQ9QxP0DCXpHZygZ2iC3pEJznSNksnac7atjAW4YmfVnOO31eVBXGrdyirY\nFCE8ze1xcfm1TbTuq+Ppxzp48pE2Hvz5CZ462M41L9qKeVkd7lUelRjyBOmbGMC2bXVZyaZm53JM\ntZ0pnDY08fxx7HTaWel2E9y5y+lebt1LYNu2845YTkym6R2aoGcw6dwPTd9PMD6RXrC91+OiqToy\np3XbXBMhFNCpQFI8myqEp3l9Hq6+0WkBP/lIG8881sH9/36cJx9p59qXbGVna82qBWTIGyJn55jK\nThHwlG6QmEix2bZNureH5JH8cd1jx8glE4X1vqZmwq17CLbuIXSJiSuw8O8jMZmmZ3CC3qGk05ot\n3C8etG6XQVV5kO0NMWriQWrjIWrjQWoqQuzeXsXgYGLBa0SKaVOG8LRA0MsNL93BZVc3cejBMxx9\nqouf/fgoTzzcxvU3badlR8WKh/Hs05QUwrLRZUaG893LR0kePUxmcGZqeU9lJZGrrnaO6+7eU7h+\n6/hEmtODE/QMDc9t0Q4mF4xEhiWCtiJITTxEZcyPe4mr6qx2r5fIcmzqEJ4Wjvq56dcv4Yrrmzn4\nwGmOH+7hp//yDHVNMa6/aTsNLeUr9l6hWacpVS56TRGR9Ss3OUHSspyW7pEjpDo7Cutc4TCRa64l\n1LoHtl1CvzvMqeFJp/v4/vZCC3epoK0uD7KjsYzaeMgJ22UErchapxCeJVYe5OWvaeWK/Oxbp58b\n4H9990mat8W5/sB2qusuflL9sGf6Sko6TUnWv+z4OFPtbSSPWySPHGby1MnCpfIMrw927CbRsI2e\nsibaXGX0DE3S+0SSxEPWgn1NB+3OxjJqCq1Zp2VboaCVDUohvIjK6gi/cetl9HSO8sj9J2k/NUT7\nqUNsN6u57qatxCvDL3jfocI1hTVCWtaPTDbLRN8AE2fOMNXWRrr9DJnOdhgeKmxjGwaj5XWcjTRi\nuas54a4ga7idmea7poBe3C6DmrgTtLUVoULI1sSDVMYCuNb4ldBEVppC+BxqG2L89n+7grOnndm3\nTlp9nDreh3lZHde8aCvRsgs/phvKt4QHJnTZZXlhcjmbVCZLKpMjk8mRyuRIpbOk84/TmSypdI50\nNucsm7UulcmSzq9LpfPbZvLbTa/LZAmOD1E+3ktFop+qiQFqJgcJ5abmlGPcHaAn1ECPv4IufxVt\nwTqm3D48bqdFe9l0t3Hc6TaujQepUNCKzKEQXoamrXEat1zF6ef6eeQXpzj2dDfHD/ew98oGrtq/\nhdAFzP9aEXCOA//oxE851PsUN9ZfxzW1VxSOFcvGYNs2U+ksU6ksk4VbhonU9LJMYfn857YBiWTa\nCcVCQM6EazZnn78Ay+S2s1RNDVObGqR5apC61BDVU4N4c3OPyyYCMbpjzYyX1zIRr2WysgEjGsPr\ndRH3uGkJefltBa3IBTNse+X+oJejr29sRd+w2NeqzOVsnjvSw8EHTjM2MonH62Lftc3su64J/zLO\nL7Rtm8MDx/hl5yMcHjhGzs7hdXm5suYybqy/jp3l29bs+cPFqGs7m8XOZLDTaexsJv84g51JO48L\nt3R++azns7fN78cVCuGJx/HGK/CUx3GXl+PyLvx/upDQnExlzrvdVCrLxXzQDQN8Hjdejwuf14XX\n48bnceHzuPB6Zp57vdPL3LPWuZzX5tcV9pNN4Rnoxt3TCT1nsTvPku3pguysK/u4XPjqG/C3tBBo\n3oK/pQV/Swvu0As/BLNW6Tq3xaF6dlRXRxf9YlcIv0DZbI6jT3Vx6FdnSCZS+AMerryhhUuvbsS7\nzNm3RqZGeaTrEL/qepT+iQEAaoJV7G+4luvrrqHMf/EDwabZtg3ZrBNO2Wz+cSYfVossm36emX6e\nIRr0MjI0tkgwpp19zAnGeaGZnh+is4I0O7MNRfg8TnmDJHxhEt4wo+4gI64gQ0aQMU+ocEu5Lmxu\ncZ/HRcA36+Lq+ceB2RdY93kIzns+f7uAz01DfTlDg+MX9WMsMzrKVNsZptrOMNnWxlT7GdK9vXPq\n1/D58Dc14W/egr9lC4GWFnyNTbh8m+PKPgqH4lA9OxTCqySdyvLs4x088XAbU5MZQhEfV9+4hdZ9\n9QvOQ8yOjzPV2UG6p5tcKjUTipkMA4l+zo6cpX+8Fzubw2NDlbecmkAFZZ7IrG0XC8vZYZpZJECz\nc1s7peJ2Y3g8hRtuD7bbje3ykHO7yRlusoaLbP4+Y7hJ4yJjG6RsFynbYCrn3FI5g8mcQRrXnNdk\nDTeZ/H3OcBHIThHNJJ1b1rmPZJLEMkm89sJTYaZlPT4y4RjZcAw7WoYRK8ddXo4nHsdXUYm/soJg\nvIxgwIff51rRkbsX8pm2bZvMQL8TtG2nmWprY7LtDNnh4TnbuUIhJ2ibW/C3OKHrq6vDcG/eK/uU\n+rtjs1A9OxTCq2xqMs2Tj7bz9MGzZNI5ImE3l9alaUh1kOrqJNXZQXZ0dGXf1O3GyN+cx57Cc8Pt\nBs+859MheI5tDLcnv93M/qb3HYtHGJtIk8FN2jZI4QRjKmswmYPJnMFk1mAiCxMZmMjaTKQNxtM2\nibTNRDrHxFSGyakMqUzugv+5BhDwewj53QT8HoJ+DyG/04oM+T2FZUGf27nPPw7kt/F7Z1qdhgG5\niSSZoaH8bZDM8LBzPzREemiIzPAQufHxc9a/p7wcT76r2xuP44nHC889FXE8ZeUXfIH4pT7TdjZL\nqrt7poXb3sZUW9ucWacAPPE4/llhG2hpwVNZtWYPc5TKWvnu2OhUz46lQlgDsy7CdMs2lb81dHYS\n7ernhHcbHbbJwyfchKci7Bgcpy7gJXzZ5fgaGvHV1+MKBGcFXP7mmRt6XVN9PNF/mCcGD5PMpci6\nYEfFDq5vuJZ9dZfj86xMt2E6k2M0kWIkkWIkMcXI+PTjFCPjU4wkUowmUkxMpUlOZrBZugW5FK/H\nVQjGiqh/JiT9+cD0zX2+WKj6fe4VnUTfHQrjDoXxNzYtuU0ulXLCeXioENCZfEA7z4eZPHmicG7s\nou8TjeXDeTqg88EdnwluV2DuwLxcKsXU2bNMtZ/Jh24bU2fbZ+ZWzvPW1uYvVO8Err+5pTDzlIis\nfWoJL0MhbDs6SHV1MNXZSaqjg+zYwpatp6oKf0Mj6aomrKkaTvXa2DbUNES5/qbtNG298FmyUtkU\nT/Q+w0NdB3lu+CTgnOp0bd1V3Fh/LU3RhgWvsW2bxGSmEKIzwbowZBeboWg2t8sgFvZRFvHjdRuz\nAnReq3N+oAZm1ns28BSBdi5HdnSE9ODQEmHt3Nup1JL7cAWDTuu5vBwSYyTPdswNdrcbf0NjvnXb\nQqBlC/7m5gXhLcunFlpxqJ4d6o5ehuzYGFNdTsBOdea7kc8Ttk7LtgF/YyO+uvoFk84P9Sd49IFT\nnLT6Aed0p2tfspV4ZYhcziaXs7Hz9wse2wuX9yeGeKbH4uTISaZyKQzbIGiXEcnU4Z6MMzVpMznl\njNC1bRsDpxsXjMLj6ZvXbeCfPerW7cLjduF1GbjdBh6XgcswcBlOHng9LtKZJY4tn+N/9bz/4ed8\n7XlefZ7VHq8bv9+DL+CZe+934w948Pk9c+69Ps+qnF5j2za5ZDLf7T1EJh/Y6Xxrenp5LpHAFQjg\na2om0DLTpexvaLzgbm05N4VDcaieHQrhWbJjY/lu5M5Z3cmdi4att6oaX0MDvobGfOg24KtvwOX3\nX9B79naN8ugvTtF+auj8G69Rhss45+jlizrmeI6XnnevS72vbZPNXvjHzed34/Png3lOcHsWCW73\nnOc+v+eiLgyQS6WoqY/TP6Cr+6w2hUNxqJ4dm/KYcGZslFSnMyhqKn+f6uwgO7bwA+Gtqiawbd+s\nsM0fu73AsF1KTX2M37jtMh56tJ2jT3Yxlco6MxrlZzXK4TTq5t/I33s9Lue0Fr/HGVwUcAYjBQMe\nXL4sA5lO2ifOMJYZByNHLBCltfISWit3EfaFcLkMXC4DI3+/4LGxcNns54ZhrMs/plwuR2oqS2oq\nw9Skc0tNZWaeT2VI5ZdNP56+Hx+dYnDqwsPQ43UtCO+FLfH5gT7TYjc0R7LIprEhQjgzNpo/XttZ\naOEuGraGgbeqisC27U7YNjbiq1/ZsJ3Ptm3aesZ56HA3jxzpYSQxc1zQ53FRFvFRFvZTEfYRi/go\nzx97jYV9lIV9lEf8REPeZRxTvZScneP54ZP8qvNRnux7lrMTz3Nfh4vLqvZwY/217Kk0cRmb6wve\n5XIRCLoIBF/Yhdpt254T4gvCetbj+QE/kUgxPJC84FOfa+qj7NpbyyV7a19wuUVkfVjX3dFTHWfp\n+vxnSA0MzF2RD9s5x2sb8sdsVyls5xsYmeThI908dLiHzn6nNRUOeLiutZYb9tbSVB3JnyqzOqeN\nJNNJHu15ggc7H6VjvAuAcn8ZN9Rfw/76a6kKVlzwPtdjS7jUbNsmk84taGVPzQrrOcGdTNN9doRc\nzsbtNti+u5rWy+tpaCnXKUYrTJ/n4lA9OzbkMeFUVyf93/p77FA4343cUPSwnS05meExq5eHnu3G\nancmS/C4XVyxs5L9e+u4bEdl0UcJ27ZN+1gHv+p6lMe6n2QyOwmAGd/JjQ3Xsa9qL1738lpb+mMq\njlDAx4P3P8+Rp7oYGXQueVkWD9K6rx7z0lpCkeJ/tjcifZ6LQ/Xs2JAhDKX/D85kczx7cpAHD3fz\n5HP9ZLLOaSWXNJdz46V1XGNWE1rGnNLFMH2q0686H+XEyCkAwp4Q19ZdyY0N19EYqT/n60td15vF\ndD3btk3X2RGOPtnFCauPbCaHy2WwZWclrfvqad5WoQslXAR9notD9exQCK8g27Y52TnKQ4e7efRo\nL+MTzgQK9ZUh9u+t44a9tVSVre3zN3sSvTzU9RgPdz3GWNqZFWpLrJkb66/l6torCHoWXqZRf0zF\nsVg9T02mee5wL0ee6mSgN394I+pn9+V1tF5e/4Iuq7nZ6fNcHKpnh0J4BfQOJXnocA8PHe6md8jp\nJoyFvFy/p479l9aypTa67o7bZXNZnhk4ykOdj3J4wMLGxufyclXtPm6sv47tZVsK/yb9MRXHuerZ\ntm36usc4+lQXzx3pJZ1yzttu3handV8DW3dVXtQpUpuJPs/FoXp2KIRfoPGJNAeP9vDg4W5OdDjn\nEfs8Lq66pJr9l9axZ2t8RSfvL6WhyWEe6T7Eg50HGZgcBKA2VMONDddyfd3VbG+s1x9TESz3M51O\nZThxrI8jT3XRk/9sBkJezEvraN1XT7wytNpFXdcUDsWhenYohC9AOpPlqecHeOhwN0+fGCCbc2ae\nat0aZ//eOq66pJqgf0Oc3bWonJ3j+NAJHuo6yJO9z5Cxs7gMFzsrttIYbGBrrJmtZS1UBirWXct/\nPXghn+nBvgRHn+rCerabqfw0pPVNZbReUc8OsxrPMi+vuZkoHIpD9exQCJ9HzrZ5rn2Yhw53c/BY\nHxNTzhdZc02E/XvruH5PLfHo5huVmkgnOdj9BI92P077eAc5e2Y+44g37ARyrIUtsWa2xpoJedX6\nulgX85nOZnKcPN7H0ae66DjjjND3+d1csreW1n0NVNVGVrKo65rCoThUzw6F8BI6+xM8dLibhw/3\nMDDqnL5THvFxw946btxbR1ONvrSmxeJ+njhlcXq0LX9rZ3By7jScNaEqtsZa8rdmGiP1eFwbt9dg\nNazUl9bI0ATHnu7i2DPdJMedSWKq66K07qtn154afBu4N2c5FA7FoXp2KIRnGUmkePSIc5z3TLfz\nWr/PzTVmNfv31rG7Ja5TPxaxWF2PpsY4M9rO6REnlE+PthfORQbwuDw0RxpmtZZbqAqqG/tcVvpL\nK5fLcebEIEef7KLt5AC27UytuXN3Da1X1FPbENuU/x8Kh+JQPTs2fQhPpbM8cbyPhw73cPjUIDnb\nxmUYXLq9gv1767hiVxV+HTc7p+XUdc7O0ZvsL7SUT4+20THetaAbe7r7eku+xRxWN3bBan5pjY9N\nYT3dxdGnuxkbcX4sxatC+YlA6jbVNJkKh+JQPTs2ZQjncjZH24Z46NluDh3vYyp/Ose2+ig37K3j\n+tZaYmHfShZnQ3uhf0ypbJr2sQ7OzArmgfnd2MEqJ5DLnHBuijRs2m7sYnxp2bZNx5khjjzZxanj\n/eRyNi63wfZLqmndV0/jlo0/TabCoThUz45NFcJtPWM8fLiHh490M5w/FlZVFuCGvXXs31tLfWV4\nJYuwaazkH1OhGzvflX1mrJ2JzKxubMNNU7RxzsCv6mDlhg8GKP6X1kQyxfFnezjyVBfDA0kAYuUB\ndl9ez+7L6ghv0AGJCofiUD07NnwID45O8sgRZyKNs33OjEIhv4drW2vYv7eOnU1luDbBF/hqWs0/\nptnd2GfyreWz87qxw95Q4biyE8xNRLwb7wdVqb60bNumu2OUo092cuJYH5lMDsOALTucaTJbdlTg\n2iDnxIPCoVhUz44NGcI52+Zw2wj//uApjp0ZwgbcLoPLd1Ry46V1XL6jEq9Hx3lXSrH/mFLZNGfH\nOwqt5dOj7YVJRAplClbOCuVmmqINeNd5N/Za+NKamszw3JEejj7VRX+PM61pOOLDzE+TGStf29Oy\nLsdaqOfNQPXs2JAh/PSJfj5BkDOPAAAgAElEQVT7z08DsLOpjP1767h2dw2RTTS4pJjWwh/TWGp8\nVmvZuU1kJgrrPYabxmhD4RSp5mgjNcEq3K7182NsLdTzbDPTZPaQmnLGVTRtjdO6r55tu6pwe9Zn\n63it1fNGpXp2bMgQzmRzHOsYpSbmp2YD/DJf69biH1POztGX7C8EstON3TmnG9tjuKkN19AQrqMh\nUkdDuI7GSD3l/rI1eYx5LdYzQDqV5YTlTATSfXYEgEDQi3lpLTtaa6iqjayreavXaj1vNKpnx4YM\nYdB/cDGtl7pOZ9O0j3dyZrSds+OddI330JXoJpVLz9ku6AnSEK6lIVI/J6BD3tL+oFsP9TzUn+Do\n011Yz/Qwmb+KmMfjoro+Sl1TGXWNMeoay9b0KU/roZ43AtWzQyEsF20913XOztE/MUhnopvO8S46\nx7vpTHTTm+zHZu5HstxfRkOkjsZwfSGYa8M1RTvWvJ7qOZvJcfr5Ac6eGaLn7AgD+UGR08orQ4VA\nrmuKUV4RWjO9D+upntcz1bNDISwXbSPWdSqbpjvZUwjlznHnNpIanbOdy3BRE6zKh7ITzo2ROioC\ncVzGynbBrud6Tk1l6OkcpfvsCN0do/R0jhYutwjgD3gKgVzXWEZ1fRRviSbJWc/1vJ6onh1LhfD6\nHkYqcpF8bi8t0SZaok1zlo+nE3SNd9MxK5i7Et10J3t5nKdnvd7ndGXP6s5uiNQR9W3OOcd9fg/N\n2ypo3lYBOBPmDPYl6O4YcW5nRzlzYoAzJwYAcLkMqmoj1DbGqG8qo7axjMgGPS9ZZDFqCcuybfa6\ntm2bwcmhmRZz/r472TtnIBhA1BeZ053dEKmjPlyLz33+Gdo2ej0nxqfoPjtKT4fTWu7rHiOXm/la\niMT8c44rV9aEV+X85I1ez2uF6tmh7mi5aKrrxWVyGXqSfbOCuYuO8W6GpobnbGdgUBWsmDMQrDFc\nR3Woak6X9mar50w6S1/3GN0dM93Y04O9wLnYRG3DzHHl2oYY/sDFD/jabPVcKqpnh0JYLprq+sJM\nZCboSvTQMT7dcnYGhCVnndcM4HV5qAvXFoJ5V10LnlSAykCcgCdQotKXjm3bjAxNFAK5u2OEof7k\nnG3iVaFC93V9U4xYefCCB3zp81wcqmfHRYWwaZqfAW4AbODdlmUdnLWuGbgH8AGPW5b19nPtSyG8\nfqmuL55t24ykRvPB3EVXose5T/aSyWUWbB/2hqgMxKkMVFARdO4rA3EqgxVUBOL4l9G9vRFMTaad\ngV4do3SdHaG3a5RMeuYQQCDkdbqvm8qcAV91ETznmS1Pn+fiUD07XvDALNM0DwC7LMvab5pmK/AN\nYP+sTe4C7rIs64emaf5/pmm2WJbVtiKlFtlgDMOg3F9Gub+MvZVmYXk2l6VvYoDORDcTrnHa+rsY\nmBxiYHKQzkQPbWMdi+4v6o3kwzkf0ME4FYEKqgJxKgJxvO61e57uhfAHvGzZUcmWHZWAc43kgd7E\nnNby6ecGOP1cfsCX26C6LjpzelRjjFBEA75k7TlvS9g0zY8AbZZlfS3//BhwnWVZo6ZpuoAOoMmy\nrOy59jNNLeH1S3VdHPPrOWfnGEuNMzA5yMDEkBPOE4MM5kN6cHKYrL34n1/MFy2E83QrerpFHQ+U\nr/t5tmcbH52cc1y5v2eM2V9vsfLAnNOjLmmtY2BgvHQF3iT0veG4mFOU6oBDs5735ZeNAtXAGPAZ\n0zSvAh6wLOv9F1lWEZnFZbgo88co88fYXrZ1wfqcnWNkapSBySEnmCcG861o5/GZsXZOjZ5Z8DoD\ngzJ/zAnmeUFdGYwT95evqzm3I7EAO2MBdrbWAM40m71do/lubCeYjx/u4fjhHgAMl0Ew5CUU9hGO\n+AhF/ITCPkKRec/DvnU7P7asfS/kZ7Ax73Ej8DngNPAT0zRfbVnWT5Z6cTweOu+xmgtVXR1d0f3J\n0lTXxXGh9VxLGdC86LpsLsvQxAi9iX56EwP0JQYK932JAU6OnuHEyOkFrzMMg8pgnOpwJTXhysL9\n9OOK4NoP6YbGcrjGeWznbPr7xjl7eoj2U4P0940zPjrF8ECycKWopQRDXiKxAJGon2gsQCTmJxIL\nEI0695GYn0g0gD+wcXoWVpK+N5a2nE9MJ07Ld1oD0JV/3A+csSzrBIBpmvcBe4ElQ3hoKLnUqhdE\nXR3Fo7oujtWpZy/VRj3VkXqYN49INpdlaGqEwUJ3d74lnX98rO95jvY9t2CPLsNF3F9OZSBOdaiK\n+nBt4RbzRdfM9JRzuKBpe5ym7fFCPdu2TWoqSzIxRXI8RWI8RXI8Nfd5IsXIUJK+7nP/v3i8LsKz\nWtROq3rhc3/AszbrZxXoe8Ox1A+R5YTwvcCHgS/nu5w7LcsaA7AsK2Oa5knTNHdZlvUccDXOSGkR\nWSfcLjdVwQqqghUQX7g+k8swNDmSD+dBBqePS+dD+/jwCY4Pn5jzmqAnmA/kGurDzkQldeEaynyx\nNRc+hmHgD3jwBzzEK8Pn3DaTzpJMpGaFsxPUyfEUiUSK5LjzfGRo4pz7cbkMJ5QL4ewnvEhwB8Pe\nVZmoRNaO5Z6i9LfATUAOeBdwJTCSHxG9E/gm4AKeAd5hWVZuqX1pYNb6pboujvVWz+lsmt6JfroS\nPXQneujK3/omBhbMJOaEc00+oOuoyz8uRTivZj3ncjkmEmmSiRSJ8cWDejrMZ88Wtphg2Es47CcY\n8RHOB3Mw5CMY8hIM+5zHYS+BoHdNXkpyvX2eV4sm65CLproujo1Sz+lcht5k35xg7kr00jfRvybC\neS3Us23bTE1m5gT1/OCefj77vOil+AOefDA7A86CIW8hpOcHt8/vLsoPn7VQz2uBLuAgIkXldXlo\njNTTGKmfs3ypcD492s7JkbmjuGeHc92sY85rsVv7hTAMg0DQacVWVp9723QqQzKRYiKRZiKZYiKZ\nZiLh3Cfz9xNJZ/3wwPnH3rjcxrxgPkdwhzRCfLUohEWkqM4Vzn3JfroS3YVg7kr0nDOc60K11Ec2\nXjgvxuvzUObzULbIcfv5crkckxOZfEinSC4S3NP3yxkdDs4Vsha2qBcP7s008OxiKYRFZE3wujzO\nVacidXOWX1g4B5xW8yYK58W4XK7COc7LkU5lCyGdzAf3Yi3uiUSK0aEJzncU0+UyCIS8hEI+yuJB\nvH43kaifcMzv3Eede59fYa0QFpE1balwzuQy9BbCubfQtb1UONeF8qEcqaU+VIs3uou50x5sXl6f\nG68vSKw8eN5tczmbqcl0IaTnd4XPDu6R4Qn6e5duZXu8rkIoTwfznPuYn0DQu6GDWiEsIuuS5wLD\necHMYU9BZSDO1lgL28q2sK2shaZIA54NNJXnanC5po8l+4Bzn9IFUBYLcvrUAImxKcbHpmbuR6dI\njDuPhweXPqXL7TbOGdLhiJ9g2IfLtT6DWp82EdlQzh/OTij3THVj9Z/kUO9THOp9qvDa5kgj28pa\n8uHcQtxfvqFbYqvN5/cQrwwRrwwtuU0mkyU5nmJ8dCao54d2V/vIkq83DJYO6vx9KOJbk6dwKYRF\nZFOYH87V1VF6e0fpmxjg9Ggbp0baOD16ZkGLucwXZWvZFrbFnGBuiTVtmktIFovH4yZWfu7u8Gw2\n55xrPb9FPeu+t3OUnnMcrw5FfAvCee5j34pPq3w+CmER2bQMw6AmVEVNqIrr6q4CIJVN0TbWkQ/m\nM5waOcNTfc/yVN+zgDNdZ2OkvhDK28paqA5WqbW8ytxuF9GyANGywJLb5HI2E8l8UI8uHtT9veP0\ndi193nIg6KWqNsJvvP7SogSyQlhEZBaf28fO8m3sLN8GOBNqDE+NcCofyqdH22gb66B9rINfdDwE\nQNgTYmtZixPMZS1sjTUT9Jx/kJOsLJfLIBxxjhPX1C++jW3bTE6klwzp8bEpkokUuaxdlIRUCIuI\nnINhGMQD5cQD5VxVczngHF/uGO/i1Egbp0bPcHqkjcMDxzg8cMx5DQa14Rq25VvK22JbqAvX4DLW\n3jHJzcYwZgaWVdeV/upOCmERkQvkcXnYEmtmS6yZl/IiAMZS44Vjy6dG2zgz2kZ3ooeHug4CEHD7\n2RJrntVabiHqi5zrbWQTUAiLiKyAqC/CZVV7uKxqDwA5O0dXosc5rjzaxumRNqyh57GGni+8pipY\nWQjlbTHnFKm1fo1mWVkKYRGRVTA9gKsxUs+LG28AIJme4MyoM/p6OpgP9jzBwZ4nAGdikpZoUz6U\nnXOXy/1lpfxnyCpTCIuIFEnIG6S18hJaKy8BnEFCvRP9nM53YZ/Ot5pPjJwuvKbcX1ZoLbdEG6kI\nVBD3l6nFvEEohEVESsQwDGpD1dSGqrm+/moAprIp2kbbCy3lk6NneKLvGZ7oe2bmdRiU+8uoDMap\nDFRQEYhTEYhTGYhTGYwT95crpNcJhbCIyBrid/vYFd/BrvgOwGktD04Oc2r0DF3j3QxMDjM4Ocjg\n5DAnhk/zPKcW7GM6pCsC5VQEKvJhPR3UFcQDZZqec43Q/4KIyBpmGIYTosE41M5dl8llGJ4aYWBi\niIHJIQbzt4HJQQYmhjg5cmZO13ZhnxiU+WMzred8QFfkwzoeiONVSBeFallEZJ3yuDxUBSupClYu\nuj6byzI0NZIP5iEGJwbnhPXp0TZOLhLSAGW+GJXB+Nyu7kAFFcE4Ff5yvG7vKv7LNg+FsIjIBuV2\nuakKVlAVrFh0fTaXZXhqlMFJJ5wLAZ1vWS92WchpMV90pos7OPe4dEUgjk8hvSwKYRGRTcrtche6\nunctsj6byzKSGmVwcpiBicFCi3q6Vd021sGp0bZF9x31RagMVNBYXkPcXelcxzlcS3WwUjOHzaIQ\nFhGRRbld7kILd3ou7dlydo6RqdFCC3pgYu5x6fb8hTBm87g81IVqqA/XzrrVURmMb8pwVgiLiMgL\n4jJchXm1YfGQNsIZDredKFzHuSvRTVeil7PjnXO29bq81IVraAjXzQnoeKB8Q4ezQlhERFaFy3BR\nHa7EqPJxaVVrYXnOzjEwMZQP5J45t/axjjn78Ll91IfyoRxxWs0N4VrK/WUb4vKRCmERESkql+Gi\nOlRJdaiSy6v3Fpbn7Bx9EwNOII/3FEK6Y7yTM2Ptc/YRcAeoD9fM6dKuj9RS5outq3BWCIuIyJrg\nMlyFGcSuqL60sDyby9I3MUDnvJbzmbGzCwaGBT3BecebnYCO+SJrMpwVwiIisqa5XW7qwjXUhWvm\nLM/kMvQm+xd0ay92/nPYG5ppMc8K6FJfTlIhLCIi65LH5aEhUkdDpG7O8nQuQ2+yj67xbjpnDQg7\nMXya54fnTvMZ8YadwWCRmVZzY6SOoCdYnH9DUd5FRESkSLwuT+EykrOlsml6kr2FFnPnuNOCPj58\nguPDJwrbBdx+Pnzj+4h4w6teVoWwiIhsCj63l+ZoI83RxjnLp7Ipumd1Z9u2TdAdKEqZFMIiIrKp\n+d0+tsSa2RJrLvp7b9wzoEVERNY4hbCIiEiJKIRFRERKRCEsIiJSIgphERGRElEIi4iIlIhCWERE\npEQUwiIiIiWiEBYRESkRhbCIiEiJKIRFRERKRCEsIiJSIgphERGRElEIi4iIlIhCWEREpESWdT1h\n0zQ/A9wA2MC7Lcs6uMg2fwPstyzrpStaQhERkQ3qvC1h0zQPALssy9oPvBX4/CLb7AFuWvniiYiI\nbFzL6Y5+OfAjAMuyjgJx0zRj87a5C/jLFS6biIjIhracEK4D+mY978svA8A0zTcD9wOnV7JgIiIi\nG92yjgnPY0w/ME2zAvh/gVcAjct5cTwewuNxv4C3XVp1dXRF9ydLU10Xh+q5OFTPxaF6XtpyQriT\nWS1foAHoyj9+GVANPAD4gR2maX7Gsqw7l9rZ0FDyBRZ1cdXVUfr6xlZ0n7I41XVxqJ6LQ/VcHKpn\nx1I/RJbTHX0v8HoA0zSvAjotyxoDsCzrXyzL2mNZ1g3A7wCPnyuARUREZMZ5Q9iyrAeBQ6ZpPogz\nMvpdpmm+2TTN31n10omIiGxgyzombFnW++YtemqRbU4DL734IomIiGwOmjFLRESkRBTCIiIiJaIQ\nFhERKRGFsIiISIkohEVEREpEISwiIlIiCmEREZESUQiLiIiUiEJYRESkRBTCIiIiJaIQFhERKRGF\nsIiISIkohEVEREpEISwiIlIiCmEREZESUQiLiIiUiEJYRESkRBTCIiIiJaIQFhERKRGFsIiISIko\nhEVEREpEISwiIlIiCmEREZESUQiLiIiUiEJYRESkRBTCIiIiJaIQFhERKRGFsIiISIkohEVEREpE\nISwiIlIiCmEREZESUQiLiIiUiEJYRESkRBTCIiIiJaIQFhERKRGFsIiISIkohEVEREpEISwiIlIi\nCmEREZESUQiLiIiUiEJYRESkRBTCIiIiJeIpdQHWikRinA9/+ANMTEwwOTnJnXe+l0RinC9/+Qu4\nXC5e8YpXcvvtv8fBgw8vWPb617+Gb33r+4RCIe6++7Ns374DgIcffpD+/j4+/OGP873v/SNHjhwm\nlUrx2tfeymte81q6u7v42Mc+RC6Xo66unne/+0/5oz96C/fc8wMMw+Dee/8NyzrKH//xn5S4dkRE\nZDWsuRD+p58/z8Fjvcve3u02yGbtc25z7e4abn/ZznNuMzAwwG/91mu56aaXcujQQb7znX/gxInn\n+eIXv0EsFuP97/9Tbrnlddx11ycWLFtKT083X/rSN0ilUtTVNfDHf/wnTE1Ncvvtr+U1r3ktX/nK\nF3jDG97Ii198gC984XOcPXuWnTt38uyzT3PZZft44IH7eeMb/2DZdSEiIuvLmgvhUqmoqOQf/uFr\n3HPPt0mn00xOTuDz+YjH4wB88pOfZWhocMGyc2lt3YNhGPj9fkZHR3j729+Cx+NheHgIgOPHj/Hu\nd/8pAO9857sBeNWrXs19993L7t176OrqZPfuPav1TxYRkRJbcyF8+8t2nrfVOlt1dZS+vrGLft9/\n+qfvUlVVwwc/+FGOHTvCxz/+YXK5uS1sl8u1YBmAYRiFx5lMpvDY4/EC8MQTh3j88ce4++6v4PF4\n+LVfe8mS+7vhhhfx1a9+iUOHDnLjjS++6H+XiIisXcsamGWa5mdM03zINM0HTdO8dt66m03TfNg0\nzV+ZpvkN0zTX5WCvkZFhGhubALj//v8kFAqTy2Xp6+vFtm3+/M/fg8vlXrBsbGyMUCjMwEA/2WyW\nw4efWXTfNTW1eDwefvnL+8lmc6TTaXbv3sPjjx8E4Gtf+xIHDz6Cx+Phiiuu5Otf/xKvfOVvFLUO\nRESkuM4bmKZpHgB2WZa1H3gr8Pl5m3wFeL1lWS8CosCrVryURfCqV72a73//O9x557vYu/dSBgYG\n+L3f+30+8IG/4O1vfwtXX30t0WiUP/3T9y1Yduutt/MXf3Enf/mX72Xbtu0L9n3NNddz9mwbd9zx\nNjo6znLjjS/m05/+G9761j/ixz/+EXfc8Ta6ujq46qprAHjZy14JGDQ1NRe5FkREpJgM2z73oCbT\nND8CtFmW9bX882PAdZZljeafx2Y9/gLwkGVZ315qf319Y+d+wwu0Ut3Ra8nXv/5l6urqefWrf7vU\nRZljI9b1WqR6Lg7Vc3Gonh3V1VFjseXLOSZcBxya9bwvv2wUYFYA1wOvBD54rp3F4yE8Hvcy3nb5\nqqujK7q/Unrb295GIBDgve+9E7d7ZetpJWykul7LVM/FoXouDtXz0l7IwKwFaW6aZg3wv4F3WpY1\ncK4XDw0lX8BbLm2j/cr667++C4DBwZWtp5Ww0ep6rVI9F4fquThUz46lfogsJ4Q7cVq+0xqArukn\npmnGgH8D/tKyrHsvoowiIiKbynJGMt8LvB7ANM2rgE7Lsmb/rLkL+IxlWf++CuUTERHZsM7bErYs\n60HTNA+ZpvkgkAPeZZrmm4ER4D+APwB2mab5h/mXfNeyrK+sVoFFREQ2imUdE7Ys633zFj0167F/\n5YojIiKyeazLiTVWw09/+r+5++5zT0MpIiKykhTCIiIiJbLm5o4utX/6p3u47z5nkPdLXnKAN73p\nzTz66MN89atfwO8PEI9X8KEPfYzHH39swTKPR9UpIiLLt+ZS41+f/z880btw/uWluF0G2UUuqjDb\nlTWX8bqdv3XefXV1dXDo0KN89avfAuBtb/t/uPnmV/CDH3yfO+64k337ruT++3/OyMjwossqK6uW\nXW4RERF1R89y/Phx9u69DI/Hg8fj4bLL9vH888e5+eZX8KlP/Q3f+tY32LXLpLKyatFlIiIiF2LN\ntYRft/O3ltVqnbaSs7EYBsyeSzudTmMYLl71qldz/fX7+cUv/ou/+Is7+djHPrnosi1btq5IOURE\nZHNQS3iWSy4xefbZZ8hkMmQyGY4cOcwll5h885tfw+32cMstr+PlL38lp0+fXHSZiIjIhVhzLeFS\nqqtr4Morr+GP//ht5HI2r3nNLdTV1VNbW8d73vNOotEY0WiUN7zhTSSTyQXLRERELsR5L2W40nQp\nw/VLdV0cqufiUD0Xh+rZsdSlDNUdLSIiUiIKYRERkRJRCIuIiJSIQlhERKREFMIiIiIlohAWEREp\nEYXwBXj9619DMpksdTFERGSDUAiLiIiUiGbMAt7yljfy8Y/fRV1dHd3dXbz//X9KdXUNExMTTE5O\ncued72XPnkvPu5977vlH/uu/7iOXy7F//4t4y1vextjYGB/5yAdIJBJEIhH+x//4ONlsdsGye+75\nNuXl5dx66+9y8uTz/N3ffZK77/4Kb3jD73DJJbu57rrrqa2t52tf+xJer5doNMpHPvK3eL1ePvvZ\nT3PkyLO43W7e+973881vfp3f/u3f4ZprriOVSvGmN93Gd7/7A11qUURkjVlz38p9//w9xh47uOzt\nz7hdZLO5c24TveZaqm97w5Lrb7rpZn71q19w662388AD93PTTTezY8cubrrppRw6dJDvfOcf+Ou/\n/tSyyvOFL3wNl8vF7bffwu/+7u9xzz3f5rrr9nPbbW/g+9//Do899ijHjh1ZsGwpnZ0dfPzjn2b7\n9h38/Oc/40Mf+hgNDY189KN/xSOPPITf76e3t4evfOWbPPnk49x33//l13/9N7nvvv/LNddcx6FD\nj3LDDTcqgEVE1iB9M+OE8N13f5Zbb72dX/7yfu64406+971vc8893yadThMIBJa1n0AgwB13vA23\n283w8DCjo6McP36MP/zDdwDwu7/7RgB+/ON/XbDsueesJfYZZPv2HQCUl5fziU98jGw2S2dnB1df\nfS1DQ4Ncdtk+AK644iquuOIqMpkMX/zi58lkMjzwwP385m++5oVXjoiIrJo1F8LVt73hnK3WBduv\nwLyk27fvYGCgj56ebsbGxnjggf+iqqqGD37woxw7doS77/7seffR3d3F97//Hb7xje8QCoX4/d+/\nHQCXy41tz22pL7bMMGamFc1kMoXHXu/Mf9Hf/M1H+dSnPsvWrdv4u7/7xJL78ng8XHvtDTz22KOc\nOnWSSy+9fJk1ISIixaSBWXn797+Yr3zlC7zkJQcYGRmmsbEJgPvv/885obiU4eFh4vE4oVAIyzpG\nd3c36XSa1tY9HDrkdK//6Ec/4N/+7f8suiwcDtPf3w/A008/ueh7JBLj1NbWMTY2xuOPHyrs//HH\nHwPg+PFj3HWXE86//uu/yde//iWuvPLqi6sYERFZNQrhvAMHbuZnP/sPXvrSl/OqV72a73//O9x5\n57vYu/dSBgYG+MlPfnzO1+/adQnBYIh3vOMt3Hffvdxyy+u4665PcNtt/41nn32aO+54Gw8++EsO\nHLh50WUHDryMX/7yft7znncyPj6+6Hu87nW38Y53vJVPfvKveeMb/4B//Mdv0tTUwpYt23jnO/+Q\nz37207z2tbcCsHt3K6Ojo/zar71qxetKRERWhi5luEG1tZ3hrrs+wec+94UV26fqujhUz8Whei4O\n1bNjqUsZrrljwmvdL395P9/73ncWLL/ttv/GgQM3l6BEC/3oR//Cj3/8Q/7yLz9c6qKIiMg5qCUs\ny6a6Lg7Vc3GonotD9exYqiWsY8IiIiIlohAWEREpEYWwiIhIiSiERURESkQhLCIiUiIK4QtwvusJ\nv/rVLy9iaUREZL1TCIuIiJTImpus48Gfn+Dksd5lb+9yu8id51KG23fXcOPLdiy5fqWuJzztxInn\n+bu/+wSGYRAKhfnAB/4HLpebv/qr95FKpUin0/zJn/wFjY1NC5aZ5u5lv4+IiKxvay6ES2ElrycM\n8LnPfZp3vvPd7N17Kd/97rf553/+Hjt37qK6uob3v/+v6Og4S3t7G93dnQuWiYjI5rHmQvjGl+04\nZ6t1vpWYjWWlric87fTpU+zd67Scr7rqGv7+77/CLbfcyle/+kU+9amPc+DAy7jhhhvp7+9fsExE\nRDYPHRNm6esJf/GLX+fP/ux9F7XvTCaNy+WiqqqKb37zHg4ceBk//OG/8Pd//9VFl4mIyOax5lrC\npTL7esLDw0Ps2LELWP71hGfbtm0Hzz77NJdeejlPPPE4ptnKwYOPkMlk2L//RWzduo277vrbRZeJ\niMjmoRDOO3DgZt7+9rfwzW/ew+TkBB/72If4z//8Gbfeejs/+9m9572e8Gzvec+fFQZmRaNR/vt/\n/xCjo6N85CMf5Dvf+QdcLhdvfesfUVNTu2CZiIhsHrqKkiyb6ro4VM/FoXouDtWzQ9cTXiHr4XrC\nIiKyPiiEL9CLX3yAF7/4QKmLISIiG4BGR4uIiJSIQlhERKREFMIiIiIlsqxjwqZpfga4AbCBd1uW\ndXDWulcAHweywE8ty/roahRURERkozlvS9g0zQPALsuy9gNvBT4/b5PPA7cCLwJeaZrmnhUvpYiI\nyAa0nO7olwM/ArAs6ygQN00zBmCa5nZg0LKsdsuycsBP89uLiIjIeSwnhOuAvlnP+/LLFlvXC9Sv\nTNFEREQ2thdynvCis5OZEUMAAAMqSURBVH4sYx2w9KwhF6O6OrrSu5QlqK6LQ/VcHKrn4lA9L205\nLeFOZlq+AA1A1xLrGvPLRERE5DyWE8L3Aq8HME3zKqDTsqwxAMuyTgMx0zS3mqbpAX4rv72IiIic\nx7Iu4GCa5t8CNwE54F3AlcCIZVk/NE3zJuAT+U1/YFnWp1ersCIiIhtJ0a+iJCIiIg7NmCUiIlIi\nCmEREZESWdeXMjzXdJqyckzT/CTwEpzPy99YlvWvJS7ShmWaZhB4FvioZVnfLHFxNizTNN8I/DmQ\nAf7KsqyflLhIG45pmhHgW0Ac8AMftizrP0pbqrVn3baElzGdpqwA0zRvBi7N1/OrgM+WuEgb3QeA\nwVIXYiMzTbMS+BDwYpwzOm4pbYk2rDcDlmVZN+OcYfO50hZnbVq3Icw5ptOUFfUL4Lb842EgbJqm\nu4Tl2bBM09wN7AHUKltdrwB+ZlnWmGVZXZZlva3UBdqg+oHK/ON4/rnMs55D+FzTacoKsSwra1lW\nIv/0rThXysqWskwb2F3An5S6EJvAViBkmuaPTdN8wDRNzXe/CizL+h7QYprm8zg/5v+sxEVak9Zz\nCM+34tNhygzTNG/BCeE7Sl2Wjcg0zT8AHrIs61Spy7IJGDgttNfhdJn+vWma+v5YYaZpvglosyxr\nJ/D/t3eHKpFGYRjH/xewYHHLFuNzCYZtiyBiME7bZLDsFcjCGhfrFouYRS/AwbJlmGIwvqAgCxs2\nikbDhplBUDDIN5z5hv8vHU562hPew3u+AL8aR1pIfS7ht9ZpqkNJNoF9YKuq7lvnWVLbwE6SMbAL\nfJ/+1a3u/QNGVfVUVbfAA/CxcaZl9Bm4AKiqa+CTo6zX+vw6eggcAEcv12mqO0lWgENgo6p8MDQn\nVTWYnZP8AO6q6rJdoqU2BE6S/GQyq/yA88p5uAHWgfMka8Cjo6zXelvCVTVKcpVkxPM6TXVvAKwC\np0lmd1+r6k+7SNL7VdXfJGfAeHr1bfofurp1BBwn+c2ka/Ya51lIrq2UJKmRPs+EJUnqNUtYkqRG\nLGFJkhqxhCVJasQSliSpEUtYkqRGLGFJkhqxhCVJauQ/91BqTWnpZ3kAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "_eFqYMAARPTv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.7)\n",
        "Try running `model.fit()` again, and notice that training continues where it left off."
      ]
    },
    {
      "metadata": {
        "id": "-HzM0t9_RPTv",
        "colab_type": "code",
        "outputId": "e7137da6-e9ac-4a62-9f8c-669185cbbacf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        }
      },
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train, y_train, epochs=10,\n",
        "                    validation_data=(X_valid, y_valid))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 55000 samples, validate on 5000 samples\n",
            "Epoch 1/10\n",
            "55000/55000 [==============================] - 7s 136us/sample - loss: 0.4507 - accuracy: 0.8368 - val_loss: 0.5274 - val_accuracy: 0.8320\n",
            "Epoch 2/10\n",
            "55000/55000 [==============================] - 7s 133us/sample - loss: 0.4325 - accuracy: 0.8434 - val_loss: 0.5267 - val_accuracy: 0.8252\n",
            "Epoch 3/10\n",
            "55000/55000 [==============================] - 7s 132us/sample - loss: 0.4189 - accuracy: 0.8475 - val_loss: 0.4983 - val_accuracy: 0.8376\n",
            "Epoch 4/10\n",
            "55000/55000 [==============================] - 7s 130us/sample - loss: 0.4051 - accuracy: 0.8525 - val_loss: 0.4917 - val_accuracy: 0.8432\n",
            "Epoch 5/10\n",
            "55000/55000 [==============================] - 7s 128us/sample - loss: 0.3986 - accuracy: 0.8548 - val_loss: 0.4757 - val_accuracy: 0.8434\n",
            "Epoch 6/10\n",
            "55000/55000 [==============================] - 7s 132us/sample - loss: 0.3912 - accuracy: 0.8573 - val_loss: 0.4973 - val_accuracy: 0.8386\n",
            "Epoch 7/10\n",
            "55000/55000 [==============================] - 7s 133us/sample - loss: 0.3828 - accuracy: 0.8605 - val_loss: 0.4793 - val_accuracy: 0.8434\n",
            "Epoch 8/10\n",
            "55000/55000 [==============================] - 7s 132us/sample - loss: 0.3762 - accuracy: 0.8615 - val_loss: 0.4729 - val_accuracy: 0.8492\n",
            "Epoch 9/10\n",
            "55000/55000 [==============================] - 7s 130us/sample - loss: 0.3688 - accuracy: 0.8644 - val_loss: 0.4625 - val_accuracy: 0.8512\n",
            "Epoch 10/10\n",
            "55000/55000 [==============================] - 7s 131us/sample - loss: 0.3643 - accuracy: 0.8648 - val_loss: 0.4682 - val_accuracy: 0.8492\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5_eewosXRPTw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.8)\n",
        "Call the model's `evaluate()` method, passing it the test set (`X_test` and `y_test`). This will compute the loss (cross-entropy) on the test set, as well as all the additional metrics (in this case, the accuracy). Your model should achieve over 80% accuracy on the test set."
      ]
    },
    {
      "metadata": {
        "id": "7_mXDKUmRPTx",
        "colab_type": "code",
        "outputId": "c4fc034a-f22d-434c-cdf7-f7370d69c66b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "model.evaluate(X_test, y_test)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 1s 70us/sample - loss: 0.5018 - accuracy: 0.8346\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5018334259986877, 0.8346]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "metadata": {
        "id": "FQ75RKXbRPTy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.9)\n",
        "Define `X_new` as the first 10 instances of the test set. Call the model's `predict()` method to estimate the probability of each class for each instance (for better readability, you may use the output array's `round()` method):"
      ]
    },
    {
      "metadata": {
        "id": "apo-SxVsRPTy",
        "colab_type": "code",
        "outputId": "52f3b93b-3e53-43a2-acb5-09f5397b0a0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        }
      },
      "cell_type": "code",
      "source": [
        "n_new = 10\n",
        "X_new = X_test[:n_new]\n",
        "y_proba = model.predict(X_new)\n",
        "y_proba.round(2)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.99],\n",
              "       [0.  , 0.  , 0.94, 0.  , 0.01, 0.  , 0.05, 0.  , 0.  , 0.  ],\n",
              "       [0.  , 1.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
              "       [0.  , 1.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
              "       [0.38, 0.  , 0.1 , 0.03, 0.02, 0.  , 0.46, 0.  , 0.01, 0.  ],\n",
              "       [0.  , 1.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
              "       [0.  , 0.  , 0.01, 0.  , 0.96, 0.  , 0.02, 0.  , 0.  , 0.  ],\n",
              "       [0.  , 0.  , 0.03, 0.  , 0.51, 0.  , 0.46, 0.  , 0.  , 0.  ],\n",
              "       [0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  , 0.  , 0.  , 0.  ],\n",
              "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  , 0.  ]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "metadata": {
        "id": "0TS0offQRPTz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.10)\n",
        "Often, you may only be interested in the most likely class. Use `np.argmax()` to get the class ID of the most likely class for each instance. **Tip**: you want to set `axis=1`."
      ]
    },
    {
      "metadata": {
        "id": "Xg_P8onGRPT1",
        "colab_type": "code",
        "outputId": "64400b6f-7f8f-44ec-f305-d92d513ea559",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "cell_type": "code",
      "source": [
        "y_pred = y_proba.argmax(axis=1)\n",
        "y_pred"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([9, 2, 1, 1, 6, 1, 4, 4, 5, 7])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "metadata": {
        "id": "jmlHeqeWRPT2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.11)\n",
        "Call the model's `predict_classes()` method for `X_new`. You should get the same result as above."
      ]
    },
    {
      "metadata": {
        "id": "mDfoAE2QRPT3",
        "colab_type": "code",
        "outputId": "200f45b4-b25b-4aca-9506-6d44cb67cfc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "cell_type": "code",
      "source": [
        "y_pred = model.predict_classes(X_new)\n",
        "y_pred"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([9, 2, 1, 1, 6, 1, 4, 4, 5, 7])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "metadata": {
        "id": "4WfPVC2-RPT4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.12)\n",
        "(Optional) It is often useful to know how confident the model is for each prediction. Try finding the estimated probability for each predicted class using `np.max()`."
      ]
    },
    {
      "metadata": {
        "id": "NkvgQ6IXRPT5",
        "colab_type": "code",
        "outputId": "64094d00-b526-4b9b-9d2e-ddd2910205d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "y_proba.max(axis=1).round(2)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.99, 0.94, 1.  , 1.  , 0.46, 1.  , 0.96, 0.51, 1.  , 1.  ],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "metadata": {
        "id": "uDOQNtGPRPT6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.13)\n",
        "(Optional) It is frequent to want the top k classes and their estimated probabilities rather just the most likely class. You can use `np.argsort()` for this."
      ]
    },
    {
      "metadata": {
        "id": "G2hi2a61RPT6",
        "colab_type": "code",
        "outputId": "c7b29651-a4aa-489f-ce05-9a1e34b19c96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        }
      },
      "cell_type": "code",
      "source": [
        "k = 3\n",
        "top_k = np.argsort(-y_proba, axis=1)[:, :k]\n",
        "top_k"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[9, 7, 5],\n",
              "       [2, 6, 4],\n",
              "       [1, 3, 2],\n",
              "       [1, 0, 6],\n",
              "       [6, 0, 2],\n",
              "       [1, 3, 2],\n",
              "       [4, 6, 2],\n",
              "       [4, 6, 2],\n",
              "       [5, 7, 9],\n",
              "       [7, 5, 8]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "metadata": {
        "id": "gN-S84wtRPT7",
        "colab_type": "code",
        "outputId": "07fa82f2-6cdc-4bb7-e9d6-1c01fe3ab63c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        }
      },
      "cell_type": "code",
      "source": [
        "row_indices = np.tile(np.arange(len(top_k)), [k, 1]).T\n",
        "y_proba[row_indices, top_k].round(2)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.99, 0.01, 0.  ],\n",
              "       [0.94, 0.05, 0.01],\n",
              "       [1.  , 0.  , 0.  ],\n",
              "       [1.  , 0.  , 0.  ],\n",
              "       [0.46, 0.38, 0.1 ],\n",
              "       [1.  , 0.  , 0.  ],\n",
              "       [0.96, 0.02, 0.01],\n",
              "       [0.51, 0.46, 0.03],\n",
              "       [1.  , 0.  , 0.  ],\n",
              "       [1.  , 0.  , 0.  ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "metadata": {
        "id": "Bha9eJ6oRPT8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![Exercise](https://c1.staticflickr.com/9/8101/8553474140_c50cf08708_b.jpg)"
      ]
    },
    {
      "metadata": {
        "id": "69UqxP0bRPT8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Exercise 3 – Scale the features"
      ]
    },
    {
      "metadata": {
        "id": "8dXCkemVRPT-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3.1)\n",
        "When using Gradient Descent, it is usually best to ensure that the features all have a similar scale, preferably with a Normal distribution. Try to standardize the pixel values and see if this improves the performance of your neural network.\n",
        "\n",
        "**Tips**:\n",
        "* For each feature (pixel intensity), you must subtract the `mean()` of that feature (across all instances, so use `axis=0`) and divide by its standard deviation (`std()`, again `axis=0`). Alternatively, you can use Scikit-Learn's `StandardScaler`.\n",
        "* Make sure you compute the means and standard deviations on the training set, and use these statistics to scale the training set, the validation set and the test set (you should not fit the validation set or the test set, and computing the means and standard deviations counts as \"fitting\")."
      ]
    },
    {
      "metadata": {
        "id": "IM3z3Oe5RPT-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BJSMnRZKRPT_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wnyW2dHrRPT_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EdsgSzdSRPUA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3.2)\n",
        "Plot the learning curves. Do they look better than earlier?"
      ]
    },
    {
      "metadata": {
        "id": "1-Jv3WIZRPUA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NzLcNABhRPUC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S1w1axBuRPUD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q6n527NLRPUD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![Exercise solution](https://camo.githubusercontent.com/250388fde3fac9135ead9471733ee28e049f7a37/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f302f30362f46696c6f735f736567756e646f5f6c6f676f5f253238666c69707065642532392e6a7067)"
      ]
    },
    {
      "metadata": {
        "id": "bwjFFv2kRPUE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Exercise 3 – Solution"
      ]
    },
    {
      "metadata": {
        "id": "ZkC5WxjXRPUE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3.1)\n",
        "When using Gradient Descent, it is usually best to ensure that the features all have a similar scale, preferably with a Normal distribution. Try to standardize the pixel values and see if this improves the performance of your neural network."
      ]
    },
    {
      "metadata": {
        "id": "OlUXWhJuRPUE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pixel_means = X_train.mean(axis = 0)\n",
        "pixel_stds = X_train.std(axis = 0)\n",
        "X_train_scaled = (X_train - pixel_means) / pixel_stds\n",
        "X_valid_scaled = (X_valid - pixel_means) / pixel_stds\n",
        "X_test_scaled = (X_test - pixel_means) / pixel_stds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fOpS4nRvRPUF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train.astype(np.float32).reshape(-1, 1)).reshape(-1, 28, 28)\n",
        "X_valid_scaled = scaler.transform(X_valid.astype(np.float32).reshape(-1, 1)).reshape(-1, 28, 28)\n",
        "X_test_scaled = scaler.transform(X_test.astype(np.float32).reshape(-1, 1)).reshape(-1, 28, 28)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PlvU_v98RPUG",
        "colab_type": "code",
        "outputId": "a9e5e40b-6a65-495d-9cbc-44293b5b09f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 703
        }
      },
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.Dense(300, activation=\"relu\"),\n",
        "    keras.layers.Dense(100, activation=\"relu\"),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
        "history = model.fit(X_train_scaled, y_train, epochs=20,\n",
        "                    validation_data=(X_valid_scaled, y_valid))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 55000 samples, validate on 5000 samples\n",
            "Epoch 1/20\n",
            "55000/55000 [==============================] - 8s 144us/sample - loss: 0.8943 - accuracy: 0.7103 - val_loss: 0.6087 - val_accuracy: 0.7948\n",
            "Epoch 2/20\n",
            "55000/55000 [==============================] - 8s 148us/sample - loss: 0.5744 - accuracy: 0.8012 - val_loss: 0.5170 - val_accuracy: 0.8260\n",
            "Epoch 3/20\n",
            "55000/55000 [==============================] - 8s 146us/sample - loss: 0.5095 - accuracy: 0.8219 - val_loss: 0.4783 - val_accuracy: 0.8382\n",
            "Epoch 4/20\n",
            "55000/55000 [==============================] - 8s 146us/sample - loss: 0.4742 - accuracy: 0.8333 - val_loss: 0.4566 - val_accuracy: 0.8460\n",
            "Epoch 5/20\n",
            "55000/55000 [==============================] - 8s 141us/sample - loss: 0.4507 - accuracy: 0.8425 - val_loss: 0.4375 - val_accuracy: 0.8508\n",
            "Epoch 6/20\n",
            "55000/55000 [==============================] - 8s 142us/sample - loss: 0.4329 - accuracy: 0.8476 - val_loss: 0.4233 - val_accuracy: 0.8554\n",
            "Epoch 7/20\n",
            "55000/55000 [==============================] - 8s 144us/sample - loss: 0.4192 - accuracy: 0.8524 - val_loss: 0.4146 - val_accuracy: 0.8592\n",
            "Epoch 8/20\n",
            "55000/55000 [==============================] - 8s 142us/sample - loss: 0.4075 - accuracy: 0.8561 - val_loss: 0.4063 - val_accuracy: 0.8614\n",
            "Epoch 9/20\n",
            "55000/55000 [==============================] - 8s 142us/sample - loss: 0.3982 - accuracy: 0.8600 - val_loss: 0.3971 - val_accuracy: 0.8652\n",
            "Epoch 10/20\n",
            "55000/55000 [==============================] - 8s 143us/sample - loss: 0.3897 - accuracy: 0.8633 - val_loss: 0.3922 - val_accuracy: 0.8650\n",
            "Epoch 11/20\n",
            "55000/55000 [==============================] - 8s 143us/sample - loss: 0.3822 - accuracy: 0.8660 - val_loss: 0.3867 - val_accuracy: 0.8674\n",
            "Epoch 12/20\n",
            "55000/55000 [==============================] - 8s 139us/sample - loss: 0.3753 - accuracy: 0.8677 - val_loss: 0.3823 - val_accuracy: 0.8694\n",
            "Epoch 13/20\n",
            "55000/55000 [==============================] - 8s 146us/sample - loss: 0.3694 - accuracy: 0.8713 - val_loss: 0.3754 - val_accuracy: 0.8688\n",
            "Epoch 14/20\n",
            "55000/55000 [==============================] - 8s 141us/sample - loss: 0.3637 - accuracy: 0.8717 - val_loss: 0.3728 - val_accuracy: 0.8694\n",
            "Epoch 15/20\n",
            "55000/55000 [==============================] - 8s 143us/sample - loss: 0.3585 - accuracy: 0.8739 - val_loss: 0.3745 - val_accuracy: 0.8704\n",
            "Epoch 16/20\n",
            "55000/55000 [==============================] - 8s 142us/sample - loss: 0.3539 - accuracy: 0.8759 - val_loss: 0.3650 - val_accuracy: 0.8730\n",
            "Epoch 17/20\n",
            "55000/55000 [==============================] - 8s 144us/sample - loss: 0.3491 - accuracy: 0.8769 - val_loss: 0.3654 - val_accuracy: 0.8692\n",
            "Epoch 18/20\n",
            "55000/55000 [==============================] - 8s 139us/sample - loss: 0.3449 - accuracy: 0.8783 - val_loss: 0.3590 - val_accuracy: 0.8740\n",
            "Epoch 19/20\n",
            "55000/55000 [==============================] - 8s 148us/sample - loss: 0.3407 - accuracy: 0.8799 - val_loss: 0.3599 - val_accuracy: 0.8734\n",
            "Epoch 20/20\n",
            "55000/55000 [==============================] - 8s 143us/sample - loss: 0.3371 - accuracy: 0.8813 - val_loss: 0.3552 - val_accuracy: 0.8764\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "L6r7NtFJRPUG",
        "colab_type": "code",
        "outputId": "4cd0d8cf-5bb8-4234-fd26-910dc0bdd919",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "model.evaluate(X_test_scaled, y_test)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 1s 85us/sample - loss: 0.3883 - accuracy: 0.8606\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.38833879073858263, 0.8606]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "metadata": {
        "id": "EOGYbFicRPUI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3.2)\n",
        "Plot the learning curves. Do they look better than earlier?"
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "YuzOixXaRPUI",
        "colab_type": "code",
        "outputId": "3c4d20a9-91a6-4b13-94cb-542a96e0c4ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        }
      },
      "cell_type": "code",
      "source": [
        "plot_learning_curves(history)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeEAAAEzCAYAAAAGvLsqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "HcRJexXlRPUJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![Exercise](https://c1.staticflickr.com/9/8101/8553474140_c50cf08708_b.jpg)"
      ]
    },
    {
      "metadata": {
        "id": "4eW2GR0dRPUJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Exercise 4 – Use Callbacks"
      ]
    },
    {
      "metadata": {
        "id": "xhuImJtfRPUJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 4.1)\n",
        "The `fit()` method accepts a `callbacks` argument. Try training your model with a large number of epochs, a validation set, and with a few callbacks from `keras.callbacks`:\n",
        "* `TensorBoard`: specify a log directory. It should be a subdirectory of a root logdir, such as `./my_logs/run_1`, and it should be different every time you train your model. You can use a timestamp in the subdirectory's path to ensure that it changes at every run.\n",
        "* `EarlyStopping`: specify `patience=5`\n",
        "* `ModelCheckpoint`: specify the path of the checkpoint file to save (e.g., `\"my_mnist_model.h5\"`) and set `save_best_only=True`\n",
        "\n",
        "Notice that the `EarlyStopping` callback will interrupt training before it reaches the requested number of epochs. This reduces the risk of overfitting."
      ]
    },
    {
      "metadata": {
        "id": "3Uh0cas-RPUJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qfIiunnqRPUK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0dE7fJRsRPUL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MTbUE9DJRPUM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 4.2)\n",
        "Run the following code (from this [StackOverflow answer](https://stackoverflow.com/questions/38189119/simple-way-to-visualize-a-tensorflow-graph-in-jupyter/53978715#53978715)) to start a TensorBoard server and open a new tab to visualize the learning curve. When you are done, you can stop the tensorboard server by running `server.kill()`."
      ]
    },
    {
      "metadata": {
        "id": "kvdwecuERPUM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "root_logdir = os.path.join(os.curdir, \"my_logs\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gN1YbNsERPUN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def tb(logdir=root_logdir, port=6006, open_tab=True, sleep=2):\n",
        "    import subprocess\n",
        "    proc = subprocess.Popen(\n",
        "        \"tensorboard --logdir={0} --port={1}\".format(logdir, port), shell=True)\n",
        "    if open_tab:\n",
        "        import time\n",
        "        time.sleep(sleep)\n",
        "        import webbrowser\n",
        "        webbrowser.open(\"http://127.0.0.1:{}/\".format(port))\n",
        "    return proc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vpPfgcHMRPUN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#server = tb() # uncomment and run this to start the TensorBoard server"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fk3F0iTGRPUO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#server.kill() # uncomment and run this to stop the server"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CPv5oiYiRPUP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 4.3)\n",
        "The early stopping callback only stopped training after 10 epochs without progress, so your model may already have started to overfit the training set. Fortunately, since the `ModelCheckpoint` callback only saved the best models (on the validation set), the last saved model is the best on the validation set, so try loading it using `keras.models.load_model()`. Finally evaluate it on the test set."
      ]
    },
    {
      "metadata": {
        "id": "U-uBMu_KRPUQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JU-DNyVeRPUQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "haw9qfblRPUR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VSHSbodvRPUS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 4.4)\n",
        "Look at the list of available callbacks at https://keras.io/callbacks/"
      ]
    },
    {
      "metadata": {
        "id": "3mSJZzYrRPUS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J6mWmGomRPUT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TUerq00vRPUU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_ZI8eYmxRPUV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![Exercise solution](https://camo.githubusercontent.com/250388fde3fac9135ead9471733ee28e049f7a37/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f302f30362f46696c6f735f736567756e646f5f6c6f676f5f253238666c69707065642532392e6a7067)"
      ]
    },
    {
      "metadata": {
        "id": "fnv07ZWURPUV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Exercise 4 – Solution"
      ]
    },
    {
      "metadata": {
        "id": "Dm92YfodRPUV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 4.1)\n",
        "The `fit()` method accepts a `callbacks` argument. Try training your model with a large number of epochs, a validation set, and with a few callbacks from `keras.callbacks`:\n",
        "* `TensorBoard`: specify a log directory. It should be a subdirectory of a root logdir, such as `./my_logs/run_1`, and it should be different every time you train your model. You can use a timestamp in the subdirectory's path to ensure that it changes at every run.\n",
        "* `EarlyStopping`: specify `patience=5`\n",
        "* `ModelCheckpoint`: specify the path of the checkpoint file to save (e.g., `\"my_mnist_model.h5\"`) and set `save_best_only=True`\n",
        "\n",
        "Notice that the `EarlyStopping` callback will interrupt training before it reaches the requested number of epochs. This reduces the risk of overfitting."
      ]
    },
    {
      "metadata": {
        "id": "kodOM0A1RPUW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.Dense(300, activation=\"relu\"),\n",
        "    keras.layers.Dense(100, activation=\"relu\"),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=\"sgd\", metrics=[\"accuracy\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-kO79hDsfS65",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        " root_logdir = './tf_logs'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CWb7RpkKRPUW",
        "colab_type": "code",
        "outputId": "47977378-08fd-4b77-f049-b3bdb3ba3dde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1678
        }
      },
      "cell_type": "code",
      "source": [
        "logdir = os.path.join(root_logdir, \"run_{}\".format(time.time()))\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.TensorBoard(logdir),\n",
        "    keras.callbacks.EarlyStopping(patience=5),\n",
        "    keras.callbacks.ModelCheckpoint(\"my_mnist_model.h5\", save_best_only=True),\n",
        "]\n",
        "history = model.fit(X_train_scaled, y_train, epochs=50,\n",
        "                    validation_data=(X_valid_scaled, y_valid),\n",
        "                    callbacks=callbacks)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 55000 samples, validate on 5000 samples\n",
            "Epoch 1/50\n",
            "55000/55000 [==============================] - 8s 143us/sample - loss: 0.9055 - accuracy: 0.7004 - val_loss: 0.6318 - val_accuracy: 0.7852\n",
            "Epoch 2/50\n",
            "55000/55000 [==============================] - 7s 133us/sample - loss: 0.5878 - accuracy: 0.7944 - val_loss: 0.5288 - val_accuracy: 0.8154\n",
            "Epoch 3/50\n",
            "55000/55000 [==============================] - 7s 136us/sample - loss: 0.5157 - accuracy: 0.8179 - val_loss: 0.4812 - val_accuracy: 0.8314\n",
            "Epoch 4/50\n",
            "55000/55000 [==============================] - 7s 133us/sample - loss: 0.4779 - accuracy: 0.8296 - val_loss: 0.4526 - val_accuracy: 0.8404\n",
            "Epoch 5/50\n",
            "55000/55000 [==============================] - 7s 134us/sample - loss: 0.4534 - accuracy: 0.8383 - val_loss: 0.4344 - val_accuracy: 0.8436\n",
            "Epoch 6/50\n",
            "55000/55000 [==============================] - 7s 133us/sample - loss: 0.4355 - accuracy: 0.8456 - val_loss: 0.4219 - val_accuracy: 0.8490\n",
            "Epoch 7/50\n",
            "55000/55000 [==============================] - 7s 136us/sample - loss: 0.4215 - accuracy: 0.8504 - val_loss: 0.4107 - val_accuracy: 0.8566\n",
            "Epoch 8/50\n",
            "55000/55000 [==============================] - 7s 133us/sample - loss: 0.4099 - accuracy: 0.8549 - val_loss: 0.4025 - val_accuracy: 0.8580\n",
            "Epoch 9/50\n",
            "55000/55000 [==============================] - 7s 134us/sample - loss: 0.4000 - accuracy: 0.8573 - val_loss: 0.3954 - val_accuracy: 0.8606\n",
            "Epoch 10/50\n",
            "55000/55000 [==============================] - 7s 136us/sample - loss: 0.3916 - accuracy: 0.8606 - val_loss: 0.3879 - val_accuracy: 0.8638\n",
            "Epoch 11/50\n",
            "55000/55000 [==============================] - 8s 137us/sample - loss: 0.3842 - accuracy: 0.8631 - val_loss: 0.3829 - val_accuracy: 0.8644\n",
            "Epoch 12/50\n",
            "55000/55000 [==============================] - 7s 135us/sample - loss: 0.3773 - accuracy: 0.8656 - val_loss: 0.3824 - val_accuracy: 0.8626\n",
            "Epoch 13/50\n",
            "55000/55000 [==============================] - 8s 137us/sample - loss: 0.3711 - accuracy: 0.8676 - val_loss: 0.3729 - val_accuracy: 0.8708\n",
            "Epoch 14/50\n",
            "55000/55000 [==============================] - 8s 140us/sample - loss: 0.3653 - accuracy: 0.8696 - val_loss: 0.3690 - val_accuracy: 0.8700\n",
            "Epoch 15/50\n",
            "55000/55000 [==============================] - 8s 140us/sample - loss: 0.3602 - accuracy: 0.8711 - val_loss: 0.3662 - val_accuracy: 0.8708\n",
            "Epoch 16/50\n",
            "55000/55000 [==============================] - 7s 134us/sample - loss: 0.3554 - accuracy: 0.8729 - val_loss: 0.3623 - val_accuracy: 0.8728\n",
            "Epoch 17/50\n",
            "55000/55000 [==============================] - 8s 137us/sample - loss: 0.3507 - accuracy: 0.8747 - val_loss: 0.3585 - val_accuracy: 0.8734\n",
            "Epoch 18/50\n",
            "55000/55000 [==============================] - 8s 137us/sample - loss: 0.3465 - accuracy: 0.8759 - val_loss: 0.3603 - val_accuracy: 0.8744\n",
            "Epoch 19/50\n",
            "55000/55000 [==============================] - 7s 136us/sample - loss: 0.3424 - accuracy: 0.8776 - val_loss: 0.3532 - val_accuracy: 0.8748\n",
            "Epoch 20/50\n",
            "55000/55000 [==============================] - 7s 136us/sample - loss: 0.3382 - accuracy: 0.8801 - val_loss: 0.3496 - val_accuracy: 0.8748\n",
            "Epoch 21/50\n",
            "55000/55000 [==============================] - 8s 138us/sample - loss: 0.3348 - accuracy: 0.8808 - val_loss: 0.3513 - val_accuracy: 0.8756\n",
            "Epoch 22/50\n",
            "55000/55000 [==============================] - 7s 135us/sample - loss: 0.3314 - accuracy: 0.8817 - val_loss: 0.3466 - val_accuracy: 0.8768\n",
            "Epoch 23/50\n",
            "55000/55000 [==============================] - 7s 136us/sample - loss: 0.3278 - accuracy: 0.8830 - val_loss: 0.3467 - val_accuracy: 0.8798\n",
            "Epoch 24/50\n",
            "55000/55000 [==============================] - 7s 132us/sample - loss: 0.3243 - accuracy: 0.8843 - val_loss: 0.3443 - val_accuracy: 0.8776\n",
            "Epoch 25/50\n",
            "55000/55000 [==============================] - 7s 134us/sample - loss: 0.3213 - accuracy: 0.8841 - val_loss: 0.3388 - val_accuracy: 0.8788\n",
            "Epoch 26/50\n",
            "55000/55000 [==============================] - 8s 138us/sample - loss: 0.3184 - accuracy: 0.8858 - val_loss: 0.3378 - val_accuracy: 0.8786\n",
            "Epoch 27/50\n",
            "55000/55000 [==============================] - 7s 135us/sample - loss: 0.3154 - accuracy: 0.8879 - val_loss: 0.3407 - val_accuracy: 0.8772\n",
            "Epoch 28/50\n",
            "55000/55000 [==============================] - 7s 135us/sample - loss: 0.3127 - accuracy: 0.8882 - val_loss: 0.3364 - val_accuracy: 0.8790\n",
            "Epoch 29/50\n",
            "55000/55000 [==============================] - 7s 135us/sample - loss: 0.3099 - accuracy: 0.8898 - val_loss: 0.3359 - val_accuracy: 0.8802\n",
            "Epoch 30/50\n",
            "55000/55000 [==============================] - 7s 134us/sample - loss: 0.3069 - accuracy: 0.8903 - val_loss: 0.3387 - val_accuracy: 0.8794\n",
            "Epoch 31/50\n",
            "55000/55000 [==============================] - 8s 144us/sample - loss: 0.3045 - accuracy: 0.8907 - val_loss: 0.3304 - val_accuracy: 0.8832\n",
            "Epoch 32/50\n",
            "55000/55000 [==============================] - 9s 156us/sample - loss: 0.3020 - accuracy: 0.8915 - val_loss: 0.3298 - val_accuracy: 0.8808\n",
            "Epoch 33/50\n",
            "55000/55000 [==============================] - 8s 139us/sample - loss: 0.2993 - accuracy: 0.8934 - val_loss: 0.3292 - val_accuracy: 0.8846\n",
            "Epoch 34/50\n",
            "55000/55000 [==============================] - 8s 149us/sample - loss: 0.2972 - accuracy: 0.8945 - val_loss: 0.3285 - val_accuracy: 0.8812\n",
            "Epoch 35/50\n",
            "55000/55000 [==============================] - 8s 140us/sample - loss: 0.2949 - accuracy: 0.8948 - val_loss: 0.3254 - val_accuracy: 0.8848\n",
            "Epoch 36/50\n",
            "55000/55000 [==============================] - 7s 134us/sample - loss: 0.2923 - accuracy: 0.8957 - val_loss: 0.3264 - val_accuracy: 0.8812\n",
            "Epoch 37/50\n",
            "55000/55000 [==============================] - 7s 132us/sample - loss: 0.2901 - accuracy: 0.8963 - val_loss: 0.3247 - val_accuracy: 0.8858\n",
            "Epoch 38/50\n",
            "55000/55000 [==============================] - 7s 133us/sample - loss: 0.2880 - accuracy: 0.8972 - val_loss: 0.3258 - val_accuracy: 0.8832\n",
            "Epoch 39/50\n",
            "55000/55000 [==============================] - 7s 134us/sample - loss: 0.2857 - accuracy: 0.8977 - val_loss: 0.3215 - val_accuracy: 0.8844\n",
            "Epoch 40/50\n",
            "55000/55000 [==============================] - 7s 133us/sample - loss: 0.2838 - accuracy: 0.8991 - val_loss: 0.3207 - val_accuracy: 0.8840\n",
            "Epoch 41/50\n",
            "55000/55000 [==============================] - 7s 132us/sample - loss: 0.2819 - accuracy: 0.8995 - val_loss: 0.3193 - val_accuracy: 0.8848\n",
            "Epoch 42/50\n",
            "55000/55000 [==============================] - 7s 130us/sample - loss: 0.2795 - accuracy: 0.8997 - val_loss: 0.3183 - val_accuracy: 0.8846\n",
            "Epoch 43/50\n",
            "55000/55000 [==============================] - 7s 131us/sample - loss: 0.2776 - accuracy: 0.9005 - val_loss: 0.3167 - val_accuracy: 0.8870\n",
            "Epoch 44/50\n",
            "55000/55000 [==============================] - 7s 129us/sample - loss: 0.2754 - accuracy: 0.9014 - val_loss: 0.3162 - val_accuracy: 0.8866\n",
            "Epoch 45/50\n",
            "55000/55000 [==============================] - 7s 130us/sample - loss: 0.2735 - accuracy: 0.9029 - val_loss: 0.3178 - val_accuracy: 0.8884\n",
            "Epoch 46/50\n",
            "55000/55000 [==============================] - 7s 133us/sample - loss: 0.2714 - accuracy: 0.9032 - val_loss: 0.3144 - val_accuracy: 0.8870\n",
            "Epoch 47/50\n",
            "55000/55000 [==============================] - 7s 133us/sample - loss: 0.2697 - accuracy: 0.9045 - val_loss: 0.3158 - val_accuracy: 0.8866\n",
            "Epoch 48/50\n",
            "55000/55000 [==============================] - 7s 132us/sample - loss: 0.2672 - accuracy: 0.9055 - val_loss: 0.3151 - val_accuracy: 0.8884\n",
            "Epoch 49/50\n",
            "55000/55000 [==============================] - 7s 135us/sample - loss: 0.2658 - accuracy: 0.9057 - val_loss: 0.3118 - val_accuracy: 0.8862\n",
            "Epoch 50/50\n",
            "55000/55000 [==============================] - 7s 132us/sample - loss: 0.2640 - accuracy: 0.9062 - val_loss: 0.3132 - val_accuracy: 0.8852\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AKqc9QTnRPUX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 4.2)\n",
        "Run the following code (from this [StackOverflow answer](https://stackoverflow.com/questions/38189119/simple-way-to-visualize-a-tensorflow-graph-in-jupyter/53978715#53978715)) to start a TensorBoard server and open a new tab to visualize the learning curve. When you are done, you can stop the tensorboard server by running `server.kill()`."
      ]
    },
    {
      "metadata": {
        "id": "hXDsncf_D8TS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "outputId": "9c78c083-5d83-4a28-885d-b115573ecec6"
      },
      "cell_type": "code",
      "source": [
        "!pip install tensorboard"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.6/dist-packages (1.13.0)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard) (1.14.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard) (3.0.1)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard) (0.33.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard) (3.6.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard) (1.11.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard) (0.14.1)\n",
            "Requirement already satisfied: grpcio>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard) (0.7.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.0->tensorboard) (40.8.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jj2vhTcmRPUX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import tensorboard as tb\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4HW2j_0YEDCQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "outputId": "84d4e6a2-5690-4d57-9643-596f0dfbcd72"
      },
      "cell_type": "code",
      "source": [
        "server = tf()"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-ac12d5af4d9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mserver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "_cGEC4f1RPUY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "server.kill()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oIV7dtF1RPUZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "4.3) The early stopping callback only stopped training after 10 epochs without progress, so your model may already have started to overfit the training set. Fortunately, since the `ModelCheckpoint` callback only saved the best models (on the validation set), the last saved model is the best on the validation set, so try loading it using `keras.models.load_model()`. Finally evaluate it on the test set."
      ]
    },
    {
      "metadata": {
        "id": "pmcQgqONRPUZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = keras.models.load_model(\"my_mnist_model.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0Pkqmq2dRPUa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "85d13ac7-a26e-49fa-bc0a-0de2732d9adf"
      },
      "cell_type": "code",
      "source": [
        "model.evaluate(X_valid_scaled, y_valid)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5000/5000 [==============================] - 0s 86us/sample - loss: 0.3118 - accuracy: 0.8862\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.31179461154043675, 0.8862]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "metadata": {
        "id": "S33kKUQURPUb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 4.4)\n",
        "Look at the list of available callbacks at https://keras.io/callbacks/"
      ]
    },
    {
      "metadata": {
        "id": "iW9rFLGPRPUb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![Exercise](https://c1.staticflickr.com/9/8101/8553474140_c50cf08708_b.jpg)"
      ]
    },
    {
      "metadata": {
        "id": "sUkhb3UHRPUb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Exercise 5 – A neural net for regression"
      ]
    },
    {
      "metadata": {
        "id": "iZ3sB1IaRPUb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 5.1)\n",
        "Load the California housing dataset using `sklearn.datasets.fetch_california_housing`. This returns an object with a `DESCR` attribute describing the dataset, a `data` attribute with the input features, and a `target` attribute with the labels. The goal is to predict the price of houses in a district (a census block) given some stats about that district. This is a regression task (predicting values)."
      ]
    },
    {
      "metadata": {
        "id": "h52v7Nx6RPUc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pQA9lu2YRPUc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZBqKw08kRPUd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4xtrYRwlRPUe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 5.2)\n",
        "Split the dataset into a training set, a validation set and a test set using Scikit-Learn's `sklearn.model_selection.train_test_split()` function."
      ]
    },
    {
      "metadata": {
        "id": "nTTMnElIRPUe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tINtK09nRPUf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x09BHTxzRPUg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "azQG_QByRPUh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 5.3)\n",
        "Scale the input features (e.g., using a `sklearn.preprocessing.StandardScaler`). Once again, don't forget that you should not fit the validation set or the test set, only the training set."
      ]
    },
    {
      "metadata": {
        "id": "pMYad1NXRPUh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VFm345geRPUi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l4CjYbp1RPUi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OJ2r6xLNRPUm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 5.4)\n",
        "Now build, train and evaluate a neural network to tackle this problem. Then use it to make predictions on the test set.\n",
        "\n",
        "**Tips**:\n",
        "* Since you are predicting a single value per district (the median house price), there should only be one neuron in the output layer.\n",
        "* Usually for regression tasks you don't want to use any activation function in the output layer (in some cases you may want to use `\"relu\"` or `\"softplus\"` if you want to constrain the predicted values to be positive, or `\"sigmoid\"` or `\"tanh\"` if you want to constrain the predicted values to 0-1 or -1-1).\n",
        "* A good loss function for regression is generally the `\"mean_squared_error\"` (aka `\"mse\"`). When there are many outliers in your dataset, you may prefer to use the `\"mean_absolute_error\"` (aka `\"mae\"`), which is a bit less precise but less sensitive to outliers."
      ]
    },
    {
      "metadata": {
        "id": "XU12zPk0RPUm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Iw3j8OeLRPUn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "btQbiNRURPUp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SV9ob7e2RPUq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![Exercise solution](https://camo.githubusercontent.com/250388fde3fac9135ead9471733ee28e049f7a37/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f302f30362f46696c6f735f736567756e646f5f6c6f676f5f253238666c69707065642532392e6a7067)"
      ]
    },
    {
      "metadata": {
        "id": "Z4pRXEx1RPUr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Exercise 5 – Solution"
      ]
    },
    {
      "metadata": {
        "id": "Q2tMju8wRPUr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 5.1)\n",
        "Load the California housing dataset using `sklearn.datasets.fetch_california_housing`. This returns an object with a `DESCR` attribute describing the dataset, a `data` attribute with the input features, and a `target` attribute with the labels. The goal is to predict the price of houses in a district (a census block) given some stats about that district. This is a regression task (predicting values)."
      ]
    },
    {
      "metadata": {
        "id": "Z6u0ZdQSRPUr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "d8d5d5db-393d-42f4-a3fb-5ff25d9b13a5"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "housing = fetch_california_housing()"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading Cal. housing from https://ndownloader.figshare.com/files/5976036 to /root/scikit_learn_data\n",
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "I0314 07:18:23.492846 139705429510016 california_housing.py:114] Downloading Cal. housing from https://ndownloader.figshare.com/files/5976036 to /root/scikit_learn_data\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "vHxSrhR1RPUs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        },
        "outputId": "122c28e4-d8aa-43f3-d6dc-a753319aaf05"
      },
      "cell_type": "code",
      "source": [
        "print(housing.DESCR)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".. _california_housing_dataset:\n",
            "\n",
            "California Housing dataset\n",
            "--------------------------\n",
            "\n",
            "**Data Set Characteristics:**\n",
            "\n",
            "    :Number of Instances: 20640\n",
            "\n",
            "    :Number of Attributes: 8 numeric, predictive attributes and the target\n",
            "\n",
            "    :Attribute Information:\n",
            "        - MedInc        median income in block\n",
            "        - HouseAge      median house age in block\n",
            "        - AveRooms      average number of rooms\n",
            "        - AveBedrms     average number of bedrooms\n",
            "        - Population    block population\n",
            "        - AveOccup      average house occupancy\n",
            "        - Latitude      house block latitude\n",
            "        - Longitude     house block longitude\n",
            "\n",
            "    :Missing Attribute Values: None\n",
            "\n",
            "This dataset was obtained from the StatLib repository.\n",
            "http://lib.stat.cmu.edu/datasets/\n",
            "\n",
            "The target variable is the median house value for California districts.\n",
            "\n",
            "This dataset was derived from the 1990 U.S. census, using one row per census\n",
            "block group. A block group is the smallest geographical unit for which the U.S.\n",
            "Census Bureau publishes sample data (a block group typically has a population\n",
            "of 600 to 3,000 people).\n",
            "\n",
            "It can be downloaded/loaded using the\n",
            ":func:`sklearn.datasets.fetch_california_housing` function.\n",
            "\n",
            ".. topic:: References\n",
            "\n",
            "    - Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\n",
            "      Statistics and Probability Letters, 33 (1997) 291-297\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "22qGCaCcRPUt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "8a9b1768-e264-4e93-af7d-a9e755fb1601"
      },
      "cell_type": "code",
      "source": [
        "housing.data.shape"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20640, 8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "metadata": {
        "id": "LjKT4fP8RPUt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "ad7236a2-3237-4839-fe68-6afc129d5c39"
      },
      "cell_type": "code",
      "source": [
        "housing.target.shape"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20640,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "metadata": {
        "id": "_eV0doc1RPUu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 5.2)\n",
        "Split the dataset into a training set, a validation set and a test set using Scikit-Learn's `sklearn.model_selection.train_test_split()` function."
      ]
    },
    {
      "metadata": {
        "id": "b2ki8xx4RPUv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target, random_state=42)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s4qsZAzORPUw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "98a12231-18df-43b2-a93e-4ff4bdece379"
      },
      "cell_type": "code",
      "source": [
        "len(X_train), len(X_valid), len(X_test)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11610, 3870, 5160)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "metadata": {
        "id": "zXVlGI4BRPUx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 5.3)\n",
        "Scale the input features (e.g., using a `sklearn.preprocessing.StandardScaler`). Once again, don't forget that you should not fit the validation set or the test set, only the training set."
      ]
    },
    {
      "metadata": {
        "id": "oBsFFQ_VRPUx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_valid_scaled = scaler.transform(X_valid)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G3ebysIFRPUx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 5.4)\n",
        "Now build, train and evaluate a neural network to tackle this problem. Then use it to make predictions on the test set."
      ]
    },
    {
      "metadata": {
        "id": "JpLrSnbkRPUz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
        "    keras.layers.Dense(1)\n",
        "])\n",
        "model.compile(loss=\"mean_squared_error\", optimizer=\"sgd\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IRmA2uJIRPUz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3283
        },
        "outputId": "50546838-7406-460c-a23a-bd610367014f"
      },
      "cell_type": "code",
      "source": [
        "callbacks = [keras.callbacks.EarlyStopping(patience=10)]\n",
        "history = model.fit(X_train_scaled, y_train,\n",
        "                    validation_data=(X_test_scaled, y_test), epochs=100,\n",
        "                    callbacks=callbacks)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 11610 samples, validate on 5160 samples\n",
            "Epoch 1/100\n",
            "11610/11610 [==============================] - 1s 61us/sample - loss: 2.7587 - val_loss: 1.0242\n",
            "Epoch 2/100\n",
            "11610/11610 [==============================] - 1s 50us/sample - loss: 0.8472 - val_loss: 0.7047\n",
            "Epoch 3/100\n",
            "11610/11610 [==============================] - 1s 53us/sample - loss: 0.6823 - val_loss: 0.6489\n",
            "Epoch 4/100\n",
            "11610/11610 [==============================] - 1s 49us/sample - loss: 0.6320 - val_loss: 0.6022\n",
            "Epoch 5/100\n",
            "11610/11610 [==============================] - 1s 49us/sample - loss: 0.5953 - val_loss: 0.5699\n",
            "Epoch 6/100\n",
            "11610/11610 [==============================] - 1s 49us/sample - loss: 0.5680 - val_loss: 0.5506\n",
            "Epoch 7/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.5452 - val_loss: 0.5262\n",
            "Epoch 8/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.5274 - val_loss: 0.5096\n",
            "Epoch 9/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.5127 - val_loss: 0.4965\n",
            "Epoch 10/100\n",
            "11610/11610 [==============================] - 1s 50us/sample - loss: 0.4998 - val_loss: 0.4843\n",
            "Epoch 11/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.4900 - val_loss: 0.4780\n",
            "Epoch 12/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.4802 - val_loss: 0.4680\n",
            "Epoch 13/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.4728 - val_loss: 0.4654\n",
            "Epoch 14/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.4664 - val_loss: 0.4565\n",
            "Epoch 15/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.4600 - val_loss: 0.4546\n",
            "Epoch 16/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.4551 - val_loss: 0.4479\n",
            "Epoch 17/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.4503 - val_loss: 0.4403\n",
            "Epoch 18/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.4473 - val_loss: 0.4385\n",
            "Epoch 19/100\n",
            "11610/11610 [==============================] - 1s 49us/sample - loss: 0.4430 - val_loss: 0.4342\n",
            "Epoch 20/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.4399 - val_loss: 0.4350\n",
            "Epoch 21/100\n",
            "11610/11610 [==============================] - 1s 49us/sample - loss: 0.4369 - val_loss: 0.4289\n",
            "Epoch 22/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.4345 - val_loss: 0.4273\n",
            "Epoch 23/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.4318 - val_loss: 0.4247\n",
            "Epoch 24/100\n",
            "11610/11610 [==============================] - 1s 49us/sample - loss: 0.4292 - val_loss: 0.4214\n",
            "Epoch 25/100\n",
            "11610/11610 [==============================] - 1s 49us/sample - loss: 0.4272 - val_loss: 0.4255\n",
            "Epoch 26/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.4254 - val_loss: 0.4184\n",
            "Epoch 27/100\n",
            "11610/11610 [==============================] - 1s 49us/sample - loss: 0.4237 - val_loss: 0.4167\n",
            "Epoch 28/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.4222 - val_loss: 0.4158\n",
            "Epoch 29/100\n",
            "11610/11610 [==============================] - 1s 49us/sample - loss: 0.4199 - val_loss: 0.4138\n",
            "Epoch 30/100\n",
            "11610/11610 [==============================] - 1s 49us/sample - loss: 0.4191 - val_loss: 0.4133\n",
            "Epoch 31/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.4174 - val_loss: 0.4108\n",
            "Epoch 32/100\n",
            "11610/11610 [==============================] - 1s 49us/sample - loss: 0.4161 - val_loss: 0.4136\n",
            "Epoch 33/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.4150 - val_loss: 0.4109\n",
            "Epoch 34/100\n",
            "11610/11610 [==============================] - 1s 49us/sample - loss: 0.4137 - val_loss: 0.4088\n",
            "Epoch 35/100\n",
            "11610/11610 [==============================] - 1s 49us/sample - loss: 0.4121 - val_loss: 0.4062\n",
            "Epoch 36/100\n",
            "11610/11610 [==============================] - 1s 49us/sample - loss: 0.4110 - val_loss: 0.4052\n",
            "Epoch 37/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.4099 - val_loss: 0.4039\n",
            "Epoch 38/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.4093 - val_loss: 0.4033\n",
            "Epoch 39/100\n",
            "11610/11610 [==============================] - 1s 49us/sample - loss: 0.4069 - val_loss: 0.4062\n",
            "Epoch 40/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.4069 - val_loss: 0.4012\n",
            "Epoch 41/100\n",
            "11610/11610 [==============================] - 1s 50us/sample - loss: 0.4056 - val_loss: 0.4005\n",
            "Epoch 42/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.4043 - val_loss: 0.4002\n",
            "Epoch 43/100\n",
            "11610/11610 [==============================] - 1s 49us/sample - loss: 0.4035 - val_loss: 0.3981\n",
            "Epoch 44/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.4025 - val_loss: 0.3995\n",
            "Epoch 45/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.4014 - val_loss: 0.3964\n",
            "Epoch 46/100\n",
            "11610/11610 [==============================] - 1s 49us/sample - loss: 0.4004 - val_loss: 0.3956\n",
            "Epoch 47/100\n",
            "11610/11610 [==============================] - 1s 49us/sample - loss: 0.3997 - val_loss: 0.3946\n",
            "Epoch 48/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.3986 - val_loss: 0.3960\n",
            "Epoch 49/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3973 - val_loss: 0.3926\n",
            "Epoch 50/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.3970 - val_loss: 0.3929\n",
            "Epoch 51/100\n",
            "11610/11610 [==============================] - 1s 49us/sample - loss: 0.3957 - val_loss: 0.3912\n",
            "Epoch 52/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.3943 - val_loss: 0.3951\n",
            "Epoch 53/100\n",
            "11610/11610 [==============================] - 1s 49us/sample - loss: 0.3943 - val_loss: 0.3896\n",
            "Epoch 54/100\n",
            "11610/11610 [==============================] - 1s 49us/sample - loss: 0.3933 - val_loss: 0.3894\n",
            "Epoch 55/100\n",
            "11610/11610 [==============================] - 1s 49us/sample - loss: 0.3925 - val_loss: 0.3879\n",
            "Epoch 56/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.3918 - val_loss: 0.3870\n",
            "Epoch 57/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3909 - val_loss: 0.3867\n",
            "Epoch 58/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.3902 - val_loss: 0.3865\n",
            "Epoch 59/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.3893 - val_loss: 0.3847\n",
            "Epoch 60/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.3887 - val_loss: 0.3857\n",
            "Epoch 61/100\n",
            "11610/11610 [==============================] - 1s 49us/sample - loss: 0.3881 - val_loss: 0.3844\n",
            "Epoch 62/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.3870 - val_loss: 0.3828\n",
            "Epoch 63/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.3859 - val_loss: 0.3821\n",
            "Epoch 64/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.3858 - val_loss: 0.3812\n",
            "Epoch 65/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3853 - val_loss: 0.3815\n",
            "Epoch 66/100\n",
            "11610/11610 [==============================] - 1s 49us/sample - loss: 0.3848 - val_loss: 0.3805\n",
            "Epoch 67/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.3837 - val_loss: 0.3796\n",
            "Epoch 68/100\n",
            "11610/11610 [==============================] - 1s 49us/sample - loss: 0.3838 - val_loss: 0.3796\n",
            "Epoch 69/100\n",
            "11610/11610 [==============================] - 1s 49us/sample - loss: 0.3827 - val_loss: 0.3783\n",
            "Epoch 70/100\n",
            "11610/11610 [==============================] - 1s 49us/sample - loss: 0.3818 - val_loss: 0.3781\n",
            "Epoch 71/100\n",
            "11610/11610 [==============================] - 1s 49us/sample - loss: 0.3816 - val_loss: 0.3776\n",
            "Epoch 72/100\n",
            "11610/11610 [==============================] - 1s 49us/sample - loss: 0.3811 - val_loss: 0.3779\n",
            "Epoch 73/100\n",
            "11610/11610 [==============================] - 1s 49us/sample - loss: 0.3806 - val_loss: 0.3766\n",
            "Epoch 74/100\n",
            "11610/11610 [==============================] - 1s 52us/sample - loss: 0.3796 - val_loss: 0.3764\n",
            "Epoch 75/100\n",
            "11610/11610 [==============================] - 1s 50us/sample - loss: 0.3794 - val_loss: 0.3756\n",
            "Epoch 76/100\n",
            "11610/11610 [==============================] - 1s 49us/sample - loss: 0.3787 - val_loss: 0.3750\n",
            "Epoch 77/100\n",
            "11610/11610 [==============================] - 1s 49us/sample - loss: 0.3782 - val_loss: 0.3743\n",
            "Epoch 78/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.3776 - val_loss: 0.3741\n",
            "Epoch 79/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.3772 - val_loss: 0.3735\n",
            "Epoch 80/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.3769 - val_loss: 0.3728\n",
            "Epoch 81/100\n",
            "11610/11610 [==============================] - 1s 49us/sample - loss: 0.3765 - val_loss: 0.3725\n",
            "Epoch 82/100\n",
            "11610/11610 [==============================] - 1s 51us/sample - loss: 0.3753 - val_loss: 0.3728\n",
            "Epoch 83/100\n",
            "11610/11610 [==============================] - 1s 58us/sample - loss: 0.3755 - val_loss: 0.3718\n",
            "Epoch 84/100\n",
            "11610/11610 [==============================] - 1s 57us/sample - loss: 0.3750 - val_loss: 0.3713\n",
            "Epoch 85/100\n",
            "11610/11610 [==============================] - 1s 56us/sample - loss: 0.3748 - val_loss: 0.3710\n",
            "Epoch 86/100\n",
            "11610/11610 [==============================] - 1s 58us/sample - loss: 0.3742 - val_loss: 0.3708\n",
            "Epoch 87/100\n",
            "11610/11610 [==============================] - 1s 57us/sample - loss: 0.3740 - val_loss: 0.3704\n",
            "Epoch 88/100\n",
            "11610/11610 [==============================] - 1s 56us/sample - loss: 0.3734 - val_loss: 0.3700\n",
            "Epoch 89/100\n",
            "11610/11610 [==============================] - 1s 57us/sample - loss: 0.3731 - val_loss: 0.3696\n",
            "Epoch 90/100\n",
            "11610/11610 [==============================] - 1s 56us/sample - loss: 0.3724 - val_loss: 0.3693\n",
            "Epoch 91/100\n",
            "11610/11610 [==============================] - 1s 56us/sample - loss: 0.3725 - val_loss: 0.3690\n",
            "Epoch 92/100\n",
            "11610/11610 [==============================] - 1s 55us/sample - loss: 0.3718 - val_loss: 0.3692\n",
            "Epoch 93/100\n",
            "11610/11610 [==============================] - 1s 56us/sample - loss: 0.3710 - val_loss: 0.3690\n",
            "Epoch 94/100\n",
            "11610/11610 [==============================] - 1s 56us/sample - loss: 0.3713 - val_loss: 0.3684\n",
            "Epoch 95/100\n",
            "11610/11610 [==============================] - 1s 56us/sample - loss: 0.3708 - val_loss: 0.3681\n",
            "Epoch 96/100\n",
            "11610/11610 [==============================] - 1s 56us/sample - loss: 0.3707 - val_loss: 0.3679\n",
            "Epoch 97/100\n",
            "11610/11610 [==============================] - 1s 64us/sample - loss: 0.3701 - val_loss: 0.3674\n",
            "Epoch 98/100\n",
            "11610/11610 [==============================] - 1s 67us/sample - loss: 0.3700 - val_loss: 0.3670\n",
            "Epoch 99/100\n",
            "11610/11610 [==============================] - 1s 59us/sample - loss: 0.3696 - val_loss: 0.3670\n",
            "Epoch 100/100\n",
            "11610/11610 [==============================] - 1s 58us/sample - loss: 0.3693 - val_loss: 0.3667\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NjvLzqEKRPU0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "93a22338-5a7c-448f-ebeb-8cf94a58d6cf"
      },
      "cell_type": "code",
      "source": [
        "model.evaluate(X_test_scaled, y_test)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5160/5160 [==============================] - 0s 25us/sample - loss: 0.3667\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3666810190723848"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "metadata": {
        "id": "QHFtKObWRPU1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "7a8527fb-be77-47a5-e524-9e7257c9db00"
      },
      "cell_type": "code",
      "source": [
        "model.predict(X_test_scaled)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.7650698],\n",
              "       [1.7622952],\n",
              "       [3.9009204],\n",
              "       ...,\n",
              "       [1.5346807],\n",
              "       [2.4927392],\n",
              "       [3.9904246]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "6Z70mSYPRPU2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        },
        "outputId": "aa1410b8-05ba-4497-d413-2a308bca381f"
      },
      "cell_type": "code",
      "source": [
        "plot_learning_curves(history)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeEAAAEzCAYAAAAGvLsqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nE6QEE1xtAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "h9Vfd0VSRPU3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![Exercise](https://c1.staticflickr.com/9/8101/8553474140_c50cf08708_b.jpg)"
      ]
    },
    {
      "metadata": {
        "id": "spC-tgEkRPU3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Exercise 6 – Hyperparameter search"
      ]
    },
    {
      "metadata": {
        "id": "KXVy4tICRPU3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 6.1)\n",
        "Try training your model multiple times, with different a learning rate each time (e.g., 1e-4, 3e-4, 1e-3, 3e-3, 3e-2), and compare the learning curves. For this, you need to create a `keras.optimizers.SGD` optimizer and specify the `learning_rate` in its constructor, then pass this `SGD` instance to the `compile()` method using the `optimizer` argument."
      ]
    },
    {
      "metadata": {
        "id": "RxjiOK7-RPU3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gC1gj6CXRPU4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bgKL5EssRPU5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VHNI4KtiRPU5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 6.2)\n",
        "Let's look at a more sophisticated way to tune hyperparameters. Create a `build_model()` function that takes three arguments, `n_hidden`, `n_neurons`, `learning_rate`, and builds, compiles and returns a model with the given number of hidden layers, the given number of neurons and the given learning rate. It is good practice to give a reasonable default value to each argument."
      ]
    },
    {
      "metadata": {
        "id": "yzI2uGl6RPU6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-wsos2zjRPU7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8ZR4wueqRPU8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GK5cEbagRPU8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 6.3)\n",
        "Create a `keras.wrappers.scikit_learn.KerasRegressor` and pass the `build_model` function to the constructor. This gives you a Scikit-Learn compatible predictor. Try training it and using it to make predictions. Note that you can pass the `n_epochs`, `callbacks` and `validation_data` to the `fit()` method."
      ]
    },
    {
      "metadata": {
        "id": "o8ftwuH-RPU9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M7dy12aqRPU9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Hz6X76_dRPU-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aSz_7o9uRPU_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 6.4)\n",
        "Use a `sklearn.model_selection.RandomizedSearchCV` to search the hyperparameter space of your `KerasRegressor`.\n",
        "\n",
        "**Tips**:\n",
        "* create a `param_distribs` dictionary where each key is the name of a hyperparameter you want to fine-tune (e.g., `\"n_hidden\"`), and each value is the list of values you want to explore (e.g., `[0, 1, 2, 3]`), or a Scipy distribution from `scipy.stats`.\n",
        "* You can use the reciprocal distribution for the learning rate (e.g, `reciprocal(3e-3, 3e-2)`).\n",
        "* Create a `RandomizedSearchCV`, passing the `KerasRegressor` and the `param_distribs` to its constructor, as well as the number of iterations (`n_iter`), and the number of cross-validation folds (`cv`). If you are short on time, you can set `n_iter=10` and `cv=3`. You may also want to set `verbose=2`.\n",
        "* Finally, call the `RandomizedSearchCV`'s `fit()` method on the training set. Once again you can pass it `n_epochs`, `validation_data` and `callbacks` if you want to.\n",
        "* The best parameters found will be available in the `best_params_` attribute, the best score will be in `best_score_`, and the best model will be in `best_estimator_`."
      ]
    },
    {
      "metadata": {
        "id": "4Ha4-pQvRPU_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jYSVsgUuRPVA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ER2fMw5cRPVB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rz9MEwcYRPVB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 6.5)\n",
        "Evaluate the best model found on the test set. You can either use the best estimator's `score()` method, or get its underlying Keras model *via* its `model` attribute, and call this model's `evaluate()` method. Note that the estimator returns the negative mean square error (it's a score, not a loss, so higher is better)."
      ]
    },
    {
      "metadata": {
        "id": "3RspJiT0RPVC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pGsxF1QORPVC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "42ivTfuGRPVD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YxWcFsviRPVD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 6.6)\n",
        "Finally, save the best Keras model found. **Tip**: it is available via the best estimator's `model` attribute, and just need to call its `save()` method."
      ]
    },
    {
      "metadata": {
        "id": "Wfx0s6FpRPVE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vqE3xoTTRPVG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DdizSlmpRPVH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "k1d9T3x8RPVK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Tip**: while a randomized search is nice and simple, there are more powerful (but complex) options available out there for hyperparameter search, for example:\n",
        "* [Hyperopt](https://github.com/hyperopt/hyperopt)\n",
        "* [Hyperas](https://github.com/maxpumperla/hyperas)\n",
        "* [Sklearn-Deap](https://github.com/rsteca/sklearn-deap)\n",
        "* [Scikit-Optimize](https://scikit-optimize.github.io/)\n",
        "* [Spearmint](https://github.com/JasperSnoek/spearmint)\n",
        "* [PyMC3](https://docs.pymc.io/)\n",
        "* [GPFlow](https://gpflow.readthedocs.io/)\n",
        "* [Yelp/MOE](https://github.com/Yelp/MOE)\n",
        "* Commercial services such as: [Google Cloud ML Engine](https://cloud.google.com/ml-engine/docs/tensorflow/using-hyperparameter-tuning), [Arimo](https://arimo.com/) or [Oscar](http://oscar.calldesk.ai/)"
      ]
    },
    {
      "metadata": {
        "id": "hEV5vY9HRPVK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![Exercise solution](https://camo.githubusercontent.com/250388fde3fac9135ead9471733ee28e049f7a37/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f302f30362f46696c6f735f736567756e646f5f6c6f676f5f253238666c69707065642532392e6a7067)"
      ]
    },
    {
      "metadata": {
        "id": "ZPiVESaMRPVK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Exercise 6 – Solution"
      ]
    },
    {
      "metadata": {
        "id": "aBczbliMRPVL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 6.1)\n",
        "Try training your model multiple times, with different a learning rate each time (e.g., 1e-4, 3e-4, 1e-3, 3e-3, 3e-2), and compare the learning curves. For this, you need to create a `keras.optimizers.SGD` optimizer and specify the `learning_rate` in its constructor, then pass this `SGD` instance to the `compile()` method using the `optimizer` argument."
      ]
    },
    {
      "metadata": {
        "id": "Wt7wAUMYRPVL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17405
        },
        "outputId": "677bec9a-7bc8-404c-da25-9ec347348888"
      },
      "cell_type": "code",
      "source": [
        "learning_rates = [1e-4, 3e-4, 1e-3, 3e-3, 1e-2, 3e-2]\n",
        "histories = []\n",
        "for learning_rate in learning_rates:\n",
        "    model = keras.models.Sequential([\n",
        "        keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
        "        keras.layers.Dense(1)\n",
        "    ])\n",
        "    optimizer = keras.optimizers.SGD(learning_rate)\n",
        "    model.compile(loss=\"mean_squared_error\", optimizer=optimizer)\n",
        "    callbacks = [keras.callbacks.EarlyStopping(patience=10)]\n",
        "    history = model.fit(X_train_scaled, y_train,\n",
        "                        validation_data=(X_test_scaled, y_test), epochs=100,\n",
        "                        callbacks=callbacks)\n",
        "    histories.append(history)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 11610 samples, validate on 5160 samples\n",
            "Epoch 1/100\n",
            "11610/11610 [==============================] - 1s 57us/sample - loss: 4.7070 - val_loss: 3.9082\n",
            "Epoch 2/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 3.4921 - val_loss: 2.9626\n",
            "Epoch 3/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 2.7114 - val_loss: 2.3413\n",
            "Epoch 4/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 2.1870 - val_loss: 1.9224\n",
            "Epoch 5/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 1.8220 - val_loss: 1.6287\n",
            "Epoch 6/100\n",
            "11610/11610 [==============================] - 1s 45us/sample - loss: 1.5632 - val_loss: 1.4202\n",
            "Epoch 7/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 1.3770 - val_loss: 1.2686\n",
            "Epoch 8/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 1.2396 - val_loss: 1.1552\n",
            "Epoch 9/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 1.1360 - val_loss: 1.0699\n",
            "Epoch 10/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 1.0573 - val_loss: 1.0046\n",
            "Epoch 11/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.9967 - val_loss: 0.9540\n",
            "Epoch 12/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.9494 - val_loss: 0.9142\n",
            "Epoch 13/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.9121 - val_loss: 0.8824\n",
            "Epoch 14/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.8823 - val_loss: 0.8569\n",
            "Epoch 15/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.8583 - val_loss: 0.8360\n",
            "Epoch 16/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.8386 - val_loss: 0.8185\n",
            "Epoch 17/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.8222 - val_loss: 0.8038\n",
            "Epoch 18/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.8083 - val_loss: 0.7910\n",
            "Epoch 19/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.7963 - val_loss: 0.7799\n",
            "Epoch 20/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.7859 - val_loss: 0.7700\n",
            "Epoch 21/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.7766 - val_loss: 0.7611\n",
            "Epoch 22/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.7682 - val_loss: 0.7530\n",
            "Epoch 23/100\n",
            "11610/11610 [==============================] - 1s 49us/sample - loss: 0.7606 - val_loss: 0.7455\n",
            "Epoch 24/100\n",
            "11610/11610 [==============================] - 1s 49us/sample - loss: 0.7536 - val_loss: 0.7385\n",
            "Epoch 25/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.7470 - val_loss: 0.7319\n",
            "Epoch 26/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.7409 - val_loss: 0.7257\n",
            "Epoch 27/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.7351 - val_loss: 0.7198\n",
            "Epoch 28/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.7296 - val_loss: 0.7142\n",
            "Epoch 29/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.7243 - val_loss: 0.7088\n",
            "Epoch 30/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.7193 - val_loss: 0.7036\n",
            "Epoch 31/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.7144 - val_loss: 0.6986\n",
            "Epoch 32/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.7097 - val_loss: 0.6938\n",
            "Epoch 33/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.7052 - val_loss: 0.6891\n",
            "Epoch 34/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.7008 - val_loss: 0.6846\n",
            "Epoch 35/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.6965 - val_loss: 0.6802\n",
            "Epoch 36/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.6924 - val_loss: 0.6759\n",
            "Epoch 37/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.6884 - val_loss: 0.6717\n",
            "Epoch 38/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.6845 - val_loss: 0.6677\n",
            "Epoch 39/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.6806 - val_loss: 0.6637\n",
            "Epoch 40/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.6769 - val_loss: 0.6599\n",
            "Epoch 41/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.6733 - val_loss: 0.6561\n",
            "Epoch 42/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.6697 - val_loss: 0.6524\n",
            "Epoch 43/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.6662 - val_loss: 0.6488\n",
            "Epoch 44/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.6628 - val_loss: 0.6453\n",
            "Epoch 45/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.6594 - val_loss: 0.6418\n",
            "Epoch 46/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.6562 - val_loss: 0.6384\n",
            "Epoch 47/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.6529 - val_loss: 0.6351\n",
            "Epoch 48/100\n",
            "11610/11610 [==============================] - 1s 49us/sample - loss: 0.6497 - val_loss: 0.6318\n",
            "Epoch 49/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.6466 - val_loss: 0.6286\n",
            "Epoch 50/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.6436 - val_loss: 0.6255\n",
            "Epoch 51/100\n",
            "11610/11610 [==============================] - 1s 49us/sample - loss: 0.6406 - val_loss: 0.6224\n",
            "Epoch 52/100\n",
            "11610/11610 [==============================] - 1s 54us/sample - loss: 0.6376 - val_loss: 0.6193\n",
            "Epoch 53/100\n",
            "11610/11610 [==============================] - 1s 55us/sample - loss: 0.6347 - val_loss: 0.6164\n",
            "Epoch 54/100\n",
            "11610/11610 [==============================] - 1s 54us/sample - loss: 0.6319 - val_loss: 0.6134\n",
            "Epoch 55/100\n",
            "11610/11610 [==============================] - 1s 55us/sample - loss: 0.6291 - val_loss: 0.6106\n",
            "Epoch 56/100\n",
            "11610/11610 [==============================] - 1s 67us/sample - loss: 0.6263 - val_loss: 0.6078\n",
            "Epoch 57/100\n",
            "11610/11610 [==============================] - 1s 74us/sample - loss: 0.6236 - val_loss: 0.6050\n",
            "Epoch 58/100\n",
            "11610/11610 [==============================] - 1s 74us/sample - loss: 0.6210 - val_loss: 0.6022\n",
            "Epoch 59/100\n",
            "11610/11610 [==============================] - 1s 74us/sample - loss: 0.6184 - val_loss: 0.5995\n",
            "Epoch 60/100\n",
            "11610/11610 [==============================] - 1s 73us/sample - loss: 0.6158 - val_loss: 0.5969\n",
            "Epoch 61/100\n",
            "11610/11610 [==============================] - 1s 75us/sample - loss: 0.6133 - val_loss: 0.5943\n",
            "Epoch 62/100\n",
            "11610/11610 [==============================] - 1s 73us/sample - loss: 0.6107 - val_loss: 0.5917\n",
            "Epoch 63/100\n",
            "11610/11610 [==============================] - 1s 69us/sample - loss: 0.6083 - val_loss: 0.5892\n",
            "Epoch 64/100\n",
            "11610/11610 [==============================] - 1s 53us/sample - loss: 0.6058 - val_loss: 0.5868\n",
            "Epoch 65/100\n",
            "11610/11610 [==============================] - 1s 52us/sample - loss: 0.6034 - val_loss: 0.5843\n",
            "Epoch 66/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.6011 - val_loss: 0.5819\n",
            "Epoch 67/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.5987 - val_loss: 0.5796\n",
            "Epoch 68/100\n",
            "11610/11610 [==============================] - 1s 45us/sample - loss: 0.5965 - val_loss: 0.5773\n",
            "Epoch 69/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.5942 - val_loss: 0.5750\n",
            "Epoch 70/100\n",
            "11610/11610 [==============================] - 1s 45us/sample - loss: 0.5920 - val_loss: 0.5727\n",
            "Epoch 71/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.5898 - val_loss: 0.5705\n",
            "Epoch 72/100\n",
            "11610/11610 [==============================] - 1s 45us/sample - loss: 0.5876 - val_loss: 0.5683\n",
            "Epoch 73/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.5855 - val_loss: 0.5661\n",
            "Epoch 74/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.5834 - val_loss: 0.5640\n",
            "Epoch 75/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.5813 - val_loss: 0.5619\n",
            "Epoch 76/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.5793 - val_loss: 0.5598\n",
            "Epoch 77/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.5773 - val_loss: 0.5578\n",
            "Epoch 78/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.5753 - val_loss: 0.5558\n",
            "Epoch 79/100\n",
            "11610/11610 [==============================] - 1s 49us/sample - loss: 0.5734 - val_loss: 0.5538\n",
            "Epoch 80/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.5714 - val_loss: 0.5519\n",
            "Epoch 81/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.5695 - val_loss: 0.5500\n",
            "Epoch 82/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.5676 - val_loss: 0.5481\n",
            "Epoch 83/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.5658 - val_loss: 0.5463\n",
            "Epoch 84/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.5639 - val_loss: 0.5445\n",
            "Epoch 85/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.5621 - val_loss: 0.5427\n",
            "Epoch 86/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.5604 - val_loss: 0.5409\n",
            "Epoch 87/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.5586 - val_loss: 0.5392\n",
            "Epoch 88/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.5569 - val_loss: 0.5375\n",
            "Epoch 89/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.5552 - val_loss: 0.5358\n",
            "Epoch 90/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.5536 - val_loss: 0.5342\n",
            "Epoch 91/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.5519 - val_loss: 0.5326\n",
            "Epoch 92/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.5503 - val_loss: 0.5310\n",
            "Epoch 93/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.5487 - val_loss: 0.5294\n",
            "Epoch 94/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.5472 - val_loss: 0.5279\n",
            "Epoch 95/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.5456 - val_loss: 0.5264\n",
            "Epoch 96/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.5441 - val_loss: 0.5248\n",
            "Epoch 97/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.5426 - val_loss: 0.5234\n",
            "Epoch 98/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.5411 - val_loss: 0.5219\n",
            "Epoch 99/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.5396 - val_loss: 0.5205\n",
            "Epoch 100/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.5382 - val_loss: 0.5191\n",
            "Train on 11610 samples, validate on 5160 samples\n",
            "Epoch 1/100\n",
            "11610/11610 [==============================] - 1s 57us/sample - loss: 4.1705 - val_loss: 2.4842\n",
            "Epoch 2/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 1.8498 - val_loss: 1.3520\n",
            "Epoch 3/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 1.1466 - val_loss: 0.9624\n",
            "Epoch 4/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.8845 - val_loss: 0.8031\n",
            "Epoch 5/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.7709 - val_loss: 0.7288\n",
            "Epoch 6/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.7156 - val_loss: 0.6885\n",
            "Epoch 7/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.6843 - val_loss: 0.6627\n",
            "Epoch 8/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.6631 - val_loss: 0.6440\n",
            "Epoch 9/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.6469 - val_loss: 0.6289\n",
            "Epoch 10/100\n",
            "11610/11610 [==============================] - 1s 49us/sample - loss: 0.6331 - val_loss: 0.6158\n",
            "Epoch 11/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.6210 - val_loss: 0.6044\n",
            "Epoch 12/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.6099 - val_loss: 0.5937\n",
            "Epoch 13/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.5997 - val_loss: 0.5841\n",
            "Epoch 14/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.5901 - val_loss: 0.5751\n",
            "Epoch 15/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.5811 - val_loss: 0.5667\n",
            "Epoch 16/100\n",
            "11610/11610 [==============================] - 1s 49us/sample - loss: 0.5727 - val_loss: 0.5589\n",
            "Epoch 17/100\n",
            "11610/11610 [==============================] - 1s 45us/sample - loss: 0.5648 - val_loss: 0.5517\n",
            "Epoch 18/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.5574 - val_loss: 0.5448\n",
            "Epoch 19/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.5504 - val_loss: 0.5387\n",
            "Epoch 20/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.5439 - val_loss: 0.5326\n",
            "Epoch 21/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.5377 - val_loss: 0.5269\n",
            "Epoch 22/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.5318 - val_loss: 0.5217\n",
            "Epoch 23/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.5263 - val_loss: 0.5163\n",
            "Epoch 24/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.5210 - val_loss: 0.5116\n",
            "Epoch 25/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.5161 - val_loss: 0.5071\n",
            "Epoch 26/100\n",
            "11610/11610 [==============================] - 1s 45us/sample - loss: 0.5114 - val_loss: 0.5028\n",
            "Epoch 27/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.5069 - val_loss: 0.4989\n",
            "Epoch 28/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.5027 - val_loss: 0.4950\n",
            "Epoch 29/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.4987 - val_loss: 0.4912\n",
            "Epoch 30/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.4949 - val_loss: 0.4882\n",
            "Epoch 31/100\n",
            "11610/11610 [==============================] - 1s 45us/sample - loss: 0.4914 - val_loss: 0.4850\n",
            "Epoch 32/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.4880 - val_loss: 0.4818\n",
            "Epoch 33/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.4848 - val_loss: 0.4787\n",
            "Epoch 34/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.4817 - val_loss: 0.4762\n",
            "Epoch 35/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.4788 - val_loss: 0.4734\n",
            "Epoch 36/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.4760 - val_loss: 0.4709\n",
            "Epoch 37/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.4733 - val_loss: 0.4682\n",
            "Epoch 38/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.4708 - val_loss: 0.4663\n",
            "Epoch 39/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.4684 - val_loss: 0.4640\n",
            "Epoch 40/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.4661 - val_loss: 0.4617\n",
            "Epoch 41/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.4639 - val_loss: 0.4599\n",
            "Epoch 42/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.4618 - val_loss: 0.4574\n",
            "Epoch 43/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.4598 - val_loss: 0.4556\n",
            "Epoch 44/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.4579 - val_loss: 0.4539\n",
            "Epoch 45/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.4560 - val_loss: 0.4522\n",
            "Epoch 46/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.4542 - val_loss: 0.4501\n",
            "Epoch 47/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.4526 - val_loss: 0.4487\n",
            "Epoch 48/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.4509 - val_loss: 0.4471\n",
            "Epoch 49/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.4493 - val_loss: 0.4458\n",
            "Epoch 50/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.4478 - val_loss: 0.4441\n",
            "Epoch 51/100\n",
            "11610/11610 [==============================] - 1s 45us/sample - loss: 0.4464 - val_loss: 0.4426\n",
            "Epoch 52/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.4449 - val_loss: 0.4410\n",
            "Epoch 53/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.4436 - val_loss: 0.4398\n",
            "Epoch 54/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.4423 - val_loss: 0.4388\n",
            "Epoch 55/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.4410 - val_loss: 0.4374\n",
            "Epoch 56/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.4398 - val_loss: 0.4362\n",
            "Epoch 57/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.4386 - val_loss: 0.4352\n",
            "Epoch 58/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.4375 - val_loss: 0.4341\n",
            "Epoch 59/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.4364 - val_loss: 0.4327\n",
            "Epoch 60/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.4354 - val_loss: 0.4316\n",
            "Epoch 61/100\n",
            "11610/11610 [==============================] - 1s 49us/sample - loss: 0.4344 - val_loss: 0.4307\n",
            "Epoch 62/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.4334 - val_loss: 0.4297\n",
            "Epoch 63/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.4324 - val_loss: 0.4290\n",
            "Epoch 64/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.4315 - val_loss: 0.4281\n",
            "Epoch 65/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.4307 - val_loss: 0.4272\n",
            "Epoch 66/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.4298 - val_loss: 0.4263\n",
            "Epoch 67/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.4289 - val_loss: 0.4252\n",
            "Epoch 68/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.4281 - val_loss: 0.4242\n",
            "Epoch 69/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.4273 - val_loss: 0.4236\n",
            "Epoch 70/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.4265 - val_loss: 0.4228\n",
            "Epoch 71/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.4258 - val_loss: 0.4217\n",
            "Epoch 72/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.4250 - val_loss: 0.4210\n",
            "Epoch 73/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.4243 - val_loss: 0.4206\n",
            "Epoch 74/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.4236 - val_loss: 0.4201\n",
            "Epoch 75/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.4229 - val_loss: 0.4190\n",
            "Epoch 76/100\n",
            "11610/11610 [==============================] - 1s 45us/sample - loss: 0.4222 - val_loss: 0.4183\n",
            "Epoch 77/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.4215 - val_loss: 0.4176\n",
            "Epoch 78/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.4209 - val_loss: 0.4171\n",
            "Epoch 79/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.4202 - val_loss: 0.4166\n",
            "Epoch 80/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.4196 - val_loss: 0.4161\n",
            "Epoch 81/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.4189 - val_loss: 0.4154\n",
            "Epoch 82/100\n",
            "11610/11610 [==============================] - 1s 45us/sample - loss: 0.4183 - val_loss: 0.4145\n",
            "Epoch 83/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.4177 - val_loss: 0.4136\n",
            "Epoch 84/100\n",
            "11610/11610 [==============================] - 1s 45us/sample - loss: 0.4171 - val_loss: 0.4131\n",
            "Epoch 85/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.4165 - val_loss: 0.4127\n",
            "Epoch 86/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.4159 - val_loss: 0.4118\n",
            "Epoch 87/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.4154 - val_loss: 0.4113\n",
            "Epoch 88/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.4148 - val_loss: 0.4106\n",
            "Epoch 89/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.4142 - val_loss: 0.4104\n",
            "Epoch 90/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.4137 - val_loss: 0.4097\n",
            "Epoch 91/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.4131 - val_loss: 0.4091\n",
            "Epoch 92/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.4126 - val_loss: 0.4085\n",
            "Epoch 93/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.4120 - val_loss: 0.4080\n",
            "Epoch 94/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.4115 - val_loss: 0.4072\n",
            "Epoch 95/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.4110 - val_loss: 0.4066\n",
            "Epoch 96/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.4105 - val_loss: 0.4060\n",
            "Epoch 97/100\n",
            "11610/11610 [==============================] - 1s 45us/sample - loss: 0.4100 - val_loss: 0.4054\n",
            "Epoch 98/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.4095 - val_loss: 0.4048\n",
            "Epoch 99/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.4089 - val_loss: 0.4045\n",
            "Epoch 100/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.4084 - val_loss: 0.4041\n",
            "Train on 11610 samples, validate on 5160 samples\n",
            "Epoch 1/100\n",
            "11610/11610 [==============================] - 1s 58us/sample - loss: 2.1924 - val_loss: 0.8020\n",
            "Epoch 2/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.6933 - val_loss: 0.6391\n",
            "Epoch 3/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.6123 - val_loss: 0.5890\n",
            "Epoch 4/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.5736 - val_loss: 0.5547\n",
            "Epoch 5/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.5454 - val_loss: 0.5288\n",
            "Epoch 6/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.5229 - val_loss: 0.5093\n",
            "Epoch 7/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.5040 - val_loss: 0.4921\n",
            "Epoch 8/100\n",
            "11610/11610 [==============================] - 1s 49us/sample - loss: 0.4889 - val_loss: 0.4796\n",
            "Epoch 9/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.4762 - val_loss: 0.4671\n",
            "Epoch 10/100\n",
            "11610/11610 [==============================] - 1s 49us/sample - loss: 0.4655 - val_loss: 0.4571\n",
            "Epoch 11/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.4564 - val_loss: 0.4489\n",
            "Epoch 12/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.4485 - val_loss: 0.4444\n",
            "Epoch 13/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.4425 - val_loss: 0.4364\n",
            "Epoch 14/100\n",
            "11610/11610 [==============================] - 1s 49us/sample - loss: 0.4368 - val_loss: 0.4306\n",
            "Epoch 15/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.4320 - val_loss: 0.4279\n",
            "Epoch 16/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.4278 - val_loss: 0.4249\n",
            "Epoch 17/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.4241 - val_loss: 0.4208\n",
            "Epoch 18/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.4208 - val_loss: 0.4167\n",
            "Epoch 19/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.4179 - val_loss: 0.4134\n",
            "Epoch 20/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.4151 - val_loss: 0.4122\n",
            "Epoch 21/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.4127 - val_loss: 0.4102\n",
            "Epoch 22/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.4104 - val_loss: 0.4079\n",
            "Epoch 23/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.4082 - val_loss: 0.4055\n",
            "Epoch 24/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.4063 - val_loss: 0.4036\n",
            "Epoch 25/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.4045 - val_loss: 0.3999\n",
            "Epoch 26/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.4026 - val_loss: 0.3981\n",
            "Epoch 27/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.4009 - val_loss: 0.3971\n",
            "Epoch 28/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3992 - val_loss: 0.3970\n",
            "Epoch 29/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.3980 - val_loss: 0.3934\n",
            "Epoch 30/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3967 - val_loss: 0.3930\n",
            "Epoch 31/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3952 - val_loss: 0.3919\n",
            "Epoch 32/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3940 - val_loss: 0.3904\n",
            "Epoch 33/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.3929 - val_loss: 0.3891\n",
            "Epoch 34/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.3918 - val_loss: 0.3891\n",
            "Epoch 35/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3904 - val_loss: 0.3858\n",
            "Epoch 36/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3895 - val_loss: 0.3866\n",
            "Epoch 37/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3886 - val_loss: 0.3845\n",
            "Epoch 38/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3873 - val_loss: 0.3841\n",
            "Epoch 39/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.3864 - val_loss: 0.3822\n",
            "Epoch 40/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3856 - val_loss: 0.3826\n",
            "Epoch 41/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3846 - val_loss: 0.3806\n",
            "Epoch 42/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3839 - val_loss: 0.3801\n",
            "Epoch 43/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3832 - val_loss: 0.3795\n",
            "Epoch 44/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3824 - val_loss: 0.3786\n",
            "Epoch 45/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3814 - val_loss: 0.3784\n",
            "Epoch 46/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3809 - val_loss: 0.3776\n",
            "Epoch 47/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3802 - val_loss: 0.3770\n",
            "Epoch 48/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3795 - val_loss: 0.3765\n",
            "Epoch 49/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.3788 - val_loss: 0.3761\n",
            "Epoch 50/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.3781 - val_loss: 0.3750\n",
            "Epoch 51/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.3777 - val_loss: 0.3749\n",
            "Epoch 52/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3770 - val_loss: 0.3744\n",
            "Epoch 53/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3764 - val_loss: 0.3742\n",
            "Epoch 54/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3760 - val_loss: 0.3731\n",
            "Epoch 55/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.3755 - val_loss: 0.3728\n",
            "Epoch 56/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.3750 - val_loss: 0.3724\n",
            "Epoch 57/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3744 - val_loss: 0.3717\n",
            "Epoch 58/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3741 - val_loss: 0.3716\n",
            "Epoch 59/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3735 - val_loss: 0.3714\n",
            "Epoch 60/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3732 - val_loss: 0.3712\n",
            "Epoch 61/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.3726 - val_loss: 0.3708\n",
            "Epoch 62/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.3723 - val_loss: 0.3703\n",
            "Epoch 63/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3717 - val_loss: 0.3702\n",
            "Epoch 64/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3715 - val_loss: 0.3693\n",
            "Epoch 65/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3710 - val_loss: 0.3689\n",
            "Epoch 66/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3706 - val_loss: 0.3693\n",
            "Epoch 67/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3703 - val_loss: 0.3688\n",
            "Epoch 68/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3699 - val_loss: 0.3685\n",
            "Epoch 69/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3696 - val_loss: 0.3678\n",
            "Epoch 70/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.3692 - val_loss: 0.3679\n",
            "Epoch 71/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.3689 - val_loss: 0.3671\n",
            "Epoch 72/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3683 - val_loss: 0.3674\n",
            "Epoch 73/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3681 - val_loss: 0.3665\n",
            "Epoch 74/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3679 - val_loss: 0.3662\n",
            "Epoch 75/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3676 - val_loss: 0.3661\n",
            "Epoch 76/100\n",
            "11610/11610 [==============================] - 1s 45us/sample - loss: 0.3671 - val_loss: 0.3657\n",
            "Epoch 77/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3668 - val_loss: 0.3657\n",
            "Epoch 78/100\n",
            "11610/11610 [==============================] - 1s 45us/sample - loss: 0.3665 - val_loss: 0.3652\n",
            "Epoch 79/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3662 - val_loss: 0.3650\n",
            "Epoch 80/100\n",
            "11610/11610 [==============================] - 1s 45us/sample - loss: 0.3658 - val_loss: 0.3646\n",
            "Epoch 81/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.3655 - val_loss: 0.3652\n",
            "Epoch 82/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3652 - val_loss: 0.3642\n",
            "Epoch 83/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3648 - val_loss: 0.3640\n",
            "Epoch 84/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3646 - val_loss: 0.3635\n",
            "Epoch 85/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3640 - val_loss: 0.3633\n",
            "Epoch 86/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3637 - val_loss: 0.3632\n",
            "Epoch 87/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3632 - val_loss: 0.3631\n",
            "Epoch 88/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3628 - val_loss: 0.3627\n",
            "Epoch 89/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.3626 - val_loss: 0.3625\n",
            "Epoch 90/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.3622 - val_loss: 0.3622\n",
            "Epoch 91/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3617 - val_loss: 0.3617\n",
            "Epoch 92/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3614 - val_loss: 0.3616\n",
            "Epoch 93/100\n",
            "11610/11610 [==============================] - 1s 45us/sample - loss: 0.3610 - val_loss: 0.3613\n",
            "Epoch 94/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3607 - val_loss: 0.3613\n",
            "Epoch 95/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3604 - val_loss: 0.3611\n",
            "Epoch 96/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3599 - val_loss: 0.3611\n",
            "Epoch 97/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3597 - val_loss: 0.3605\n",
            "Epoch 98/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3592 - val_loss: 0.3605\n",
            "Epoch 99/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3589 - val_loss: 0.3603\n",
            "Epoch 100/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3586 - val_loss: 0.3602\n",
            "Train on 11610 samples, validate on 5160 samples\n",
            "Epoch 1/100\n",
            "11610/11610 [==============================] - 1s 57us/sample - loss: 1.2119 - val_loss: 0.7726\n",
            "Epoch 2/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.8674 - val_loss: 0.5656\n",
            "Epoch 3/100\n",
            "11610/11610 [==============================] - 1s 45us/sample - loss: 0.5253 - val_loss: 0.4804\n",
            "Epoch 4/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.4733 - val_loss: 0.4453\n",
            "Epoch 5/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.4470 - val_loss: 0.4263\n",
            "Epoch 6/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.4315 - val_loss: 0.4136\n",
            "Epoch 7/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.4206 - val_loss: 0.4062\n",
            "Epoch 8/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.4128 - val_loss: 0.4001\n",
            "Epoch 9/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.4071 - val_loss: 0.3947\n",
            "Epoch 10/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.4027 - val_loss: 0.3912\n",
            "Epoch 11/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3988 - val_loss: 0.3885\n",
            "Epoch 12/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3959 - val_loss: 0.3857\n",
            "Epoch 13/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3931 - val_loss: 0.3846\n",
            "Epoch 14/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.3914 - val_loss: 0.3820\n",
            "Epoch 15/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3890 - val_loss: 0.3806\n",
            "Epoch 16/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3871 - val_loss: 0.3794\n",
            "Epoch 17/100\n",
            "11610/11610 [==============================] - 1s 49us/sample - loss: 0.3856 - val_loss: 0.3781\n",
            "Epoch 18/100\n",
            "11610/11610 [==============================] - 1s 45us/sample - loss: 0.3839 - val_loss: 0.3760\n",
            "Epoch 19/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3826 - val_loss: 0.3751\n",
            "Epoch 20/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3810 - val_loss: 0.3746\n",
            "Epoch 21/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3803 - val_loss: 0.3721\n",
            "Epoch 22/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3784 - val_loss: 0.3718\n",
            "Epoch 23/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3777 - val_loss: 0.3705\n",
            "Epoch 24/100\n",
            "11610/11610 [==============================] - 1s 49us/sample - loss: 0.3762 - val_loss: 0.3692\n",
            "Epoch 25/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.3752 - val_loss: 0.3683\n",
            "Epoch 26/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3738 - val_loss: 0.3674\n",
            "Epoch 27/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3724 - val_loss: 0.3684\n",
            "Epoch 28/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3721 - val_loss: 0.3661\n",
            "Epoch 29/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3706 - val_loss: 0.3648\n",
            "Epoch 30/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3699 - val_loss: 0.3643\n",
            "Epoch 31/100\n",
            "11610/11610 [==============================] - 1s 45us/sample - loss: 0.3687 - val_loss: 0.3634\n",
            "Epoch 32/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3678 - val_loss: 0.3628\n",
            "Epoch 33/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3669 - val_loss: 0.3630\n",
            "Epoch 34/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3663 - val_loss: 0.3630\n",
            "Epoch 35/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3652 - val_loss: 0.3620\n",
            "Epoch 36/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3649 - val_loss: 0.3608\n",
            "Epoch 37/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3638 - val_loss: 0.3612\n",
            "Epoch 38/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3634 - val_loss: 0.3599\n",
            "Epoch 39/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3628 - val_loss: 0.3594\n",
            "Epoch 40/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3620 - val_loss: 0.3603\n",
            "Epoch 41/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.3612 - val_loss: 0.3595\n",
            "Epoch 42/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3608 - val_loss: 0.3587\n",
            "Epoch 43/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3603 - val_loss: 0.3586\n",
            "Epoch 44/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3597 - val_loss: 0.3580\n",
            "Epoch 45/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3589 - val_loss: 0.3576\n",
            "Epoch 46/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3583 - val_loss: 0.3573\n",
            "Epoch 47/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3577 - val_loss: 0.3579\n",
            "Epoch 48/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3573 - val_loss: 0.3564\n",
            "Epoch 49/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3566 - val_loss: 0.3559\n",
            "Epoch 50/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3565 - val_loss: 0.3554\n",
            "Epoch 51/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3557 - val_loss: 0.3566\n",
            "Epoch 52/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3556 - val_loss: 0.3551\n",
            "Epoch 53/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3551 - val_loss: 0.3551\n",
            "Epoch 54/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3551 - val_loss: 0.3549\n",
            "Epoch 55/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3541 - val_loss: 0.3554\n",
            "Epoch 56/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3537 - val_loss: 0.3545\n",
            "Epoch 57/100\n",
            "11610/11610 [==============================] - 1s 45us/sample - loss: 0.3536 - val_loss: 0.3539\n",
            "Epoch 58/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3533 - val_loss: 0.3542\n",
            "Epoch 59/100\n",
            "11610/11610 [==============================] - 1s 45us/sample - loss: 0.3529 - val_loss: 0.3549\n",
            "Epoch 60/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3522 - val_loss: 0.3525\n",
            "Epoch 61/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3515 - val_loss: 0.3527\n",
            "Epoch 62/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3517 - val_loss: 0.3532\n",
            "Epoch 63/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3513 - val_loss: 0.3519\n",
            "Epoch 64/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3507 - val_loss: 0.3514\n",
            "Epoch 65/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3502 - val_loss: 0.3517\n",
            "Epoch 66/100\n",
            "11610/11610 [==============================] - 1s 45us/sample - loss: 0.3495 - val_loss: 0.3513\n",
            "Epoch 67/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3490 - val_loss: 0.3525\n",
            "Epoch 68/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3498 - val_loss: 0.3502\n",
            "Epoch 69/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3488 - val_loss: 0.3501\n",
            "Epoch 70/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3480 - val_loss: 0.3497\n",
            "Epoch 71/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3471 - val_loss: 0.3496\n",
            "Epoch 72/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3476 - val_loss: 0.3499\n",
            "Epoch 73/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3466 - val_loss: 0.3492\n",
            "Epoch 74/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3463 - val_loss: 0.3492\n",
            "Epoch 75/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3457 - val_loss: 0.3484\n",
            "Epoch 76/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3453 - val_loss: 0.3477\n",
            "Epoch 77/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3449 - val_loss: 0.3478\n",
            "Epoch 78/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3441 - val_loss: 0.3474\n",
            "Epoch 79/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.3440 - val_loss: 0.3466\n",
            "Epoch 80/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.3434 - val_loss: 0.3460\n",
            "Epoch 81/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3433 - val_loss: 0.3456\n",
            "Epoch 82/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3422 - val_loss: 0.3461\n",
            "Epoch 83/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.3417 - val_loss: 0.3465\n",
            "Epoch 84/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.3417 - val_loss: 0.3455\n",
            "Epoch 85/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.3409 - val_loss: 0.3463\n",
            "Epoch 86/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3413 - val_loss: 0.3442\n",
            "Epoch 87/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.3404 - val_loss: 0.3447\n",
            "Epoch 88/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.3398 - val_loss: 0.3436\n",
            "Epoch 89/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3395 - val_loss: 0.3438\n",
            "Epoch 90/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3395 - val_loss: 0.3434\n",
            "Epoch 91/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3390 - val_loss: 0.3440\n",
            "Epoch 92/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3388 - val_loss: 0.3425\n",
            "Epoch 93/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3376 - val_loss: 0.3433\n",
            "Epoch 94/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3378 - val_loss: 0.3415\n",
            "Epoch 95/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3375 - val_loss: 0.3421\n",
            "Epoch 96/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3368 - val_loss: 0.3410\n",
            "Epoch 97/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3367 - val_loss: 0.3415\n",
            "Epoch 98/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.3362 - val_loss: 0.3410\n",
            "Epoch 99/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3364 - val_loss: 0.3404\n",
            "Epoch 100/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3355 - val_loss: 0.3407\n",
            "Train on 11610 samples, validate on 5160 samples\n",
            "Epoch 1/100\n",
            "11610/11610 [==============================] - 1s 56us/sample - loss: 0.6766 - val_loss: 0.4920\n",
            "Epoch 2/100\n",
            "11610/11610 [==============================] - 1s 45us/sample - loss: 0.4639 - val_loss: 0.4290\n",
            "Epoch 3/100\n",
            "11610/11610 [==============================] - 1s 45us/sample - loss: 0.4313 - val_loss: 0.4786\n",
            "Epoch 4/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.4854 - val_loss: 0.4702\n",
            "Epoch 5/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.5234 - val_loss: 0.4267\n",
            "Epoch 6/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.4500 - val_loss: 0.3994\n",
            "Epoch 7/100\n",
            "11610/11610 [==============================] - 1s 45us/sample - loss: 0.4141 - val_loss: 0.3950\n",
            "Epoch 8/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3941 - val_loss: 0.3883\n",
            "Epoch 9/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3909 - val_loss: 0.4177\n",
            "Epoch 10/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3925 - val_loss: 0.3817\n",
            "Epoch 11/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3900 - val_loss: 0.3751\n",
            "Epoch 12/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3790 - val_loss: 0.3721\n",
            "Epoch 13/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3814 - val_loss: 0.3697\n",
            "Epoch 14/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3752 - val_loss: 0.3825\n",
            "Epoch 15/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3723 - val_loss: 0.3651\n",
            "Epoch 16/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.3696 - val_loss: 0.3681\n",
            "Epoch 17/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3698 - val_loss: 0.3691\n",
            "Epoch 18/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3664 - val_loss: 0.3618\n",
            "Epoch 19/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3664 - val_loss: 0.3638\n",
            "Epoch 20/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3632 - val_loss: 0.3620\n",
            "Epoch 21/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3615 - val_loss: 0.3607\n",
            "Epoch 22/100\n",
            "11610/11610 [==============================] - 1s 45us/sample - loss: 0.3622 - val_loss: 0.3597\n",
            "Epoch 23/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3602 - val_loss: 0.3592\n",
            "Epoch 24/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3591 - val_loss: 0.3582\n",
            "Epoch 25/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.3578 - val_loss: 0.3625\n",
            "Epoch 26/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3558 - val_loss: 0.3567\n",
            "Epoch 27/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3538 - val_loss: 0.3516\n",
            "Epoch 28/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3528 - val_loss: 0.3510\n",
            "Epoch 29/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3512 - val_loss: 0.3540\n",
            "Epoch 30/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3502 - val_loss: 0.3479\n",
            "Epoch 31/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3505 - val_loss: 0.3533\n",
            "Epoch 32/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.3491 - val_loss: 0.3504\n",
            "Epoch 33/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3464 - val_loss: 0.3459\n",
            "Epoch 34/100\n",
            "11610/11610 [==============================] - 1s 49us/sample - loss: 0.3442 - val_loss: 0.3453\n",
            "Epoch 35/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.3460 - val_loss: 0.3457\n",
            "Epoch 36/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3415 - val_loss: 0.3430\n",
            "Epoch 37/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3391 - val_loss: 0.3405\n",
            "Epoch 38/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3411 - val_loss: 0.3386\n",
            "Epoch 39/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3379 - val_loss: 0.3382\n",
            "Epoch 40/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3352 - val_loss: 0.3343\n",
            "Epoch 41/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3440 - val_loss: 0.3358\n",
            "Epoch 42/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3330 - val_loss: 0.3316\n",
            "Epoch 43/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3330 - val_loss: 0.3463\n",
            "Epoch 44/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.3323 - val_loss: 0.3299\n",
            "Epoch 45/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.3383 - val_loss: 0.3354\n",
            "Epoch 46/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3282 - val_loss: 0.3303\n",
            "Epoch 47/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.3265 - val_loss: 0.3361\n",
            "Epoch 48/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3261 - val_loss: 0.3316\n",
            "Epoch 49/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3248 - val_loss: 0.3290\n",
            "Epoch 50/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.3255 - val_loss: 0.3243\n",
            "Epoch 51/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.3254 - val_loss: 0.3253\n",
            "Epoch 52/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3237 - val_loss: 0.3902\n",
            "Epoch 53/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3235 - val_loss: 0.3253\n",
            "Epoch 54/100\n",
            "11610/11610 [==============================] - 1s 45us/sample - loss: 0.3192 - val_loss: 0.3235\n",
            "Epoch 55/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3193 - val_loss: 0.3228\n",
            "Epoch 56/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3196 - val_loss: 0.3241\n",
            "Epoch 57/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3240 - val_loss: 0.3266\n",
            "Epoch 58/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3203 - val_loss: 0.3232\n",
            "Epoch 59/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3204 - val_loss: 0.3194\n",
            "Epoch 60/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3163 - val_loss: 0.3209\n",
            "Epoch 61/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3157 - val_loss: 0.3203\n",
            "Epoch 62/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3170 - val_loss: 0.3191\n",
            "Epoch 63/100\n",
            "11610/11610 [==============================] - 1s 45us/sample - loss: 0.3178 - val_loss: 0.3203\n",
            "Epoch 64/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3148 - val_loss: 0.3173\n",
            "Epoch 65/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3154 - val_loss: 0.3183\n",
            "Epoch 66/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3145 - val_loss: 0.3537\n",
            "Epoch 67/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3144 - val_loss: 0.3225\n",
            "Epoch 68/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3315 - val_loss: 0.3420\n",
            "Epoch 69/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.4675 - val_loss: 0.4087\n",
            "Epoch 70/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3619 - val_loss: 0.4838\n",
            "Epoch 71/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.5605 - val_loss: 0.3887\n",
            "Epoch 72/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3968 - val_loss: 0.4027\n",
            "Epoch 73/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.4488 - val_loss: 0.3959\n",
            "Epoch 74/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3725 - val_loss: 0.3469\n",
            "Train on 11610 samples, validate on 5160 samples\n",
            "Epoch 1/100\n",
            "11610/11610 [==============================] - 1s 56us/sample - loss: 0.5607 - val_loss: 0.5527\n",
            "Epoch 2/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.4215 - val_loss: 0.3910\n",
            "Epoch 3/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.4662 - val_loss: 0.3960\n",
            "Epoch 4/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.4384 - val_loss: 0.4909\n",
            "Epoch 5/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.4522 - val_loss: 0.4030\n",
            "Epoch 6/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.4133 - val_loss: 0.5324\n",
            "Epoch 7/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.4056 - val_loss: 0.3669\n",
            "Epoch 8/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3831 - val_loss: 0.3589\n",
            "Epoch 9/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3728 - val_loss: 0.3538\n",
            "Epoch 10/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3612 - val_loss: 0.3488\n",
            "Epoch 11/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3594 - val_loss: 0.3519\n",
            "Epoch 12/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3534 - val_loss: 0.5506\n",
            "Epoch 13/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3536 - val_loss: 0.3411\n",
            "Epoch 14/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3527 - val_loss: 0.3458\n",
            "Epoch 15/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3443 - val_loss: 0.3370\n",
            "Epoch 16/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3377 - val_loss: 0.3357\n",
            "Epoch 17/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3354 - val_loss: 0.3298\n",
            "Epoch 18/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3334 - val_loss: 0.3372\n",
            "Epoch 19/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3296 - val_loss: 0.3512\n",
            "Epoch 20/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3310 - val_loss: 0.3302\n",
            "Epoch 21/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3270 - val_loss: 0.3317\n",
            "Epoch 22/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3241 - val_loss: 0.3280\n",
            "Epoch 23/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3244 - val_loss: 0.3285\n",
            "Epoch 24/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3249 - val_loss: 0.3373\n",
            "Epoch 25/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3293 - val_loss: 0.3376\n",
            "Epoch 26/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3280 - val_loss: 0.3242\n",
            "Epoch 27/100\n",
            "11610/11610 [==============================] - 1s 45us/sample - loss: 0.3214 - val_loss: 0.3196\n",
            "Epoch 28/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3222 - val_loss: 0.3223\n",
            "Epoch 29/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3193 - val_loss: 0.3156\n",
            "Epoch 30/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3195 - val_loss: 0.3172\n",
            "Epoch 31/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3171 - val_loss: 0.3433\n",
            "Epoch 32/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3154 - val_loss: 0.3241\n",
            "Epoch 33/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3165 - val_loss: 0.3224\n",
            "Epoch 34/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3167 - val_loss: 0.3215\n",
            "Epoch 35/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.3214 - val_loss: 0.3221\n",
            "Epoch 36/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.3183 - val_loss: 0.3252\n",
            "Epoch 37/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.3153 - val_loss: 0.3177\n",
            "Epoch 38/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3145 - val_loss: 0.3137\n",
            "Epoch 39/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3168 - val_loss: 0.3354\n",
            "Epoch 40/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3138 - val_loss: 0.3213\n",
            "Epoch 41/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3155 - val_loss: 0.3332\n",
            "Epoch 42/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3123 - val_loss: 0.3183\n",
            "Epoch 43/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3105 - val_loss: 0.3144\n",
            "Epoch 44/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.3094 - val_loss: 0.3302\n",
            "Epoch 45/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.3086 - val_loss: 0.3111\n",
            "Epoch 46/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.3086 - val_loss: 0.3673\n",
            "Epoch 47/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.3085 - val_loss: 0.3152\n",
            "Epoch 48/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.3077 - val_loss: 0.3087\n",
            "Epoch 49/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.3032 - val_loss: 0.3090\n",
            "Epoch 50/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3041 - val_loss: 0.3241\n",
            "Epoch 51/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3078 - val_loss: 0.3265\n",
            "Epoch 52/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.3082 - val_loss: 0.3413\n",
            "Epoch 53/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.3021 - val_loss: 0.3203\n",
            "Epoch 54/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3037 - val_loss: 0.3324\n",
            "Epoch 55/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3007 - val_loss: 0.3262\n",
            "Epoch 56/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.3042 - val_loss: 0.3129\n",
            "Epoch 57/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3011 - val_loss: 0.3102\n",
            "Epoch 58/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3079 - val_loss: 0.3094\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uuatSosvRPVL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1960
        },
        "outputId": "cc6c6577-2d17-4e59-ffa8-4bed2e8d71cd"
      },
      "cell_type": "code",
      "source": [
        "for learning_rate, history in zip(learning_rates, histories):\n",
        "    print(\"Learning rate:\", learning_rate)\n",
        "    plot_learning_curves(history)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Learning rate: 0.0001\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeEAAAEzCAYAAAAGvLsqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Learning rate: 0.0003\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeEAAAEzCAYAAAAGvLsqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Learning rate: 0.001\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeEAAAEzCAYAAAAGvLsqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Learning rate: 0.003\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeEAAAEzCAYAAAAGvLsqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Learning rate: 0.01\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeEAAAEzCAYAAAAGvLsqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Learning rate: 0.03\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeEAAAEzCAYAAAAGvLsqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "EgrIJrLSRPVM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 6.2)\n",
        "Let's look at a more sophisticated way to tune hyperparameters. Create a `build_model()` function that takes three arguments, `n_hidden`, `n_neurons`, `learning_rate`, and builds, compiles and returns a model with the given number of hidden layers, the given number of neurons and the given learning rate. It is good practice to give a reasonable default value to each argument."
      ]
    },
    {
      "metadata": {
        "id": "QQHrfZOWRPVM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3):\n",
        "    model = keras.models.Sequential()\n",
        "    options = {\"input_shape\": X_train.shape[1:]}\n",
        "    for layer in range(n_hidden + 1):\n",
        "        model.add(keras.layers.Dense(n_neurons, activation=\"relu\", **options))\n",
        "        options = {}\n",
        "    model.add(keras.layers.Dense(1, **options))\n",
        "    optimizer = keras.optimizers.SGD(learning_rate)\n",
        "    model.compile(loss=\"mse\", optimizer=optimizer)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HbAPUh1_RPVM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 6.3)\n",
        "Create a `keras.wrappers.scikit_learn.KerasRegressor` and pass the `build_model` function to the constructor. This gives you a Scikit-Learn compatible predictor. Try training it and using it to make predictions. Note that you can pass the `n_epochs`, `callbacks` and `validation_data` to the `fit()` method."
      ]
    },
    {
      "metadata": {
        "id": "VAWIMZi9RPVM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_MNrfsNlRPVN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 570
        },
        "outputId": "3b139bae-d251-486c-cdc5-9e4ce6cf9991"
      },
      "cell_type": "code",
      "source": [
        "keras_reg.fit(X_train_scaled, y_train, epochs=100,\n",
        "              validation_data=(X_valid_scaled, y_valid),\n",
        "              callbacks=[keras.callbacks.EarlyStopping(patience=10)])"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 11610 samples, validate on 3870 samples\n",
            "Epoch 1/100\n",
            "11610/11610 [==============================] - 1s 62us/sample - loss: 1.1694 - val_loss: 13.0188\n",
            "Epoch 2/100\n",
            "11610/11610 [==============================] - 1s 51us/sample - loss: 0.6143 - val_loss: 43.3472\n",
            "Epoch 3/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.9906 - val_loss: 7.0642\n",
            "Epoch 4/100\n",
            "11610/11610 [==============================] - 1s 49us/sample - loss: 0.5274 - val_loss: 0.4250\n",
            "Epoch 5/100\n",
            "11610/11610 [==============================] - 1s 45us/sample - loss: 0.4449 - val_loss: 0.3991\n",
            "Epoch 6/100\n",
            "11610/11610 [==============================] - 1s 45us/sample - loss: 0.4238 - val_loss: 0.3959\n",
            "Epoch 7/100\n",
            "11610/11610 [==============================] - 1s 45us/sample - loss: 0.4101 - val_loss: 0.4037\n",
            "Epoch 8/100\n",
            "11610/11610 [==============================] - 1s 44us/sample - loss: 0.4014 - val_loss: 0.4117\n",
            "Epoch 9/100\n",
            "11610/11610 [==============================] - 1s 45us/sample - loss: 0.3956 - val_loss: 0.4090\n",
            "Epoch 10/100\n",
            "11610/11610 [==============================] - 1s 45us/sample - loss: 0.3898 - val_loss: 0.4359\n",
            "Epoch 11/100\n",
            "11610/11610 [==============================] - 1s 45us/sample - loss: 0.3872 - val_loss: 0.4333\n",
            "Epoch 12/100\n",
            "11610/11610 [==============================] - 1s 45us/sample - loss: 0.3828 - val_loss: 0.4224\n",
            "Epoch 13/100\n",
            "11610/11610 [==============================] - 1s 45us/sample - loss: 0.3798 - val_loss: 0.4192\n",
            "Epoch 14/100\n",
            "11610/11610 [==============================] - 1s 45us/sample - loss: 0.3780 - val_loss: 0.4283\n",
            "Epoch 15/100\n",
            "11610/11610 [==============================] - 1s 44us/sample - loss: 0.3758 - val_loss: 0.4234\n",
            "Epoch 16/100\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3737 - val_loss: 0.4149\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f0f6a039630>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "metadata": {
        "id": "wwr_qpZJRPVN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "8486768f-5fdf-40d4-b28e-9014b3cd0c10"
      },
      "cell_type": "code",
      "source": [
        "keras_reg.predict(X_test_scaled)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.9345158, 1.962505 , 3.8466477, ..., 1.4445187, 2.2777581,\n",
              "       4.015122 ], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "metadata": {
        "id": "dXZFmIpiRPVO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 6.4)\n",
        "Use a `sklearn.model_selection.RandomizedSearchCV` to search the hyperparameter space of your `KerasRegressor`."
      ]
    },
    {
      "metadata": {
        "id": "7mk0MggkRPVO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from scipy.stats import reciprocal\n",
        "\n",
        "param_distribs = {\n",
        "    \"n_hidden\": [0, 1, 2, 3],\n",
        "    \"n_neurons\": np.arange(1, 100),\n",
        "    \"learning_rate\": reciprocal(3e-4, 3e-2),\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KF9gq0u0RPVP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter=10, cv=3, verbose=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GFsZnZD9RPVQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52541
        },
        "outputId": "17f5d859-61a1-43d7-fad9-d24b82d2ecd7"
      },
      "cell_type": "code",
      "source": [
        "rnd_search_cv.fit(X_train_scaled, y_train, epochs=100,\n",
        "                  validation_data=(X_valid_scaled, y_valid),\n",
        "                  callbacks=[keras.callbacks.EarlyStopping(patience=10)])"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
            "[CV] learning_rate=0.003612504848156595, n_hidden=2, n_neurons=47 ....\n",
            "Train on 7740 samples, validate on 3870 samples\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "7740/7740 [==============================] - 1s 81us/sample - loss: 1.5314 - val_loss: 4.2895\n",
            "Epoch 2/100\n",
            "7740/7740 [==============================] - 1s 66us/sample - loss: 0.6428 - val_loss: 1.1587\n",
            "Epoch 3/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.5246 - val_loss: 0.6037\n",
            "Epoch 4/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.4681 - val_loss: 0.4564\n",
            "Epoch 5/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.4319 - val_loss: 0.4387\n",
            "Epoch 6/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.4092 - val_loss: 0.4253\n",
            "Epoch 7/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3944 - val_loss: 0.3925\n",
            "Epoch 8/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3834 - val_loss: 0.4232\n",
            "Epoch 9/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3750 - val_loss: 0.3808\n",
            "Epoch 10/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3680 - val_loss: 0.4036\n",
            "Epoch 11/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3623 - val_loss: 0.4096\n",
            "Epoch 12/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3576 - val_loss: 0.3612\n",
            "Epoch 13/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3533 - val_loss: 0.3535\n",
            "Epoch 14/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3493 - val_loss: 0.3906\n",
            "Epoch 15/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3469 - val_loss: 0.3652\n",
            "Epoch 16/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3440 - val_loss: 0.3781\n",
            "Epoch 17/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3414 - val_loss: 0.3944\n",
            "Epoch 18/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3386 - val_loss: 0.3601\n",
            "Epoch 19/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3368 - val_loss: 0.3868\n",
            "Epoch 20/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3342 - val_loss: 0.3741\n",
            "Epoch 21/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3327 - val_loss: 0.3543\n",
            "Epoch 22/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3313 - val_loss: 0.3717\n",
            "Epoch 23/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3285 - val_loss: 0.3505\n",
            "Epoch 24/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3260 - val_loss: 0.3797\n",
            "Epoch 25/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3254 - val_loss: 0.3322\n",
            "Epoch 26/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3234 - val_loss: 0.3626\n",
            "Epoch 27/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3221 - val_loss: 0.3499\n",
            "Epoch 28/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3209 - val_loss: 0.3625\n",
            "Epoch 29/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3194 - val_loss: 0.3471\n",
            "Epoch 30/100\n",
            "7740/7740 [==============================] - 0s 58us/sample - loss: 0.3184 - val_loss: 0.3119\n",
            "Epoch 31/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3167 - val_loss: 0.3522\n",
            "Epoch 32/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3149 - val_loss: 0.3570\n",
            "Epoch 33/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3139 - val_loss: 0.3591\n",
            "Epoch 34/100\n",
            "7740/7740 [==============================] - 0s 59us/sample - loss: 0.3149 - val_loss: 0.3375\n",
            "Epoch 35/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3126 - val_loss: 0.3442\n",
            "Epoch 36/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3114 - val_loss: 0.3497\n",
            "Epoch 37/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3096 - val_loss: 0.3393\n",
            "Epoch 38/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3085 - val_loss: 0.3470\n",
            "Epoch 39/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3074 - val_loss: 0.3237\n",
            "Epoch 40/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3067 - val_loss: 0.3183\n",
            "3870/3870 [==============================] - 0s 26us/sample - loss: 0.3345\n",
            "7740/7740 [==============================] - 0s 25us/sample - loss: 0.3023\n",
            "[CV]  learning_rate=0.003612504848156595, n_hidden=2, n_neurons=47, total=  18.0s\n",
            "[CV] learning_rate=0.003612504848156595, n_hidden=2, n_neurons=47 ....\n",
            "Train on 7740 samples, validate on 3870 samples\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   18.2s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "7740/7740 [==============================] - 1s 74us/sample - loss: 1.3780 - val_loss: 4.5414\n",
            "Epoch 2/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.6496 - val_loss: 0.8606\n",
            "Epoch 3/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5526 - val_loss: 0.6098\n",
            "Epoch 4/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.4857 - val_loss: 0.8229\n",
            "Epoch 5/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.4406 - val_loss: 0.8860\n",
            "Epoch 6/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4150 - val_loss: 0.7754\n",
            "Epoch 7/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3993 - val_loss: 0.5314\n",
            "Epoch 8/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3876 - val_loss: 0.4150\n",
            "Epoch 9/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3791 - val_loss: 0.3585\n",
            "Epoch 10/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3705 - val_loss: 0.3626\n",
            "Epoch 11/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3655 - val_loss: 0.3787\n",
            "Epoch 12/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3613 - val_loss: 0.4334\n",
            "Epoch 13/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3564 - val_loss: 0.5600\n",
            "Epoch 14/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3530 - val_loss: 0.5228\n",
            "Epoch 15/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3499 - val_loss: 0.5530\n",
            "Epoch 16/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3476 - val_loss: 0.6861\n",
            "Epoch 17/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3457 - val_loss: 0.5425\n",
            "Epoch 18/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3426 - val_loss: 0.6672\n",
            "Epoch 19/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3397 - val_loss: 0.6038\n",
            "3870/3870 [==============================] - 0s 24us/sample - loss: 0.3688\n",
            "7740/7740 [==============================] - 0s 23us/sample - loss: 0.3432\n",
            "[CV]  learning_rate=0.003612504848156595, n_hidden=2, n_neurons=47, total=   8.7s\n",
            "[CV] learning_rate=0.003612504848156595, n_hidden=2, n_neurons=47 ....\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/100\n",
            "7740/7740 [==============================] - 1s 73us/sample - loss: 1.3804 - val_loss: 14.8884\n",
            "Epoch 2/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.6633 - val_loss: 1.8063\n",
            "Epoch 3/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5364 - val_loss: 0.4987\n",
            "Epoch 4/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.4783 - val_loss: 0.4456\n",
            "Epoch 5/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.4446 - val_loss: 0.4259\n",
            "Epoch 6/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.4228 - val_loss: 0.4493\n",
            "Epoch 7/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4070 - val_loss: 0.4542\n",
            "Epoch 8/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3970 - val_loss: 0.3725\n",
            "Epoch 9/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3875 - val_loss: 0.3971\n",
            "Epoch 10/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3815 - val_loss: 0.4217\n",
            "Epoch 11/100\n",
            "7740/7740 [==============================] - 0s 58us/sample - loss: 0.3772 - val_loss: 0.3505\n",
            "Epoch 12/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3718 - val_loss: 0.3601\n",
            "Epoch 13/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3678 - val_loss: 0.4052\n",
            "Epoch 14/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3651 - val_loss: 0.3457\n",
            "Epoch 15/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3607 - val_loss: 0.4021\n",
            "Epoch 16/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3595 - val_loss: 0.3401\n",
            "Epoch 17/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3568 - val_loss: 0.3485\n",
            "Epoch 18/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3539 - val_loss: 0.3913\n",
            "Epoch 19/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3509 - val_loss: 0.3772\n",
            "Epoch 20/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3496 - val_loss: 0.3886\n",
            "Epoch 21/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3474 - val_loss: 0.3336\n",
            "Epoch 22/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3461 - val_loss: 0.3734\n",
            "Epoch 23/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3435 - val_loss: 0.3447\n",
            "Epoch 24/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3421 - val_loss: 0.3477\n",
            "Epoch 25/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3417 - val_loss: 0.3514\n",
            "Epoch 26/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3389 - val_loss: 0.3789\n",
            "Epoch 27/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3380 - val_loss: 0.3796\n",
            "Epoch 28/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3364 - val_loss: 0.3309\n",
            "Epoch 29/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3344 - val_loss: 0.3616\n",
            "Epoch 30/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3319 - val_loss: 0.3804\n",
            "Epoch 31/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3310 - val_loss: 0.3251\n",
            "Epoch 32/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3302 - val_loss: 0.3552\n",
            "Epoch 33/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3288 - val_loss: 0.3217\n",
            "Epoch 34/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3285 - val_loss: 0.3203\n",
            "Epoch 35/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3261 - val_loss: 0.3238\n",
            "Epoch 36/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3254 - val_loss: 0.3593\n",
            "Epoch 37/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3248 - val_loss: 0.3130\n",
            "Epoch 38/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3243 - val_loss: 0.3785\n",
            "Epoch 39/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3231 - val_loss: 0.3177\n",
            "Epoch 40/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3211 - val_loss: 0.3186\n",
            "Epoch 41/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3205 - val_loss: 0.3117\n",
            "Epoch 42/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3189 - val_loss: 0.3222\n",
            "Epoch 43/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3188 - val_loss: 0.3118\n",
            "Epoch 44/100\n",
            "7740/7740 [==============================] - 0s 60us/sample - loss: 0.3161 - val_loss: 0.3483\n",
            "Epoch 45/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3166 - val_loss: 0.3096\n",
            "Epoch 46/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3139 - val_loss: 0.3264\n",
            "Epoch 47/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3134 - val_loss: 0.3082\n",
            "Epoch 48/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3123 - val_loss: 0.3258\n",
            "Epoch 49/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3123 - val_loss: 0.3182\n",
            "Epoch 50/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3113 - val_loss: 0.3078\n",
            "Epoch 51/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3101 - val_loss: 0.3154\n",
            "Epoch 52/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3091 - val_loss: 0.3160\n",
            "Epoch 53/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3088 - val_loss: 0.3622\n",
            "Epoch 54/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3078 - val_loss: 0.3260\n",
            "Epoch 55/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3066 - val_loss: 0.3271\n",
            "Epoch 56/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3054 - val_loss: 0.3082\n",
            "Epoch 57/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3041 - val_loss: 0.3294\n",
            "Epoch 58/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3044 - val_loss: 0.2998\n",
            "Epoch 59/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3034 - val_loss: 0.3008\n",
            "Epoch 60/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3011 - val_loss: 0.3105\n",
            "Epoch 61/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3008 - val_loss: 0.3162\n",
            "Epoch 62/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3015 - val_loss: 0.3239\n",
            "Epoch 63/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2992 - val_loss: 0.2986\n",
            "Epoch 64/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2987 - val_loss: 0.3257\n",
            "Epoch 65/100\n",
            "7740/7740 [==============================] - 0s 58us/sample - loss: 0.2974 - val_loss: 0.2977\n",
            "Epoch 66/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2964 - val_loss: 0.3077\n",
            "Epoch 67/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.2966 - val_loss: 0.2978\n",
            "Epoch 68/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.2948 - val_loss: 0.3775\n",
            "Epoch 69/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.2970 - val_loss: 0.3014\n",
            "Epoch 70/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2950 - val_loss: 0.3564\n",
            "Epoch 71/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2939 - val_loss: 0.2940\n",
            "Epoch 72/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2929 - val_loss: 0.2950\n",
            "Epoch 73/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2916 - val_loss: 0.3520\n",
            "Epoch 74/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2927 - val_loss: 0.2942\n",
            "Epoch 75/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2902 - val_loss: 0.3435\n",
            "Epoch 76/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2893 - val_loss: 0.2950\n",
            "Epoch 77/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2887 - val_loss: 0.3061\n",
            "Epoch 78/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2875 - val_loss: 0.2945\n",
            "Epoch 79/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.2884 - val_loss: 0.2962\n",
            "Epoch 80/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2867 - val_loss: 0.2914\n",
            "Epoch 81/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.2854 - val_loss: 0.2886\n",
            "Epoch 82/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2864 - val_loss: 0.3006\n",
            "Epoch 83/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2857 - val_loss: 0.3471\n",
            "Epoch 84/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2849 - val_loss: 0.4788\n",
            "Epoch 85/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2858 - val_loss: 0.5120\n",
            "Epoch 86/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2860 - val_loss: 0.5672\n",
            "Epoch 87/100\n",
            "7740/7740 [==============================] - 0s 60us/sample - loss: 0.2891 - val_loss: 0.6536\n",
            "Epoch 88/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.2871 - val_loss: 0.5286\n",
            "Epoch 89/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2842 - val_loss: 0.2930\n",
            "Epoch 90/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2810 - val_loss: 0.2914\n",
            "Epoch 91/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2804 - val_loss: 0.2849\n",
            "Epoch 92/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2809 - val_loss: 0.3443\n",
            "Epoch 93/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2809 - val_loss: 0.3012\n",
            "Epoch 94/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2797 - val_loss: 0.2880\n",
            "Epoch 95/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2784 - val_loss: 0.2978\n",
            "Epoch 96/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2791 - val_loss: 0.3059\n",
            "Epoch 97/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2780 - val_loss: 0.2852\n",
            "Epoch 98/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2765 - val_loss: 0.2981\n",
            "Epoch 99/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2763 - val_loss: 0.2978\n",
            "Epoch 100/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2773 - val_loss: 0.3031\n",
            "3870/3870 [==============================] - 0s 25us/sample - loss: 0.3006\n",
            "7740/7740 [==============================] - 0s 25us/sample - loss: 0.2707\n",
            "[CV]  learning_rate=0.003612504848156595, n_hidden=2, n_neurons=47, total=  44.1s\n",
            "[CV] learning_rate=0.0014348734383753808, n_hidden=3, n_neurons=81 ...\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/100\n",
            "7740/7740 [==============================] - 1s 89us/sample - loss: 1.8964 - val_loss: 6.3849\n",
            "Epoch 2/100\n",
            "7740/7740 [==============================] - 1s 66us/sample - loss: 0.7451 - val_loss: 1.1888\n",
            "Epoch 3/100\n",
            "7740/7740 [==============================] - 1s 65us/sample - loss: 0.5800 - val_loss: 0.5272\n",
            "Epoch 4/100\n",
            "7740/7740 [==============================] - 1s 65us/sample - loss: 0.5197 - val_loss: 0.4651\n",
            "Epoch 5/100\n",
            "7740/7740 [==============================] - 1s 65us/sample - loss: 0.4797 - val_loss: 0.4401\n",
            "Epoch 6/100\n",
            "7740/7740 [==============================] - 1s 65us/sample - loss: 0.4506 - val_loss: 0.4260\n",
            "Epoch 7/100\n",
            "7740/7740 [==============================] - 1s 66us/sample - loss: 0.4288 - val_loss: 0.4092\n",
            "Epoch 8/100\n",
            "7740/7740 [==============================] - 0s 64us/sample - loss: 0.4125 - val_loss: 0.3988\n",
            "Epoch 9/100\n",
            "7740/7740 [==============================] - 1s 67us/sample - loss: 0.4006 - val_loss: 0.4081\n",
            "Epoch 10/100\n",
            "7740/7740 [==============================] - 1s 65us/sample - loss: 0.3918 - val_loss: 0.3864\n",
            "Epoch 11/100\n",
            "7740/7740 [==============================] - 1s 65us/sample - loss: 0.3846 - val_loss: 0.4164\n",
            "Epoch 12/100\n",
            "7740/7740 [==============================] - 1s 66us/sample - loss: 0.3792 - val_loss: 0.4182\n",
            "Epoch 13/100\n",
            "7740/7740 [==============================] - 1s 66us/sample - loss: 0.3746 - val_loss: 0.3874\n",
            "Epoch 14/100\n",
            "7740/7740 [==============================] - 0s 64us/sample - loss: 0.3704 - val_loss: 0.3646\n",
            "Epoch 15/100\n",
            "7740/7740 [==============================] - 0s 64us/sample - loss: 0.3665 - val_loss: 0.3874\n",
            "Epoch 16/100\n",
            "7740/7740 [==============================] - 1s 66us/sample - loss: 0.3633 - val_loss: 0.3509\n",
            "Epoch 17/100\n",
            "7740/7740 [==============================] - 1s 66us/sample - loss: 0.3602 - val_loss: 0.4083\n",
            "Epoch 18/100\n",
            "7740/7740 [==============================] - 1s 66us/sample - loss: 0.3576 - val_loss: 0.3589\n",
            "Epoch 19/100\n",
            "7740/7740 [==============================] - 1s 65us/sample - loss: 0.3549 - val_loss: 0.3594\n",
            "Epoch 20/100\n",
            "7740/7740 [==============================] - 1s 65us/sample - loss: 0.3529 - val_loss: 0.3500\n",
            "Epoch 21/100\n",
            "7740/7740 [==============================] - 1s 65us/sample - loss: 0.3508 - val_loss: 0.3473\n",
            "Epoch 22/100\n",
            "7740/7740 [==============================] - 0s 64us/sample - loss: 0.3487 - val_loss: 0.3396\n",
            "Epoch 23/100\n",
            "7740/7740 [==============================] - 1s 65us/sample - loss: 0.3458 - val_loss: 0.3586\n",
            "Epoch 24/100\n",
            "7740/7740 [==============================] - 1s 66us/sample - loss: 0.3451 - val_loss: 0.3693\n",
            "Epoch 25/100\n",
            "7740/7740 [==============================] - 1s 65us/sample - loss: 0.3434 - val_loss: 0.3750\n",
            "Epoch 26/100\n",
            "7740/7740 [==============================] - 0s 64us/sample - loss: 0.3414 - val_loss: 0.3494\n",
            "Epoch 27/100\n",
            "7740/7740 [==============================] - 0s 64us/sample - loss: 0.3397 - val_loss: 0.3468\n",
            "Epoch 28/100\n",
            "7740/7740 [==============================] - 1s 65us/sample - loss: 0.3390 - val_loss: 0.3411\n",
            "Epoch 29/100\n",
            "7740/7740 [==============================] - 1s 65us/sample - loss: 0.3376 - val_loss: 0.3929\n",
            "Epoch 30/100\n",
            "7740/7740 [==============================] - 1s 65us/sample - loss: 0.3366 - val_loss: 0.3258\n",
            "Epoch 31/100\n",
            "7740/7740 [==============================] - 0s 64us/sample - loss: 0.3339 - val_loss: 0.4201\n",
            "Epoch 32/100\n",
            "7740/7740 [==============================] - 0s 64us/sample - loss: 0.3341 - val_loss: 0.3221\n",
            "Epoch 33/100\n",
            "7740/7740 [==============================] - 0s 64us/sample - loss: 0.3324 - val_loss: 0.3749\n",
            "Epoch 34/100\n",
            "7740/7740 [==============================] - 0s 64us/sample - loss: 0.3311 - val_loss: 0.3547\n",
            "Epoch 35/100\n",
            "7740/7740 [==============================] - 0s 64us/sample - loss: 0.3304 - val_loss: 0.3205\n",
            "Epoch 36/100\n",
            "7740/7740 [==============================] - 0s 64us/sample - loss: 0.3289 - val_loss: 0.3309\n",
            "Epoch 37/100\n",
            "7740/7740 [==============================] - 0s 64us/sample - loss: 0.3270 - val_loss: 0.3562\n",
            "Epoch 38/100\n",
            "7740/7740 [==============================] - 1s 66us/sample - loss: 0.3268 - val_loss: 0.3198\n",
            "Epoch 39/100\n",
            "7740/7740 [==============================] - 1s 66us/sample - loss: 0.3261 - val_loss: 0.3251\n",
            "Epoch 40/100\n",
            "7740/7740 [==============================] - 0s 63us/sample - loss: 0.3248 - val_loss: 0.3233\n",
            "Epoch 41/100\n",
            "7740/7740 [==============================] - 0s 64us/sample - loss: 0.3236 - val_loss: 0.3419\n",
            "Epoch 42/100\n",
            "7740/7740 [==============================] - 1s 65us/sample - loss: 0.3236 - val_loss: 0.3232\n",
            "Epoch 43/100\n",
            "7740/7740 [==============================] - 1s 65us/sample - loss: 0.3223 - val_loss: 0.3597\n",
            "Epoch 44/100\n",
            "7740/7740 [==============================] - 1s 65us/sample - loss: 0.3214 - val_loss: 0.3396\n",
            "Epoch 45/100\n",
            "7740/7740 [==============================] - 1s 65us/sample - loss: 0.3206 - val_loss: 0.3173\n",
            "Epoch 46/100\n",
            "7740/7740 [==============================] - 0s 64us/sample - loss: 0.3197 - val_loss: 0.3190\n",
            "Epoch 47/100\n",
            "7740/7740 [==============================] - 0s 64us/sample - loss: 0.3194 - val_loss: 0.3379\n",
            "Epoch 48/100\n",
            "7740/7740 [==============================] - 0s 64us/sample - loss: 0.3177 - val_loss: 0.3200\n",
            "Epoch 49/100\n",
            "7740/7740 [==============================] - 0s 63us/sample - loss: 0.3167 - val_loss: 0.3959\n",
            "Epoch 50/100\n",
            "7740/7740 [==============================] - 0s 63us/sample - loss: 0.3164 - val_loss: 0.3199\n",
            "Epoch 51/100\n",
            "7740/7740 [==============================] - 1s 65us/sample - loss: 0.3161 - val_loss: 0.3730\n",
            "Epoch 52/100\n",
            "7740/7740 [==============================] - 1s 66us/sample - loss: 0.3149 - val_loss: 0.3237\n",
            "Epoch 53/100\n",
            "7740/7740 [==============================] - 0s 64us/sample - loss: 0.3140 - val_loss: 0.3300\n",
            "Epoch 54/100\n",
            "7740/7740 [==============================] - 0s 64us/sample - loss: 0.3127 - val_loss: 0.3078\n",
            "Epoch 55/100\n",
            "7740/7740 [==============================] - 1s 65us/sample - loss: 0.3124 - val_loss: 0.3473\n",
            "Epoch 56/100\n",
            "7740/7740 [==============================] - 1s 65us/sample - loss: 0.3118 - val_loss: 0.3137\n",
            "Epoch 57/100\n",
            "7740/7740 [==============================] - 0s 64us/sample - loss: 0.3105 - val_loss: 0.3685\n",
            "Epoch 58/100\n",
            "7740/7740 [==============================] - 1s 65us/sample - loss: 0.3102 - val_loss: 0.3089\n",
            "Epoch 59/100\n",
            "7740/7740 [==============================] - 1s 65us/sample - loss: 0.3098 - val_loss: 0.3662\n",
            "Epoch 60/100\n",
            "7740/7740 [==============================] - 1s 65us/sample - loss: 0.3091 - val_loss: 0.3080\n",
            "Epoch 61/100\n",
            "7740/7740 [==============================] - 0s 63us/sample - loss: 0.3086 - val_loss: 0.3342\n",
            "Epoch 62/100\n",
            "7740/7740 [==============================] - 0s 63us/sample - loss: 0.3076 - val_loss: 0.3384\n",
            "Epoch 63/100\n",
            "7740/7740 [==============================] - 0s 62us/sample - loss: 0.3070 - val_loss: 0.3147\n",
            "Epoch 64/100\n",
            "7740/7740 [==============================] - 1s 65us/sample - loss: 0.3063 - val_loss: 0.3669\n",
            "3870/3870 [==============================] - 0s 32us/sample - loss: 0.3419\n",
            "7740/7740 [==============================] - 0s 29us/sample - loss: 0.3038\n",
            "[CV]  learning_rate=0.0014348734383753808, n_hidden=3, n_neurons=81, total=  33.2s\n",
            "[CV] learning_rate=0.0014348734383753808, n_hidden=3, n_neurons=81 ...\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/100\n",
            "7740/7740 [==============================] - 1s 84us/sample - loss: 1.8343 - val_loss: 4.2298\n",
            "Epoch 2/100\n",
            "7740/7740 [==============================] - 1s 65us/sample - loss: 0.6967 - val_loss: 1.9169\n",
            "Epoch 3/100\n",
            "7740/7740 [==============================] - 0s 62us/sample - loss: 0.5931 - val_loss: 1.1640\n",
            "Epoch 4/100\n",
            "7740/7740 [==============================] - 0s 62us/sample - loss: 0.5379 - val_loss: 0.8366\n",
            "Epoch 5/100\n",
            "7740/7740 [==============================] - 0s 63us/sample - loss: 0.4993 - val_loss: 0.6507\n",
            "Epoch 6/100\n",
            "7740/7740 [==============================] - 1s 65us/sample - loss: 0.4711 - val_loss: 0.5545\n",
            "Epoch 7/100\n",
            "7740/7740 [==============================] - 0s 62us/sample - loss: 0.4493 - val_loss: 0.5176\n",
            "Epoch 8/100\n",
            "7740/7740 [==============================] - 0s 63us/sample - loss: 0.4319 - val_loss: 0.4795\n",
            "Epoch 9/100\n",
            "7740/7740 [==============================] - 0s 62us/sample - loss: 0.4186 - val_loss: 0.4738\n",
            "Epoch 10/100\n",
            "7740/7740 [==============================] - 0s 64us/sample - loss: 0.4077 - val_loss: 0.4662\n",
            "Epoch 11/100\n",
            "7740/7740 [==============================] - 1s 65us/sample - loss: 0.3985 - val_loss: 0.4718\n",
            "Epoch 12/100\n",
            "7740/7740 [==============================] - 1s 66us/sample - loss: 0.3911 - val_loss: 0.4732\n",
            "Epoch 13/100\n",
            "7740/7740 [==============================] - 0s 64us/sample - loss: 0.3852 - val_loss: 0.4642\n",
            "Epoch 14/100\n",
            "7740/7740 [==============================] - 0s 64us/sample - loss: 0.3793 - val_loss: 0.4711\n",
            "Epoch 15/100\n",
            "7740/7740 [==============================] - 1s 65us/sample - loss: 0.3754 - val_loss: 0.4760\n",
            "Epoch 16/100\n",
            "7740/7740 [==============================] - 0s 63us/sample - loss: 0.3710 - val_loss: 0.5179\n",
            "Epoch 17/100\n",
            "7740/7740 [==============================] - 0s 62us/sample - loss: 0.3670 - val_loss: 0.5570\n",
            "Epoch 18/100\n",
            "7740/7740 [==============================] - 0s 61us/sample - loss: 0.3643 - val_loss: 0.5438\n",
            "Epoch 19/100\n",
            "7740/7740 [==============================] - 0s 62us/sample - loss: 0.3609 - val_loss: 0.5677\n",
            "Epoch 20/100\n",
            "7740/7740 [==============================] - 0s 62us/sample - loss: 0.3583 - val_loss: 0.5945\n",
            "Epoch 21/100\n",
            "7740/7740 [==============================] - 0s 64us/sample - loss: 0.3556 - val_loss: 0.6518\n",
            "Epoch 22/100\n",
            "7740/7740 [==============================] - 0s 62us/sample - loss: 0.3531 - val_loss: 0.6735\n",
            "Epoch 23/100\n",
            "7740/7740 [==============================] - 0s 63us/sample - loss: 0.3510 - val_loss: 0.7205\n",
            "3870/3870 [==============================] - 0s 31us/sample - loss: 0.3659\n",
            "7740/7740 [==============================] - 0s 29us/sample - loss: 0.3491\n",
            "[CV]  learning_rate=0.0014348734383753808, n_hidden=3, n_neurons=81, total=  12.1s\n",
            "[CV] learning_rate=0.0014348734383753808, n_hidden=3, n_neurons=81 ...\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/100\n",
            "7740/7740 [==============================] - 1s 83us/sample - loss: 1.8240 - val_loss: 1.1055\n",
            "Epoch 2/100\n",
            "7740/7740 [==============================] - 0s 64us/sample - loss: 0.7798 - val_loss: 0.7130\n",
            "Epoch 3/100\n",
            "7740/7740 [==============================] - 0s 64us/sample - loss: 0.6660 - val_loss: 0.6110\n",
            "Epoch 4/100\n",
            "7740/7740 [==============================] - 1s 66us/sample - loss: 0.6117 - val_loss: 0.5613\n",
            "Epoch 5/100\n",
            "7740/7740 [==============================] - 0s 64us/sample - loss: 0.5713 - val_loss: 0.5309\n",
            "Epoch 6/100\n",
            "7740/7740 [==============================] - 0s 64us/sample - loss: 0.5362 - val_loss: 0.4982\n",
            "Epoch 7/100\n",
            "7740/7740 [==============================] - 0s 63us/sample - loss: 0.5063 - val_loss: 0.4786\n",
            "Epoch 8/100\n",
            "7740/7740 [==============================] - 1s 66us/sample - loss: 0.4794 - val_loss: 0.4614\n",
            "Epoch 9/100\n",
            "7740/7740 [==============================] - 1s 66us/sample - loss: 0.4563 - val_loss: 0.4421\n",
            "Epoch 10/100\n",
            "7740/7740 [==============================] - 0s 64us/sample - loss: 0.4374 - val_loss: 0.4272\n",
            "Epoch 11/100\n",
            "7740/7740 [==============================] - 0s 63us/sample - loss: 0.4229 - val_loss: 0.4189\n",
            "Epoch 12/100\n",
            "7740/7740 [==============================] - 0s 62us/sample - loss: 0.4106 - val_loss: 0.4311\n",
            "Epoch 13/100\n",
            "7740/7740 [==============================] - 0s 63us/sample - loss: 0.4015 - val_loss: 0.4063\n",
            "Epoch 14/100\n",
            "7740/7740 [==============================] - 0s 64us/sample - loss: 0.3943 - val_loss: 0.4307\n",
            "Epoch 15/100\n",
            "7740/7740 [==============================] - 0s 64us/sample - loss: 0.3879 - val_loss: 0.3936\n",
            "Epoch 16/100\n",
            "7740/7740 [==============================] - 1s 67us/sample - loss: 0.3834 - val_loss: 0.4039\n",
            "Epoch 17/100\n",
            "7740/7740 [==============================] - 0s 64us/sample - loss: 0.3784 - val_loss: 0.4064\n",
            "Epoch 18/100\n",
            "7740/7740 [==============================] - 0s 62us/sample - loss: 0.3755 - val_loss: 0.3935\n",
            "Epoch 19/100\n",
            "7740/7740 [==============================] - 0s 64us/sample - loss: 0.3718 - val_loss: 0.3867\n",
            "Epoch 20/100\n",
            "7740/7740 [==============================] - 0s 64us/sample - loss: 0.3685 - val_loss: 0.3768\n",
            "Epoch 21/100\n",
            "7740/7740 [==============================] - 0s 62us/sample - loss: 0.3661 - val_loss: 0.3714\n",
            "Epoch 22/100\n",
            "7740/7740 [==============================] - 0s 64us/sample - loss: 0.3628 - val_loss: 0.3655\n",
            "Epoch 23/100\n",
            "7740/7740 [==============================] - 0s 62us/sample - loss: 0.3609 - val_loss: 0.4070\n",
            "Epoch 24/100\n",
            "7740/7740 [==============================] - 0s 63us/sample - loss: 0.3588 - val_loss: 0.3683\n",
            "Epoch 25/100\n",
            "7740/7740 [==============================] - 0s 63us/sample - loss: 0.3558 - val_loss: 0.4023\n",
            "Epoch 26/100\n",
            "7740/7740 [==============================] - 1s 65us/sample - loss: 0.3541 - val_loss: 0.3610\n",
            "Epoch 27/100\n",
            "7740/7740 [==============================] - 1s 65us/sample - loss: 0.3519 - val_loss: 0.3566\n",
            "Epoch 28/100\n",
            "7740/7740 [==============================] - 0s 64us/sample - loss: 0.3498 - val_loss: 0.4192\n",
            "Epoch 29/100\n",
            "7740/7740 [==============================] - 1s 65us/sample - loss: 0.3483 - val_loss: 0.3539\n",
            "Epoch 30/100\n",
            "7740/7740 [==============================] - 1s 65us/sample - loss: 0.3466 - val_loss: 0.3846\n",
            "Epoch 31/100\n",
            "7740/7740 [==============================] - 1s 65us/sample - loss: 0.3441 - val_loss: 0.3566\n",
            "Epoch 32/100\n",
            "7740/7740 [==============================] - 1s 65us/sample - loss: 0.3432 - val_loss: 0.3471\n",
            "Epoch 33/100\n",
            "7740/7740 [==============================] - 1s 65us/sample - loss: 0.3414 - val_loss: 0.3991\n",
            "Epoch 34/100\n",
            "7740/7740 [==============================] - 1s 66us/sample - loss: 0.3401 - val_loss: 0.4083\n",
            "Epoch 35/100\n",
            "7740/7740 [==============================] - 0s 63us/sample - loss: 0.3390 - val_loss: 0.3719\n",
            "Epoch 36/100\n",
            "7740/7740 [==============================] - 0s 63us/sample - loss: 0.3372 - val_loss: 0.3945\n",
            "Epoch 37/100\n",
            "7740/7740 [==============================] - 1s 65us/sample - loss: 0.3358 - val_loss: 0.3618\n",
            "Epoch 38/100\n",
            "7740/7740 [==============================] - 1s 65us/sample - loss: 0.3337 - val_loss: 0.3725\n",
            "Epoch 39/100\n",
            "7740/7740 [==============================] - 1s 65us/sample - loss: 0.3325 - val_loss: 0.4082\n",
            "Epoch 40/100\n",
            "7740/7740 [==============================] - 1s 65us/sample - loss: 0.3323 - val_loss: 0.3338\n",
            "Epoch 41/100\n",
            "7740/7740 [==============================] - 1s 65us/sample - loss: 0.3301 - val_loss: 0.3612\n",
            "Epoch 42/100\n",
            "7740/7740 [==============================] - 0s 64us/sample - loss: 0.3293 - val_loss: 0.3326\n",
            "Epoch 43/100\n",
            "7740/7740 [==============================] - 0s 64us/sample - loss: 0.3279 - val_loss: 0.3921\n",
            "Epoch 44/100\n",
            "7740/7740 [==============================] - 1s 65us/sample - loss: 0.3265 - val_loss: 0.4088\n",
            "Epoch 45/100\n",
            "7740/7740 [==============================] - 1s 65us/sample - loss: 0.3258 - val_loss: 0.3667\n",
            "Epoch 46/100\n",
            "7740/7740 [==============================] - 1s 65us/sample - loss: 0.3246 - val_loss: 0.3810\n",
            "Epoch 47/100\n",
            "7740/7740 [==============================] - 1s 65us/sample - loss: 0.3232 - val_loss: 0.3568\n",
            "Epoch 48/100\n",
            "7740/7740 [==============================] - 1s 65us/sample - loss: 0.3224 - val_loss: 0.3414\n",
            "Epoch 49/100\n",
            "7740/7740 [==============================] - 1s 66us/sample - loss: 0.3207 - val_loss: 0.3508\n",
            "Epoch 50/100\n",
            "7740/7740 [==============================] - 0s 64us/sample - loss: 0.3199 - val_loss: 0.3160\n",
            "Epoch 51/100\n",
            "7740/7740 [==============================] - 0s 64us/sample - loss: 0.3186 - val_loss: 0.3932\n",
            "Epoch 52/100\n",
            "7740/7740 [==============================] - 1s 65us/sample - loss: 0.3183 - val_loss: 0.3415\n",
            "Epoch 53/100\n",
            "7740/7740 [==============================] - 1s 65us/sample - loss: 0.3169 - val_loss: 0.3467\n",
            "Epoch 54/100\n",
            "7740/7740 [==============================] - 1s 65us/sample - loss: 0.3162 - val_loss: 0.3196\n",
            "Epoch 55/100\n",
            "7740/7740 [==============================] - 0s 64us/sample - loss: 0.3151 - val_loss: 0.3546\n",
            "Epoch 56/100\n",
            "7740/7740 [==============================] - 0s 64us/sample - loss: 0.3144 - val_loss: 0.3132\n",
            "Epoch 57/100\n",
            "7740/7740 [==============================] - 0s 64us/sample - loss: 0.3126 - val_loss: 0.3868\n",
            "Epoch 58/100\n",
            "7740/7740 [==============================] - 0s 64us/sample - loss: 0.3121 - val_loss: 0.3101\n",
            "Epoch 59/100\n",
            "7740/7740 [==============================] - 0s 64us/sample - loss: 0.3117 - val_loss: 0.3621\n",
            "Epoch 60/100\n",
            "7740/7740 [==============================] - 1s 66us/sample - loss: 0.3107 - val_loss: 0.3353\n",
            "Epoch 61/100\n",
            "7740/7740 [==============================] - 1s 65us/sample - loss: 0.3090 - val_loss: 0.3117\n",
            "Epoch 62/100\n",
            "7740/7740 [==============================] - 0s 64us/sample - loss: 0.3086 - val_loss: 0.3229\n",
            "Epoch 63/100\n",
            "7740/7740 [==============================] - 0s 64us/sample - loss: 0.3077 - val_loss: 0.3005\n",
            "Epoch 64/100\n",
            "7740/7740 [==============================] - 0s 64us/sample - loss: 0.3068 - val_loss: 0.3073\n",
            "Epoch 65/100\n",
            "7740/7740 [==============================] - 0s 63us/sample - loss: 0.3064 - val_loss: 0.3053\n",
            "Epoch 66/100\n",
            "7740/7740 [==============================] - 0s 64us/sample - loss: 0.3048 - val_loss: 0.3686\n",
            "Epoch 67/100\n",
            "7740/7740 [==============================] - 0s 64us/sample - loss: 0.3051 - val_loss: 0.2982\n",
            "Epoch 68/100\n",
            "7740/7740 [==============================] - 1s 67us/sample - loss: 0.3039 - val_loss: 0.3074\n",
            "Epoch 69/100\n",
            "7740/7740 [==============================] - 1s 65us/sample - loss: 0.3035 - val_loss: 0.2971\n",
            "Epoch 70/100\n",
            "7740/7740 [==============================] - 1s 66us/sample - loss: 0.3020 - val_loss: 0.3413\n",
            "Epoch 71/100\n",
            "7740/7740 [==============================] - 0s 64us/sample - loss: 0.3023 - val_loss: 0.2950\n",
            "Epoch 72/100\n",
            "7740/7740 [==============================] - 1s 67us/sample - loss: 0.3010 - val_loss: 0.3171\n",
            "Epoch 73/100\n",
            "7740/7740 [==============================] - 1s 66us/sample - loss: 0.3000 - val_loss: 0.2934\n",
            "Epoch 74/100\n",
            "7740/7740 [==============================] - 1s 66us/sample - loss: 0.2992 - val_loss: 0.3069\n",
            "Epoch 75/100\n",
            "7740/7740 [==============================] - 1s 65us/sample - loss: 0.2989 - val_loss: 0.2971\n",
            "Epoch 76/100\n",
            "7740/7740 [==============================] - 1s 65us/sample - loss: 0.2979 - val_loss: 0.2970\n",
            "Epoch 77/100\n",
            "7740/7740 [==============================] - 1s 67us/sample - loss: 0.2972 - val_loss: 0.3673\n",
            "Epoch 78/100\n",
            "7740/7740 [==============================] - 0s 64us/sample - loss: 0.2967 - val_loss: 0.3088\n",
            "Epoch 79/100\n",
            "7740/7740 [==============================] - 0s 64us/sample - loss: 0.2971 - val_loss: 0.3362\n",
            "Epoch 80/100\n",
            "7740/7740 [==============================] - 0s 63us/sample - loss: 0.2948 - val_loss: 0.3254\n",
            "Epoch 81/100\n",
            "7740/7740 [==============================] - 0s 63us/sample - loss: 0.2936 - val_loss: 0.3314\n",
            "Epoch 82/100\n",
            "7740/7740 [==============================] - 0s 64us/sample - loss: 0.2944 - val_loss: 0.3436\n",
            "Epoch 83/100\n",
            "7740/7740 [==============================] - 0s 64us/sample - loss: 0.2937 - val_loss: 0.3030\n",
            "3870/3870 [==============================] - 0s 30us/sample - loss: 0.3120\n",
            "7740/7740 [==============================] - 0s 30us/sample - loss: 0.2924\n",
            "[CV]  learning_rate=0.0014348734383753808, n_hidden=3, n_neurons=81, total=  42.2s\n",
            "[CV] learning_rate=0.002367221176917048, n_hidden=2, n_neurons=21 ....\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/100\n",
            "7740/7740 [==============================] - 1s 73us/sample - loss: 1.5244 - val_loss: 3.4168\n",
            "Epoch 2/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.7166 - val_loss: 0.6878\n",
            "Epoch 3/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5972 - val_loss: 0.5320\n",
            "Epoch 4/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5406 - val_loss: 0.4797\n",
            "Epoch 5/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5002 - val_loss: 0.4482\n",
            "Epoch 6/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.4694 - val_loss: 0.4271\n",
            "Epoch 7/100\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.4482 - val_loss: 0.4157\n",
            "Epoch 8/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.4324 - val_loss: 0.4139\n",
            "Epoch 9/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.4195 - val_loss: 0.3995\n",
            "Epoch 10/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.4101 - val_loss: 0.4026\n",
            "Epoch 11/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.4023 - val_loss: 0.3882\n",
            "Epoch 12/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3951 - val_loss: 0.3889\n",
            "Epoch 13/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3899 - val_loss: 0.3901\n",
            "Epoch 14/100\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3836 - val_loss: 0.3725\n",
            "Epoch 15/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3797 - val_loss: 0.3695\n",
            "Epoch 16/100\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3765 - val_loss: 0.3793\n",
            "Epoch 17/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3746 - val_loss: 0.3713\n",
            "Epoch 18/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3728 - val_loss: 0.3652\n",
            "Epoch 19/100\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3679 - val_loss: 0.3636\n",
            "Epoch 20/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3666 - val_loss: 0.3812\n",
            "Epoch 21/100\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3653 - val_loss: 0.3850\n",
            "Epoch 22/100\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3609 - val_loss: 0.3606\n",
            "Epoch 23/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3611 - val_loss: 0.3838\n",
            "Epoch 24/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3596 - val_loss: 0.3590\n",
            "Epoch 25/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3583 - val_loss: 0.3612\n",
            "Epoch 26/100\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3547 - val_loss: 0.3575\n",
            "Epoch 27/100\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3535 - val_loss: 0.3706\n",
            "Epoch 28/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3527 - val_loss: 0.3753\n",
            "Epoch 29/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3524 - val_loss: 0.3661\n",
            "Epoch 30/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3505 - val_loss: 0.3908\n",
            "Epoch 31/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3508 - val_loss: 0.3701\n",
            "Epoch 32/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3482 - val_loss: 0.3598\n",
            "Epoch 33/100\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3455 - val_loss: 0.3624\n",
            "Epoch 34/100\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3461 - val_loss: 0.3503\n",
            "Epoch 35/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3450 - val_loss: 0.3580\n",
            "Epoch 36/100\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3425 - val_loss: 0.3493\n",
            "Epoch 37/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3417 - val_loss: 0.3546\n",
            "Epoch 38/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3414 - val_loss: 0.3623\n",
            "Epoch 39/100\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3408 - val_loss: 0.3532\n",
            "Epoch 40/100\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3409 - val_loss: 0.3571\n",
            "Epoch 41/100\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3390 - val_loss: 0.3544\n",
            "Epoch 42/100\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3380 - val_loss: 0.3582\n",
            "Epoch 43/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3358 - val_loss: 0.3578\n",
            "Epoch 44/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3369 - val_loss: 0.3510\n",
            "Epoch 45/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3349 - val_loss: 0.3540\n",
            "Epoch 46/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3340 - val_loss: 0.3439\n",
            "Epoch 47/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3340 - val_loss: 0.3455\n",
            "Epoch 48/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3333 - val_loss: 0.3460\n",
            "Epoch 49/100\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3317 - val_loss: 0.3404\n",
            "Epoch 50/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3308 - val_loss: 0.3442\n",
            "Epoch 51/100\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3302 - val_loss: 0.3454\n",
            "Epoch 52/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3287 - val_loss: 0.3428\n",
            "Epoch 53/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3302 - val_loss: 0.3389\n",
            "Epoch 54/100\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3292 - val_loss: 0.3368\n",
            "Epoch 55/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3276 - val_loss: 0.3430\n",
            "Epoch 56/100\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3273 - val_loss: 0.3478\n",
            "Epoch 57/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3249 - val_loss: 0.3525\n",
            "Epoch 58/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3256 - val_loss: 0.3488\n",
            "Epoch 59/100\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3244 - val_loss: 0.3502\n",
            "Epoch 60/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3245 - val_loss: 0.3443\n",
            "Epoch 61/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3232 - val_loss: 0.3469\n",
            "Epoch 62/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3226 - val_loss: 0.3547\n",
            "Epoch 63/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3226 - val_loss: 0.3448\n",
            "Epoch 64/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3218 - val_loss: 0.3363\n",
            "Epoch 65/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3213 - val_loss: 0.3343\n",
            "Epoch 66/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3208 - val_loss: 0.3312\n",
            "Epoch 67/100\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3201 - val_loss: 0.3300\n",
            "Epoch 68/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3198 - val_loss: 0.3316\n",
            "Epoch 69/100\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3195 - val_loss: 0.3390\n",
            "Epoch 70/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3185 - val_loss: 0.3420\n",
            "Epoch 71/100\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3179 - val_loss: 0.3351\n",
            "Epoch 72/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3180 - val_loss: 0.3452\n",
            "Epoch 73/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3171 - val_loss: 0.3460\n",
            "Epoch 74/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3168 - val_loss: 0.3477\n",
            "Epoch 75/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3159 - val_loss: 0.3494\n",
            "Epoch 76/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3160 - val_loss: 0.3347\n",
            "Epoch 77/100\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3155 - val_loss: 0.3325\n",
            "3870/3870 [==============================] - 0s 24us/sample - loss: 0.3467\n",
            "7740/7740 [==============================] - 0s 23us/sample - loss: 0.3161\n",
            "[CV]  learning_rate=0.002367221176917048, n_hidden=2, n_neurons=21, total=  32.4s\n",
            "[CV] learning_rate=0.002367221176917048, n_hidden=2, n_neurons=21 ....\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/100\n",
            "7740/7740 [==============================] - 1s 71us/sample - loss: 1.9085 - val_loss: 8.8719\n",
            "Epoch 2/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.8037 - val_loss: 2.8621\n",
            "Epoch 3/100\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.6466 - val_loss: 1.6228\n",
            "Epoch 4/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5728 - val_loss: 1.3209\n",
            "Epoch 5/100\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.5204 - val_loss: 1.0834\n",
            "Epoch 6/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.4842 - val_loss: 1.0393\n",
            "Epoch 7/100\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.4572 - val_loss: 1.1486\n",
            "Epoch 8/100\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.4380 - val_loss: 1.0166\n",
            "Epoch 9/100\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.4231 - val_loss: 1.1458\n",
            "Epoch 10/100\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.4119 - val_loss: 1.0394\n",
            "Epoch 11/100\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.4043 - val_loss: 1.1916\n",
            "Epoch 12/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3972 - val_loss: 1.2391\n",
            "Epoch 13/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3929 - val_loss: 1.1703\n",
            "Epoch 14/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3878 - val_loss: 1.2076\n",
            "Epoch 15/100\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3842 - val_loss: 1.3306\n",
            "Epoch 16/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3808 - val_loss: 1.4082\n",
            "Epoch 17/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3770 - val_loss: 1.3266\n",
            "Epoch 18/100\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3752 - val_loss: 1.4458\n",
            "3870/3870 [==============================] - 0s 24us/sample - loss: 0.3984\n",
            "7740/7740 [==============================] - 0s 23us/sample - loss: 0.3703\n",
            "[CV]  learning_rate=0.002367221176917048, n_hidden=2, n_neurons=21, total=   8.1s\n",
            "[CV] learning_rate=0.002367221176917048, n_hidden=2, n_neurons=21 ....\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/100\n",
            "7740/7740 [==============================] - 1s 76us/sample - loss: 1.4494 - val_loss: 0.6710\n",
            "Epoch 2/100\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.6208 - val_loss: 0.5709\n",
            "Epoch 3/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.5519 - val_loss: 0.5224\n",
            "Epoch 4/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5103 - val_loss: 0.4918\n",
            "Epoch 5/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4816 - val_loss: 0.4650\n",
            "Epoch 6/100\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.4604 - val_loss: 0.4496\n",
            "Epoch 7/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.4438 - val_loss: 0.4276\n",
            "Epoch 8/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.4318 - val_loss: 0.4350\n",
            "Epoch 9/100\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 0.4213 - val_loss: 0.4285\n",
            "Epoch 10/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.4132 - val_loss: 0.4122\n",
            "Epoch 11/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.4053 - val_loss: 0.4166\n",
            "Epoch 12/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.4003 - val_loss: 0.4401\n",
            "Epoch 13/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3960 - val_loss: 0.3820\n",
            "Epoch 14/100\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3919 - val_loss: 0.4405\n",
            "Epoch 15/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3892 - val_loss: 0.4112\n",
            "Epoch 16/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3859 - val_loss: 0.3811\n",
            "Epoch 17/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3835 - val_loss: 0.3769\n",
            "Epoch 18/100\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3816 - val_loss: 0.3745\n",
            "Epoch 19/100\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3785 - val_loss: 0.4009\n",
            "Epoch 20/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3772 - val_loss: 0.3709\n",
            "Epoch 21/100\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3751 - val_loss: 0.3989\n",
            "Epoch 22/100\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3736 - val_loss: 0.3871\n",
            "Epoch 23/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3717 - val_loss: 0.3692\n",
            "Epoch 24/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3702 - val_loss: 0.4142\n",
            "Epoch 25/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3684 - val_loss: 0.3603\n",
            "Epoch 26/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3670 - val_loss: 0.3886\n",
            "Epoch 27/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3662 - val_loss: 0.3724\n",
            "Epoch 28/100\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3646 - val_loss: 0.3832\n",
            "Epoch 29/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3635 - val_loss: 0.3480\n",
            "Epoch 30/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3625 - val_loss: 0.3667\n",
            "Epoch 31/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3610 - val_loss: 0.4068\n",
            "Epoch 32/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3601 - val_loss: 0.3687\n",
            "Epoch 33/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3588 - val_loss: 0.3795\n",
            "Epoch 34/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3579 - val_loss: 0.3426\n",
            "Epoch 35/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3575 - val_loss: 0.3863\n",
            "Epoch 36/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3566 - val_loss: 0.3450\n",
            "Epoch 37/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3554 - val_loss: 0.3616\n",
            "Epoch 38/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3546 - val_loss: 0.3593\n",
            "Epoch 39/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3542 - val_loss: 0.3758\n",
            "Epoch 40/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3533 - val_loss: 0.3388\n",
            "Epoch 41/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3523 - val_loss: 0.3661\n",
            "Epoch 42/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3521 - val_loss: 0.3523\n",
            "Epoch 43/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3507 - val_loss: 0.3854\n",
            "Epoch 44/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3506 - val_loss: 0.3385\n",
            "Epoch 45/100\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3495 - val_loss: 0.3723\n",
            "Epoch 46/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3491 - val_loss: 0.3844\n",
            "Epoch 47/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3486 - val_loss: 0.3509\n",
            "Epoch 48/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3479 - val_loss: 0.3382\n",
            "Epoch 49/100\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3470 - val_loss: 0.3402\n",
            "Epoch 50/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3466 - val_loss: 0.3583\n",
            "Epoch 51/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3465 - val_loss: 0.3939\n",
            "Epoch 52/100\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3452 - val_loss: 0.3367\n",
            "Epoch 53/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3454 - val_loss: 0.3345\n",
            "Epoch 54/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3442 - val_loss: 0.3652\n",
            "Epoch 55/100\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3440 - val_loss: 0.3657\n",
            "Epoch 56/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3439 - val_loss: 0.3318\n",
            "Epoch 57/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3437 - val_loss: 0.3390\n",
            "Epoch 58/100\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3421 - val_loss: 0.3309\n",
            "Epoch 59/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3420 - val_loss: 0.3941\n",
            "Epoch 60/100\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3420 - val_loss: 0.3291\n",
            "Epoch 61/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3409 - val_loss: 0.3665\n",
            "Epoch 62/100\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3406 - val_loss: 0.3487\n",
            "Epoch 63/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3398 - val_loss: 0.4755\n",
            "Epoch 64/100\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3414 - val_loss: 0.3842\n",
            "Epoch 65/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3396 - val_loss: 0.4558\n",
            "Epoch 66/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3388 - val_loss: 0.3886\n",
            "Epoch 67/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3389 - val_loss: 0.3996\n",
            "Epoch 68/100\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3373 - val_loss: 0.4067\n",
            "Epoch 69/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3381 - val_loss: 0.4244\n",
            "Epoch 70/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3377 - val_loss: 0.3322\n",
            "3870/3870 [==============================] - 0s 25us/sample - loss: 0.3421\n",
            "7740/7740 [==============================] - 0s 24us/sample - loss: 0.3358\n",
            "[CV]  learning_rate=0.002367221176917048, n_hidden=2, n_neurons=21, total=  29.9s\n",
            "[CV] learning_rate=0.0012710173398986133, n_hidden=1, n_neurons=6 ....\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/100\n",
            "7740/7740 [==============================] - 1s 68us/sample - loss: 4.1693 - val_loss: 2.6583\n",
            "Epoch 2/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 1.8855 - val_loss: 1.6465\n",
            "Epoch 3/100\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 1.4527 - val_loss: 1.3266\n",
            "Epoch 4/100\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 1.2096 - val_loss: 1.0937\n",
            "Epoch 5/100\n",
            "7740/7740 [==============================] - 0s 46us/sample - loss: 1.0325 - val_loss: 0.9437\n",
            "Epoch 6/100\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.9154 - val_loss: 0.8471\n",
            "Epoch 7/100\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.8428 - val_loss: 0.7853\n",
            "Epoch 8/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.7953 - val_loss: 0.7410\n",
            "Epoch 9/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.7611 - val_loss: 0.7076\n",
            "Epoch 10/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.7347 - val_loss: 0.6798\n",
            "Epoch 11/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.7121 - val_loss: 0.6532\n",
            "Epoch 12/100\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.6918 - val_loss: 0.6318\n",
            "Epoch 13/100\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.6730 - val_loss: 0.6127\n",
            "Epoch 14/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.6551 - val_loss: 0.5962\n",
            "Epoch 15/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.6383 - val_loss: 0.5814\n",
            "Epoch 16/100\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.6226 - val_loss: 0.5700\n",
            "Epoch 17/100\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.6079 - val_loss: 0.5767\n",
            "Epoch 18/100\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5948 - val_loss: 0.5871\n",
            "Epoch 19/100\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5836 - val_loss: 0.6012\n",
            "Epoch 20/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5730 - val_loss: 0.6124\n",
            "Epoch 21/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5642 - val_loss: 0.5994\n",
            "Epoch 22/100\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5553 - val_loss: 0.6017\n",
            "Epoch 23/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5470 - val_loss: 0.5906\n",
            "Epoch 24/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5388 - val_loss: 0.5718\n",
            "Epoch 25/100\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5307 - val_loss: 0.5631\n",
            "Epoch 26/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.5226 - val_loss: 0.5326\n",
            "Epoch 27/100\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5143 - val_loss: 0.5103\n",
            "Epoch 28/100\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5064 - val_loss: 0.4986\n",
            "Epoch 29/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.4984 - val_loss: 0.4889\n",
            "Epoch 30/100\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.4910 - val_loss: 0.4728\n",
            "Epoch 31/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.4838 - val_loss: 0.4567\n",
            "Epoch 32/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.4767 - val_loss: 0.4467\n",
            "Epoch 33/100\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.4698 - val_loss: 0.4417\n",
            "Epoch 34/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.4636 - val_loss: 0.4351\n",
            "Epoch 35/100\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.4577 - val_loss: 0.4250\n",
            "Epoch 36/100\n",
            "7740/7740 [==============================] - 0s 46us/sample - loss: 0.4522 - val_loss: 0.4192\n",
            "Epoch 37/100\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.4475 - val_loss: 0.4147\n",
            "Epoch 38/100\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.4431 - val_loss: 0.4102\n",
            "Epoch 39/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.4391 - val_loss: 0.4063\n",
            "Epoch 40/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.4358 - val_loss: 0.4027\n",
            "Epoch 41/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.4326 - val_loss: 0.4019\n",
            "Epoch 42/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.4300 - val_loss: 0.3982\n",
            "Epoch 43/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.4275 - val_loss: 0.3964\n",
            "Epoch 44/100\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.4251 - val_loss: 0.3946\n",
            "Epoch 45/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.4231 - val_loss: 0.3930\n",
            "Epoch 46/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.4214 - val_loss: 0.3915\n",
            "Epoch 47/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.4190 - val_loss: 0.3903\n",
            "Epoch 48/100\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.4170 - val_loss: 0.3894\n",
            "Epoch 49/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.4151 - val_loss: 0.3877\n",
            "Epoch 50/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.4140 - val_loss: 0.3865\n",
            "Epoch 51/100\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.4124 - val_loss: 0.3852\n",
            "Epoch 52/100\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.4113 - val_loss: 0.3849\n",
            "Epoch 53/100\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.4096 - val_loss: 0.3834\n",
            "Epoch 54/100\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.4089 - val_loss: 0.3827\n",
            "Epoch 55/100\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.4077 - val_loss: 0.3815\n",
            "Epoch 56/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.4065 - val_loss: 0.3811\n",
            "Epoch 57/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.4054 - val_loss: 0.3803\n",
            "Epoch 58/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.4048 - val_loss: 0.3795\n",
            "Epoch 59/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.4030 - val_loss: 0.3788\n",
            "Epoch 60/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.4030 - val_loss: 0.3778\n",
            "Epoch 61/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.4022 - val_loss: 0.3773\n",
            "Epoch 62/100\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.4010 - val_loss: 0.3774\n",
            "Epoch 63/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.4006 - val_loss: 0.3762\n",
            "Epoch 64/100\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.3994 - val_loss: 0.3753\n",
            "Epoch 65/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3991 - val_loss: 0.3759\n",
            "Epoch 66/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3983 - val_loss: 0.3749\n",
            "Epoch 67/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3968 - val_loss: 0.3738\n",
            "Epoch 68/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3972 - val_loss: 0.3737\n",
            "Epoch 69/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3959 - val_loss: 0.3726\n",
            "Epoch 70/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3958 - val_loss: 0.3725\n",
            "Epoch 71/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3945 - val_loss: 0.3718\n",
            "Epoch 72/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3949 - val_loss: 0.3721\n",
            "Epoch 73/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3939 - val_loss: 0.3709\n",
            "Epoch 74/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3932 - val_loss: 0.3708\n",
            "Epoch 75/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3933 - val_loss: 0.3704\n",
            "Epoch 76/100\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.3925 - val_loss: 0.3700\n",
            "Epoch 77/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3922 - val_loss: 0.3714\n",
            "Epoch 78/100\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.3917 - val_loss: 0.3705\n",
            "Epoch 79/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3906 - val_loss: 0.3683\n",
            "Epoch 80/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3907 - val_loss: 0.3704\n",
            "Epoch 81/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3907 - val_loss: 0.3689\n",
            "Epoch 82/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3894 - val_loss: 0.3678\n",
            "Epoch 83/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3898 - val_loss: 0.3672\n",
            "Epoch 84/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3895 - val_loss: 0.3684\n",
            "Epoch 85/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3886 - val_loss: 0.3668\n",
            "Epoch 86/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3884 - val_loss: 0.3666\n",
            "Epoch 87/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3883 - val_loss: 0.3667\n",
            "Epoch 88/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3876 - val_loss: 0.3661\n",
            "Epoch 89/100\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.3867 - val_loss: 0.3653\n",
            "Epoch 90/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3872 - val_loss: 0.3649\n",
            "Epoch 91/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3863 - val_loss: 0.3666\n",
            "Epoch 92/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3865 - val_loss: 0.3650\n",
            "Epoch 93/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3864 - val_loss: 0.3655\n",
            "Epoch 94/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3848 - val_loss: 0.3640\n",
            "Epoch 95/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3858 - val_loss: 0.3654\n",
            "Epoch 96/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3843 - val_loss: 0.3634\n",
            "Epoch 97/100\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.3853 - val_loss: 0.3638\n",
            "Epoch 98/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3836 - val_loss: 0.3627\n",
            "Epoch 99/100\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.3841 - val_loss: 0.3635\n",
            "Epoch 100/100\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.3839 - val_loss: 0.3626\n",
            "3870/3870 [==============================] - 0s 23us/sample - loss: 0.3974\n",
            "7740/7740 [==============================] - 0s 21us/sample - loss: 0.3826\n",
            "[CV]  learning_rate=0.0012710173398986133, n_hidden=1, n_neurons=6, total=  37.8s\n",
            "[CV] learning_rate=0.0012710173398986133, n_hidden=1, n_neurons=6 ....\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/100\n",
            "7740/7740 [==============================] - 1s 66us/sample - loss: 3.0218 - val_loss: 27.3764\n",
            "Epoch 2/100\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 1.3307 - val_loss: 21.1899\n",
            "Epoch 3/100\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.8891 - val_loss: 14.7295\n",
            "Epoch 4/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.7499 - val_loss: 11.1584\n",
            "Epoch 5/100\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.6874 - val_loss: 9.0509\n",
            "Epoch 6/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.6513 - val_loss: 7.6155\n",
            "Epoch 7/100\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.6255 - val_loss: 6.5412\n",
            "Epoch 8/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.6046 - val_loss: 5.6986\n",
            "Epoch 9/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5859 - val_loss: 4.9393\n",
            "Epoch 10/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5692 - val_loss: 4.3585\n",
            "Epoch 11/100\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5536 - val_loss: 3.8760\n",
            "Epoch 12/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5394 - val_loss: 3.4403\n",
            "Epoch 13/100\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5261 - val_loss: 3.0756\n",
            "Epoch 14/100\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5139 - val_loss: 2.7626\n",
            "Epoch 15/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5025 - val_loss: 2.4986\n",
            "Epoch 16/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.4918 - val_loss: 2.2689\n",
            "Epoch 17/100\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.4819 - val_loss: 2.0541\n",
            "Epoch 18/100\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.4729 - val_loss: 1.9117\n",
            "Epoch 19/100\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.4648 - val_loss: 1.7145\n",
            "Epoch 20/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.4575 - val_loss: 1.6196\n",
            "Epoch 21/100\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.4509 - val_loss: 1.4927\n",
            "Epoch 22/100\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.4449 - val_loss: 1.4115\n",
            "Epoch 23/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.4400 - val_loss: 1.3412\n",
            "Epoch 24/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.4355 - val_loss: 1.3039\n",
            "Epoch 25/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.4315 - val_loss: 1.2690\n",
            "Epoch 26/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.4279 - val_loss: 1.2229\n",
            "Epoch 27/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.4249 - val_loss: 1.2149\n",
            "Epoch 28/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.4221 - val_loss: 1.1952\n",
            "Epoch 29/100\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.4196 - val_loss: 1.2193\n",
            "Epoch 30/100\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.4173 - val_loss: 1.2126\n",
            "Epoch 31/100\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.4155 - val_loss: 1.2092\n",
            "Epoch 32/100\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.4137 - val_loss: 1.2400\n",
            "Epoch 33/100\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.4120 - val_loss: 1.2448\n",
            "Epoch 34/100\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.4104 - val_loss: 1.2713\n",
            "Epoch 35/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.4092 - val_loss: 1.3032\n",
            "Epoch 36/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.4079 - val_loss: 1.3179\n",
            "Epoch 37/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.4067 - val_loss: 1.3121\n",
            "Epoch 38/100\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.4056 - val_loss: 1.3254\n",
            "3870/3870 [==============================] - 0s 23us/sample - loss: 0.4279\n",
            "7740/7740 [==============================] - 0s 21us/sample - loss: 0.4043\n",
            "[CV]  learning_rate=0.0012710173398986133, n_hidden=1, n_neurons=6, total=  14.7s\n",
            "[CV] learning_rate=0.0012710173398986133, n_hidden=1, n_neurons=6 ....\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/100\n",
            "7740/7740 [==============================] - 1s 67us/sample - loss: 2.2865 - val_loss: 3.6270\n",
            "Epoch 2/100\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 1.2341 - val_loss: 2.1481\n",
            "Epoch 3/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.9233 - val_loss: 1.3813\n",
            "Epoch 4/100\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.7560 - val_loss: 0.9845\n",
            "Epoch 5/100\n",
            "7740/7740 [==============================] - 0s 46us/sample - loss: 0.6529 - val_loss: 0.7841\n",
            "Epoch 6/100\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5906 - val_loss: 0.6748\n",
            "Epoch 7/100\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5517 - val_loss: 0.6080\n",
            "Epoch 8/100\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5234 - val_loss: 0.5643\n",
            "Epoch 9/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5020 - val_loss: 0.5349\n",
            "Epoch 10/100\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.4848 - val_loss: 0.5122\n",
            "Epoch 11/100\n",
            "7740/7740 [==============================] - 0s 46us/sample - loss: 0.4710 - val_loss: 0.4960\n",
            "Epoch 12/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.4593 - val_loss: 0.4828\n",
            "Epoch 13/100\n",
            "7740/7740 [==============================] - 0s 46us/sample - loss: 0.4501 - val_loss: 0.4715\n",
            "Epoch 14/100\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.4426 - val_loss: 0.4641\n",
            "Epoch 15/100\n",
            "7740/7740 [==============================] - 0s 46us/sample - loss: 0.4366 - val_loss: 0.4576\n",
            "Epoch 16/100\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.4322 - val_loss: 0.4540\n",
            "Epoch 17/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.4284 - val_loss: 0.4505\n",
            "Epoch 18/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.4256 - val_loss: 0.4479\n",
            "Epoch 19/100\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 0.4235 - val_loss: 0.4473\n",
            "Epoch 20/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.4218 - val_loss: 0.4463\n",
            "Epoch 21/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4203 - val_loss: 0.4453\n",
            "Epoch 22/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.4187 - val_loss: 0.4441\n",
            "Epoch 23/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4171 - val_loss: 0.4450\n",
            "Epoch 24/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.4163 - val_loss: 0.4443\n",
            "Epoch 25/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.4151 - val_loss: 0.4448\n",
            "Epoch 26/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.4139 - val_loss: 0.4428\n",
            "Epoch 27/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.4132 - val_loss: 0.4423\n",
            "Epoch 28/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4121 - val_loss: 0.4425\n",
            "Epoch 29/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.4113 - val_loss: 0.4415\n",
            "Epoch 30/100\n",
            "7740/7740 [==============================] - 0s 59us/sample - loss: 0.4103 - val_loss: 0.4426\n",
            "Epoch 31/100\n",
            "7740/7740 [==============================] - 0s 62us/sample - loss: 0.4099 - val_loss: 0.4414\n",
            "Epoch 32/100\n",
            "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4093 - val_loss: 0.4396\n",
            "Epoch 33/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.4084 - val_loss: 0.4413\n",
            "Epoch 34/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.4081 - val_loss: 0.4393\n",
            "Epoch 35/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4067 - val_loss: 0.4372\n",
            "Epoch 36/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.4067 - val_loss: 0.4364\n",
            "Epoch 37/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.4057 - val_loss: 0.4369\n",
            "Epoch 38/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.4058 - val_loss: 0.4348\n",
            "Epoch 39/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.4051 - val_loss: 0.4340\n",
            "Epoch 40/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.4045 - val_loss: 0.4335\n",
            "Epoch 41/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.4041 - val_loss: 0.4332\n",
            "Epoch 42/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.4037 - val_loss: 0.4320\n",
            "Epoch 43/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.4032 - val_loss: 0.4307\n",
            "Epoch 44/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.4029 - val_loss: 0.4303\n",
            "Epoch 45/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4023 - val_loss: 0.4300\n",
            "Epoch 46/100\n",
            "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4017 - val_loss: 0.4286\n",
            "Epoch 47/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.4014 - val_loss: 0.4277\n",
            "Epoch 48/100\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.4012 - val_loss: 0.4276\n",
            "Epoch 49/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4007 - val_loss: 0.4275\n",
            "Epoch 50/100\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.4004 - val_loss: 0.4281\n",
            "Epoch 51/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3999 - val_loss: 0.4266\n",
            "Epoch 52/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3995 - val_loss: 0.4263\n",
            "Epoch 53/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3993 - val_loss: 0.4257\n",
            "Epoch 54/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3988 - val_loss: 0.4248\n",
            "Epoch 55/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3983 - val_loss: 0.4252\n",
            "Epoch 56/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3980 - val_loss: 0.4246\n",
            "Epoch 57/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3973 - val_loss: 0.4241\n",
            "Epoch 58/100\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.3971 - val_loss: 0.4232\n",
            "Epoch 59/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3970 - val_loss: 0.4237\n",
            "Epoch 60/100\n",
            "7740/7740 [==============================] - 0s 46us/sample - loss: 0.3965 - val_loss: 0.4244\n",
            "Epoch 61/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3962 - val_loss: 0.4237\n",
            "Epoch 62/100\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.3959 - val_loss: 0.4225\n",
            "Epoch 63/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3955 - val_loss: 0.4214\n",
            "Epoch 64/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3951 - val_loss: 0.4205\n",
            "Epoch 65/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3949 - val_loss: 0.4203\n",
            "Epoch 66/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3943 - val_loss: 0.4189\n",
            "Epoch 67/100\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.3941 - val_loss: 0.4181\n",
            "Epoch 68/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3940 - val_loss: 0.4180\n",
            "Epoch 69/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3937 - val_loss: 0.4181\n",
            "Epoch 70/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3934 - val_loss: 0.4172\n",
            "Epoch 71/100\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.3933 - val_loss: 0.4171\n",
            "Epoch 72/100\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.3929 - val_loss: 0.4170\n",
            "Epoch 73/100\n",
            "7740/7740 [==============================] - 0s 46us/sample - loss: 0.3920 - val_loss: 0.4181\n",
            "Epoch 74/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3924 - val_loss: 0.4156\n",
            "Epoch 75/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3920 - val_loss: 0.4151\n",
            "Epoch 76/100\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.3917 - val_loss: 0.4145\n",
            "Epoch 77/100\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.3914 - val_loss: 0.4140\n",
            "Epoch 78/100\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.3910 - val_loss: 0.4139\n",
            "Epoch 79/100\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.3909 - val_loss: 0.4136\n",
            "Epoch 80/100\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.3906 - val_loss: 0.4135\n",
            "Epoch 81/100\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.3904 - val_loss: 0.4131\n",
            "Epoch 82/100\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.3901 - val_loss: 0.4123\n",
            "Epoch 83/100\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.3899 - val_loss: 0.4122\n",
            "Epoch 84/100\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.3898 - val_loss: 0.4121\n",
            "Epoch 85/100\n",
            "7740/7740 [==============================] - 0s 46us/sample - loss: 0.3897 - val_loss: 0.4114\n",
            "Epoch 86/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3889 - val_loss: 0.4134\n",
            "Epoch 87/100\n",
            "7740/7740 [==============================] - 0s 46us/sample - loss: 0.3891 - val_loss: 0.4103\n",
            "Epoch 88/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3888 - val_loss: 0.4107\n",
            "Epoch 89/100\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.3886 - val_loss: 0.4092\n",
            "Epoch 90/100\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.3883 - val_loss: 0.4097\n",
            "Epoch 91/100\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.3884 - val_loss: 0.4091\n",
            "Epoch 92/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3878 - val_loss: 0.4088\n",
            "Epoch 93/100\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.3878 - val_loss: 0.4076\n",
            "Epoch 94/100\n",
            "7740/7740 [==============================] - 0s 45us/sample - loss: 0.3876 - val_loss: 0.4084\n",
            "Epoch 95/100\n",
            "7740/7740 [==============================] - 0s 46us/sample - loss: 0.3874 - val_loss: 0.4080\n",
            "Epoch 96/100\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.3872 - val_loss: 0.4080\n",
            "Epoch 97/100\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.3871 - val_loss: 0.4066\n",
            "Epoch 98/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3871 - val_loss: 0.4062\n",
            "Epoch 99/100\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.3866 - val_loss: 0.4063\n",
            "Epoch 100/100\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.3865 - val_loss: 0.4075\n",
            "3870/3870 [==============================] - 0s 22us/sample - loss: 0.3809\n",
            "7740/7740 [==============================] - 0s 21us/sample - loss: 0.3861\n",
            "[CV]  learning_rate=0.0012710173398986133, n_hidden=1, n_neurons=6, total=  39.5s\n",
            "[CV] learning_rate=0.02229227827393175, n_hidden=0, n_neurons=65 .....\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/100\n",
            "7740/7740 [==============================] - 1s 68us/sample - loss: 0.8516 - val_loss: 9.5115\n",
            "Epoch 2/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.5663 - val_loss: 0.3829\n",
            "Epoch 3/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.4527 - val_loss: 0.3747\n",
            "Epoch 4/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3736 - val_loss: 0.3494\n",
            "Epoch 5/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3646 - val_loss: 0.3385\n",
            "Epoch 6/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3737 - val_loss: 0.5051\n",
            "Epoch 7/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3646 - val_loss: 0.3373\n",
            "Epoch 8/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3610 - val_loss: 0.3279\n",
            "Epoch 9/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3469 - val_loss: 0.3304\n",
            "Epoch 10/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3414 - val_loss: 0.3568\n",
            "Epoch 11/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3392 - val_loss: 0.3185\n",
            "Epoch 12/100\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3335 - val_loss: 0.3658\n",
            "Epoch 13/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3305 - val_loss: 0.5862\n",
            "Epoch 14/100\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3367 - val_loss: 0.3380\n",
            "Epoch 15/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3260 - val_loss: 0.3078\n",
            "Epoch 16/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3258 - val_loss: 0.3368\n",
            "Epoch 17/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.4111 - val_loss: 0.3593\n",
            "Epoch 18/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3367 - val_loss: 0.3460\n",
            "Epoch 19/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3331 - val_loss: 0.5185\n",
            "Epoch 20/100\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3346 - val_loss: 0.3329\n",
            "Epoch 21/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3463 - val_loss: 0.3146\n",
            "Epoch 22/100\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3227 - val_loss: 0.3154\n",
            "Epoch 23/100\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3224 - val_loss: 0.3074\n",
            "Epoch 24/100\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3221 - val_loss: 0.3039\n",
            "Epoch 25/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3177 - val_loss: 0.3080\n",
            "Epoch 26/100\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3136 - val_loss: 0.3082\n",
            "Epoch 27/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3105 - val_loss: 0.3059\n",
            "Epoch 28/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3144 - val_loss: 0.3187\n",
            "Epoch 29/100\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3111 - val_loss: 0.2990\n",
            "Epoch 30/100\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3086 - val_loss: 0.3504\n",
            "Epoch 31/100\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3082 - val_loss: 0.2972\n",
            "Epoch 32/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3053 - val_loss: 0.3014\n",
            "Epoch 33/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3037 - val_loss: 0.3099\n",
            "Epoch 34/100\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3037 - val_loss: 0.3103\n",
            "Epoch 35/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3031 - val_loss: 0.2922\n",
            "Epoch 36/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3021 - val_loss: 0.2968\n",
            "Epoch 37/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3002 - val_loss: 0.4677\n",
            "Epoch 38/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.2996 - val_loss: 0.3045\n",
            "Epoch 39/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.2980 - val_loss: 0.2932\n",
            "Epoch 40/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.2989 - val_loss: 0.5640\n",
            "Epoch 41/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3029 - val_loss: 0.2887\n",
            "Epoch 42/100\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3071 - val_loss: 0.3293\n",
            "Epoch 43/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3000 - val_loss: 0.2927\n",
            "Epoch 44/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.2986 - val_loss: 0.2928\n",
            "Epoch 45/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.2966 - val_loss: 0.3894\n",
            "Epoch 46/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3002 - val_loss: 0.2950\n",
            "Epoch 47/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.2979 - val_loss: 0.3592\n",
            "Epoch 48/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.2969 - val_loss: 0.2925\n",
            "Epoch 49/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.2977 - val_loss: 0.2882\n",
            "Epoch 50/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.2949 - val_loss: 0.3361\n",
            "Epoch 51/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.2927 - val_loss: 0.2863\n",
            "Epoch 52/100\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.2944 - val_loss: 0.2888\n",
            "Epoch 53/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.2940 - val_loss: 0.2936\n",
            "Epoch 54/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.2920 - val_loss: 0.3534\n",
            "Epoch 55/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.2921 - val_loss: 0.2856\n",
            "Epoch 56/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.2928 - val_loss: 0.2956\n",
            "Epoch 57/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.2909 - val_loss: 0.2860\n",
            "Epoch 58/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.2886 - val_loss: 0.3221\n",
            "Epoch 59/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.2892 - val_loss: 0.2946\n",
            "Epoch 60/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.2874 - val_loss: 0.3042\n",
            "Epoch 61/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.2863 - val_loss: 0.3004\n",
            "Epoch 62/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.2876 - val_loss: 0.3004\n",
            "Epoch 63/100\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 0.2892 - val_loss: 0.4285\n",
            "Epoch 64/100\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 0.2879 - val_loss: 0.4487\n",
            "Epoch 65/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.2876 - val_loss: 0.3006\n",
            "3870/3870 [==============================] - 0s 23us/sample - loss: 0.3226\n",
            "7740/7740 [==============================] - 0s 21us/sample - loss: 0.2835\n",
            "[CV]  learning_rate=0.02229227827393175, n_hidden=0, n_neurons=65, total=  25.7s\n",
            "[CV] learning_rate=0.02229227827393175, n_hidden=0, n_neurons=65 .....\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/100\n",
            "7740/7740 [==============================] - 1s 66us/sample - loss: 0.6593 - val_loss: 1.9274\n",
            "Epoch 2/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.4241 - val_loss: 0.3817\n",
            "Epoch 3/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3998 - val_loss: 0.3681\n",
            "Epoch 4/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3943 - val_loss: 0.4052\n",
            "Epoch 5/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3818 - val_loss: 0.4978\n",
            "Epoch 6/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3778 - val_loss: 0.4089\n",
            "Epoch 7/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3731 - val_loss: 0.4019\n",
            "Epoch 8/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3670 - val_loss: 0.6145\n",
            "Epoch 9/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3636 - val_loss: 0.5086\n",
            "Epoch 10/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3579 - val_loss: 0.5808\n",
            "Epoch 11/100\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3595 - val_loss: 0.4036\n",
            "Epoch 12/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3575 - val_loss: 0.3573\n",
            "Epoch 13/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3531 - val_loss: 0.4633\n",
            "Epoch 14/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3680 - val_loss: 1.0989\n",
            "Epoch 15/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.4423 - val_loss: 0.3885\n",
            "Epoch 16/100\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3541 - val_loss: 0.3923\n",
            "Epoch 17/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3487 - val_loss: 0.7111\n",
            "Epoch 18/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3458 - val_loss: 0.3554\n",
            "Epoch 19/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3436 - val_loss: 1.0349\n",
            "Epoch 20/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3413 - val_loss: 0.8843\n",
            "Epoch 21/100\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3384 - val_loss: 0.3477\n",
            "Epoch 22/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3365 - val_loss: 0.3725\n",
            "Epoch 23/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3336 - val_loss: 0.3680\n",
            "Epoch 24/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3315 - val_loss: 0.6838\n",
            "Epoch 25/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3300 - val_loss: 1.1891\n",
            "Epoch 26/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3287 - val_loss: 1.0475\n",
            "Epoch 27/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3290 - val_loss: 1.0169\n",
            "Epoch 28/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3260 - val_loss: 0.6606\n",
            "Epoch 29/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3239 - val_loss: 0.3384\n",
            "Epoch 30/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3228 - val_loss: 0.3311\n",
            "Epoch 31/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3209 - val_loss: 0.6076\n",
            "Epoch 32/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3197 - val_loss: 1.0622\n",
            "Epoch 33/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3204 - val_loss: 0.3113\n",
            "Epoch 34/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3173 - val_loss: 1.2406\n",
            "Epoch 35/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3178 - val_loss: 0.3691\n",
            "Epoch 36/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3146 - val_loss: 0.3377\n",
            "Epoch 37/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3171 - val_loss: 0.3719\n",
            "Epoch 38/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3140 - val_loss: 0.7175\n",
            "Epoch 39/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3137 - val_loss: 0.3068\n",
            "Epoch 40/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3133 - val_loss: 0.3426\n",
            "Epoch 41/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3123 - val_loss: 0.3082\n",
            "Epoch 42/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3119 - val_loss: 0.3308\n",
            "Epoch 43/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3108 - val_loss: 1.7581\n",
            "Epoch 44/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3111 - val_loss: 0.3050\n",
            "Epoch 45/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3101 - val_loss: 0.9803\n",
            "Epoch 46/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3105 - val_loss: 0.5200\n",
            "Epoch 47/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3100 - val_loss: 0.3867\n",
            "Epoch 48/100\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3097 - val_loss: 0.4475\n",
            "Epoch 49/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3069 - val_loss: 0.9473\n",
            "Epoch 50/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3087 - val_loss: 0.3045\n",
            "Epoch 51/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3073 - val_loss: 1.3196\n",
            "Epoch 52/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3064 - val_loss: 0.6923\n",
            "Epoch 53/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3073 - val_loss: 1.2063\n",
            "Epoch 54/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3072 - val_loss: 0.3251\n",
            "Epoch 55/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3062 - val_loss: 0.5242\n",
            "Epoch 56/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3038 - val_loss: 0.8890\n",
            "Epoch 57/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3076 - val_loss: 0.9613\n",
            "Epoch 58/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3066 - val_loss: 0.5974\n",
            "Epoch 59/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3041 - val_loss: 0.5559\n",
            "Epoch 60/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3050 - val_loss: 1.0145\n",
            "3870/3870 [==============================] - 0s 23us/sample - loss: 0.3327\n",
            "7740/7740 [==============================] - 0s 22us/sample - loss: 0.2995\n",
            "[CV]  learning_rate=0.02229227827393175, n_hidden=0, n_neurons=65, total=  23.6s\n",
            "[CV] learning_rate=0.02229227827393175, n_hidden=0, n_neurons=65 .....\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/100\n",
            "7740/7740 [==============================] - 1s 68us/sample - loss: 0.6860 - val_loss: 10.8388\n",
            "Epoch 2/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 1.1168 - val_loss: 1.1471\n",
            "Epoch 3/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 3.1918 - val_loss: 251.2982\n",
            "Epoch 4/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 1.5649 - val_loss: 1.2181\n",
            "Epoch 5/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.4214 - val_loss: 0.4133\n",
            "Epoch 6/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3889 - val_loss: 0.3766\n",
            "Epoch 7/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3839 - val_loss: 0.4343\n",
            "Epoch 8/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3759 - val_loss: 0.5151\n",
            "Epoch 9/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3693 - val_loss: 0.3522\n",
            "Epoch 10/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3649 - val_loss: 0.4924\n",
            "Epoch 11/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3626 - val_loss: 0.3657\n",
            "Epoch 12/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3563 - val_loss: 0.3836\n",
            "Epoch 13/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3533 - val_loss: 0.3369\n",
            "Epoch 14/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3501 - val_loss: 0.3324\n",
            "Epoch 15/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3491 - val_loss: 0.3319\n",
            "Epoch 16/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3452 - val_loss: 0.3368\n",
            "Epoch 17/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3444 - val_loss: 0.3590\n",
            "Epoch 18/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3413 - val_loss: 0.3423\n",
            "Epoch 19/100\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3355 - val_loss: 1.2162\n",
            "Epoch 20/100\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3462 - val_loss: 0.3357\n",
            "Epoch 21/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3371 - val_loss: 0.3188\n",
            "Epoch 22/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3334 - val_loss: 0.4148\n",
            "Epoch 23/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3353 - val_loss: 0.3246\n",
            "Epoch 24/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3269 - val_loss: 0.3313\n",
            "Epoch 25/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3246 - val_loss: 0.3309\n",
            "Epoch 26/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3281 - val_loss: 0.3213\n",
            "Epoch 27/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3240 - val_loss: 0.3116\n",
            "Epoch 28/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3233 - val_loss: 0.3402\n",
            "Epoch 29/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3208 - val_loss: 0.3124\n",
            "Epoch 30/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3208 - val_loss: 0.5901\n",
            "Epoch 31/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3163 - val_loss: 0.3132\n",
            "Epoch 32/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3168 - val_loss: 0.3249\n",
            "Epoch 33/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3158 - val_loss: 0.3193\n",
            "Epoch 34/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3202 - val_loss: 0.3307\n",
            "Epoch 35/100\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3148 - val_loss: 0.3065\n",
            "Epoch 36/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3138 - val_loss: 0.3064\n",
            "Epoch 37/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3125 - val_loss: 0.3040\n",
            "Epoch 38/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3144 - val_loss: 0.3227\n",
            "Epoch 39/100\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3098 - val_loss: 0.3469\n",
            "Epoch 40/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3112 - val_loss: 0.3007\n",
            "Epoch 41/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3099 - val_loss: 0.3053\n",
            "Epoch 42/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3096 - val_loss: 0.3016\n",
            "Epoch 43/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3084 - val_loss: 0.3648\n",
            "Epoch 44/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3082 - val_loss: 0.3049\n",
            "Epoch 45/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3076 - val_loss: 0.3051\n",
            "Epoch 46/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3040 - val_loss: 0.3375\n",
            "Epoch 47/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3040 - val_loss: 0.3003\n",
            "Epoch 48/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3036 - val_loss: 0.3025\n",
            "Epoch 49/100\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3014 - val_loss: 0.3118\n",
            "Epoch 50/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3012 - val_loss: 0.4027\n",
            "Epoch 51/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3014 - val_loss: 0.3034\n",
            "Epoch 52/100\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3007 - val_loss: 0.4969\n",
            "Epoch 53/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3030 - val_loss: 0.3046\n",
            "Epoch 54/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.2991 - val_loss: 0.3077\n",
            "Epoch 55/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.2994 - val_loss: 0.3254\n",
            "Epoch 56/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.2991 - val_loss: 0.2973\n",
            "Epoch 57/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3000 - val_loss: 0.3210\n",
            "Epoch 58/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.2991 - val_loss: 0.3231\n",
            "Epoch 59/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.2975 - val_loss: 0.3456\n",
            "Epoch 60/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.2978 - val_loss: 0.2990\n",
            "Epoch 61/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.2989 - val_loss: 0.3003\n",
            "Epoch 62/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.2945 - val_loss: 0.2959\n",
            "Epoch 63/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.2930 - val_loss: 0.3382\n",
            "Epoch 64/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.2934 - val_loss: 0.3024\n",
            "Epoch 65/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.2926 - val_loss: 0.3236\n",
            "Epoch 66/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.2937 - val_loss: 0.4075\n",
            "Epoch 67/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.2942 - val_loss: 0.2944\n",
            "Epoch 68/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.2963 - val_loss: 0.5344\n",
            "Epoch 69/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.2936 - val_loss: 0.3137\n",
            "Epoch 70/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.2913 - val_loss: 0.2959\n",
            "Epoch 71/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.2902 - val_loss: 0.3182\n",
            "Epoch 72/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.2914 - val_loss: 0.3047\n",
            "Epoch 73/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.2924 - val_loss: 0.2997\n",
            "Epoch 74/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.2898 - val_loss: 0.2981\n",
            "Epoch 75/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.2897 - val_loss: 0.2955\n",
            "Epoch 76/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.2909 - val_loss: 0.2991\n",
            "Epoch 77/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.2893 - val_loss: 0.3606\n",
            "3870/3870 [==============================] - 0s 24us/sample - loss: 0.3033\n",
            "7740/7740 [==============================] - 0s 24us/sample - loss: 0.2824\n",
            "[CV]  learning_rate=0.02229227827393175, n_hidden=0, n_neurons=65, total=  30.5s\n",
            "[CV] learning_rate=0.013389314011285533, n_hidden=0, n_neurons=18 ....\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/100\n",
            "7740/7740 [==============================] - 1s 67us/sample - loss: 1.0593 - val_loss: 11.7325\n",
            "Epoch 2/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5510 - val_loss: 0.4482\n",
            "Epoch 3/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.4472 - val_loss: 0.3993\n",
            "Epoch 4/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.4219 - val_loss: 0.3855\n",
            "Epoch 5/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.4156 - val_loss: 0.3795\n",
            "Epoch 6/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.4128 - val_loss: 0.3747\n",
            "Epoch 7/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3973 - val_loss: 0.3704\n",
            "Epoch 8/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3905 - val_loss: 0.3770\n",
            "Epoch 9/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3913 - val_loss: 0.3773\n",
            "Epoch 10/100\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3838 - val_loss: 0.3620\n",
            "Epoch 11/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3798 - val_loss: 0.3641\n",
            "Epoch 12/100\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3754 - val_loss: 0.3543\n",
            "Epoch 13/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3725 - val_loss: 0.3517\n",
            "Epoch 14/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3725 - val_loss: 0.3514\n",
            "Epoch 15/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3725 - val_loss: 0.3508\n",
            "Epoch 16/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3708 - val_loss: 0.3465\n",
            "Epoch 17/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3617 - val_loss: 0.3458\n",
            "Epoch 18/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3595 - val_loss: 0.3463\n",
            "Epoch 19/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3604 - val_loss: 0.3476\n",
            "Epoch 20/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3592 - val_loss: 0.3570\n",
            "Epoch 21/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3558 - val_loss: 0.3486\n",
            "Epoch 22/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3581 - val_loss: 0.3512\n",
            "Epoch 23/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3527 - val_loss: 0.3675\n",
            "Epoch 24/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3580 - val_loss: 0.3459\n",
            "Epoch 25/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3499 - val_loss: 0.3464\n",
            "Epoch 26/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3498 - val_loss: 0.3468\n",
            "Epoch 27/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3511 - val_loss: 0.3493\n",
            "3870/3870 [==============================] - 0s 23us/sample - loss: 0.3725\n",
            "7740/7740 [==============================] - 0s 22us/sample - loss: 0.3440\n",
            "[CV]  learning_rate=0.013389314011285533, n_hidden=0, n_neurons=18, total=  11.0s\n",
            "[CV] learning_rate=0.013389314011285533, n_hidden=0, n_neurons=18 ....\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/100\n",
            "7740/7740 [==============================] - 1s 65us/sample - loss: 0.7563 - val_loss: 1.1414\n",
            "Epoch 2/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.4811 - val_loss: 1.0244\n",
            "Epoch 3/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.4440 - val_loss: 0.5283\n",
            "Epoch 4/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.4287 - val_loss: 0.5093\n",
            "Epoch 5/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.4131 - val_loss: 0.4612\n",
            "Epoch 6/100\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.4109 - val_loss: 0.5532\n",
            "Epoch 7/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.4017 - val_loss: 0.6895\n",
            "Epoch 8/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3980 - val_loss: 0.7547\n",
            "Epoch 9/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3936 - val_loss: 0.6264\n",
            "Epoch 10/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3926 - val_loss: 0.5808\n",
            "Epoch 11/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3866 - val_loss: 0.7388\n",
            "Epoch 12/100\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3862 - val_loss: 0.7180\n",
            "Epoch 13/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3829 - val_loss: 0.7995\n",
            "Epoch 14/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3807 - val_loss: 0.4407\n",
            "Epoch 15/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3780 - val_loss: 0.7536\n",
            "Epoch 16/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3751 - val_loss: 0.8570\n",
            "Epoch 17/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3729 - val_loss: 1.2283\n",
            "Epoch 18/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3713 - val_loss: 0.5836\n",
            "Epoch 19/100\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3697 - val_loss: 0.5400\n",
            "Epoch 20/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3688 - val_loss: 0.4967\n",
            "Epoch 21/100\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3670 - val_loss: 0.6235\n",
            "Epoch 22/100\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3667 - val_loss: 0.5195\n",
            "Epoch 23/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3636 - val_loss: 0.6069\n",
            "Epoch 24/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3667 - val_loss: 0.6849\n",
            "3870/3870 [==============================] - 0s 23us/sample - loss: 0.3966\n",
            "7740/7740 [==============================] - 0s 23us/sample - loss: 0.3660\n",
            "[CV]  learning_rate=0.013389314011285533, n_hidden=0, n_neurons=18, total=   9.9s\n",
            "[CV] learning_rate=0.013389314011285533, n_hidden=0, n_neurons=18 ....\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/100\n",
            "7740/7740 [==============================] - 1s 65us/sample - loss: 0.7229 - val_loss: 8.6390\n",
            "Epoch 2/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.4765 - val_loss: 5.4122\n",
            "Epoch 3/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.4765 - val_loss: 12.7885\n",
            "Epoch 4/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.4912 - val_loss: 0.4784\n",
            "Epoch 5/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5991 - val_loss: 0.3910\n",
            "Epoch 6/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.4132 - val_loss: 0.3882\n",
            "Epoch 7/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.4031 - val_loss: 0.3718\n",
            "Epoch 8/100\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3944 - val_loss: 0.3660\n",
            "Epoch 9/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3923 - val_loss: 0.3606\n",
            "Epoch 10/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3841 - val_loss: 0.3831\n",
            "Epoch 11/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3795 - val_loss: 0.3552\n",
            "Epoch 12/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3781 - val_loss: 0.3523\n",
            "Epoch 13/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3733 - val_loss: 0.3492\n",
            "Epoch 14/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3731 - val_loss: 0.3477\n",
            "Epoch 15/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3768 - val_loss: 0.3829\n",
            "Epoch 16/100\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 0.3733 - val_loss: 0.3527\n",
            "Epoch 17/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3790 - val_loss: 0.3536\n",
            "Epoch 18/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3692 - val_loss: 0.3427\n",
            "Epoch 19/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3653 - val_loss: 0.3431\n",
            "Epoch 20/100\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3646 - val_loss: 0.3452\n",
            "Epoch 21/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3633 - val_loss: 0.3534\n",
            "Epoch 22/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3627 - val_loss: 0.5184\n",
            "Epoch 23/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3631 - val_loss: 0.3393\n",
            "Epoch 24/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3595 - val_loss: 0.4513\n",
            "Epoch 25/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3715 - val_loss: 0.3397\n",
            "Epoch 26/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3670 - val_loss: 0.3378\n",
            "Epoch 27/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3606 - val_loss: 0.3388\n",
            "Epoch 28/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3583 - val_loss: 0.3368\n",
            "Epoch 29/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3564 - val_loss: 0.3469\n",
            "Epoch 30/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3553 - val_loss: 0.3497\n",
            "Epoch 31/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3540 - val_loss: 0.3687\n",
            "Epoch 32/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3557 - val_loss: 0.3353\n",
            "Epoch 33/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3521 - val_loss: 0.3454\n",
            "Epoch 34/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3575 - val_loss: 0.3365\n",
            "Epoch 35/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3521 - val_loss: 0.3327\n",
            "Epoch 36/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3495 - val_loss: 0.3432\n",
            "Epoch 37/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3517 - val_loss: 0.3349\n",
            "Epoch 38/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3733 - val_loss: 0.3349\n",
            "Epoch 39/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3557 - val_loss: 0.3383\n",
            "Epoch 40/100\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3496 - val_loss: 0.3779\n",
            "Epoch 41/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3505 - val_loss: 0.4080\n",
            "Epoch 42/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3478 - val_loss: 0.3311\n",
            "Epoch 43/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3479 - val_loss: 0.3347\n",
            "Epoch 44/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3458 - val_loss: 0.3750\n",
            "Epoch 45/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3469 - val_loss: 0.3304\n",
            "Epoch 46/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3476 - val_loss: 0.3315\n",
            "Epoch 47/100\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3452 - val_loss: 0.3653\n",
            "Epoch 48/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3457 - val_loss: 0.3692\n",
            "Epoch 49/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3450 - val_loss: 0.3635\n",
            "Epoch 50/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3448 - val_loss: 0.3295\n",
            "Epoch 51/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3483 - val_loss: 0.3298\n",
            "Epoch 52/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3467 - val_loss: 0.3291\n",
            "Epoch 53/100\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3431 - val_loss: 0.3303\n",
            "Epoch 54/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3495 - val_loss: 0.3260\n",
            "Epoch 55/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3462 - val_loss: 0.3267\n",
            "Epoch 56/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3426 - val_loss: 0.3286\n",
            "Epoch 57/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3399 - val_loss: 0.3236\n",
            "Epoch 58/100\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3406 - val_loss: 0.3227\n",
            "Epoch 59/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3392 - val_loss: 0.3238\n",
            "Epoch 60/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3413 - val_loss: 0.3726\n",
            "Epoch 61/100\n",
            "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3384 - val_loss: 0.3232\n",
            "Epoch 62/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3378 - val_loss: 0.3850\n",
            "Epoch 63/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3384 - val_loss: 0.3270\n",
            "Epoch 64/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3405 - val_loss: 0.3240\n",
            "Epoch 65/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3378 - val_loss: 0.3302\n",
            "Epoch 66/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3359 - val_loss: 0.3239\n",
            "Epoch 67/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3378 - val_loss: 0.3208\n",
            "Epoch 68/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3365 - val_loss: 0.3383\n",
            "Epoch 69/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3351 - val_loss: 0.4702\n",
            "Epoch 70/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3365 - val_loss: 0.3201\n",
            "Epoch 71/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3379 - val_loss: 0.3180\n",
            "Epoch 72/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3346 - val_loss: 0.3312\n",
            "Epoch 73/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3366 - val_loss: 0.3275\n",
            "Epoch 74/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3438 - val_loss: 0.3251\n",
            "Epoch 75/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3363 - val_loss: 0.3195\n",
            "Epoch 76/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3350 - val_loss: 0.3223\n",
            "Epoch 77/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3332 - val_loss: 0.3558\n",
            "Epoch 78/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3349 - val_loss: 0.3211\n",
            "Epoch 79/100\n",
            "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3332 - val_loss: 0.3329\n",
            "Epoch 80/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3331 - val_loss: 0.4395\n",
            "Epoch 81/100\n",
            "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3337 - val_loss: 0.3186\n",
            "3870/3870 [==============================] - 0s 23us/sample - loss: 0.3297\n",
            "7740/7740 [==============================] - 0s 22us/sample - loss: 0.3294\n",
            "[CV]  learning_rate=0.013389314011285533, n_hidden=0, n_neurons=18, total=  31.6s\n",
            "[CV] learning_rate=0.011696544506002878, n_hidden=3, n_neurons=35 ....\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/100\n",
            "7740/7740 [==============================] - 1s 77us/sample - loss: 0.9086 - val_loss: 0.4823\n",
            "Epoch 2/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.4678 - val_loss: 0.4253\n",
            "Epoch 3/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3994 - val_loss: 0.4096\n",
            "Epoch 4/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3783 - val_loss: 0.3518\n",
            "Epoch 5/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3631 - val_loss: 0.3461\n",
            "Epoch 6/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3582 - val_loss: 0.3657\n",
            "Epoch 7/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3490 - val_loss: 0.3926\n",
            "Epoch 8/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3422 - val_loss: 0.3511\n",
            "Epoch 9/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3396 - val_loss: 0.3583\n",
            "Epoch 10/100\n",
            "7740/7740 [==============================] - 0s 58us/sample - loss: 0.3358 - val_loss: 0.3383\n",
            "Epoch 11/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3294 - val_loss: 0.3368\n",
            "Epoch 12/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3267 - val_loss: 0.3469\n",
            "Epoch 13/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3223 - val_loss: 0.3150\n",
            "Epoch 14/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3208 - val_loss: 0.3277\n",
            "Epoch 15/100\n",
            "7740/7740 [==============================] - 0s 58us/sample - loss: 0.3136 - val_loss: 0.3235\n",
            "Epoch 16/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3114 - val_loss: 0.3737\n",
            "Epoch 17/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3103 - val_loss: 0.2956\n",
            "Epoch 18/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3076 - val_loss: 0.3105\n",
            "Epoch 19/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3017 - val_loss: 0.3317\n",
            "Epoch 20/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3017 - val_loss: 0.3203\n",
            "Epoch 21/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2991 - val_loss: 0.3214\n",
            "Epoch 22/100\n",
            "7740/7740 [==============================] - 0s 60us/sample - loss: 0.2968 - val_loss: 0.3682\n",
            "Epoch 23/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2956 - val_loss: 0.3052\n",
            "Epoch 24/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2915 - val_loss: 0.3090\n",
            "Epoch 25/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2880 - val_loss: 0.3147\n",
            "Epoch 26/100\n",
            "7740/7740 [==============================] - 0s 59us/sample - loss: 0.2859 - val_loss: 0.3827\n",
            "Epoch 27/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2876 - val_loss: 0.2774\n",
            "Epoch 28/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2813 - val_loss: 0.3060\n",
            "Epoch 29/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2818 - val_loss: 0.2909\n",
            "Epoch 30/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2790 - val_loss: 0.3043\n",
            "Epoch 31/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2785 - val_loss: 0.3087\n",
            "Epoch 32/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2780 - val_loss: 0.3016\n",
            "Epoch 33/100\n",
            "7740/7740 [==============================] - 0s 58us/sample - loss: 0.2768 - val_loss: 0.3127\n",
            "Epoch 34/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2755 - val_loss: 0.2854\n",
            "Epoch 35/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2718 - val_loss: 0.3412\n",
            "Epoch 36/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2737 - val_loss: 0.2710\n",
            "Epoch 37/100\n",
            "7740/7740 [==============================] - 0s 59us/sample - loss: 0.2697 - val_loss: 0.2765\n",
            "Epoch 38/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2681 - val_loss: 0.3657\n",
            "Epoch 39/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2909 - val_loss: 0.2730\n",
            "Epoch 40/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2750 - val_loss: 0.3485\n",
            "Epoch 41/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2714 - val_loss: 0.4198\n",
            "Epoch 42/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2682 - val_loss: 0.4332\n",
            "Epoch 43/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2708 - val_loss: 0.2711\n",
            "Epoch 44/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2663 - val_loss: 0.2702\n",
            "Epoch 45/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2662 - val_loss: 0.3660\n",
            "Epoch 46/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2656 - val_loss: 0.3011\n",
            "Epoch 47/100\n",
            "7740/7740 [==============================] - 0s 58us/sample - loss: 0.2653 - val_loss: 0.3605\n",
            "Epoch 48/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2612 - val_loss: 0.2833\n",
            "Epoch 49/100\n",
            "7740/7740 [==============================] - 0s 59us/sample - loss: 0.2642 - val_loss: 0.8127\n",
            "Epoch 50/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2806 - val_loss: 0.4825\n",
            "Epoch 51/100\n",
            "7740/7740 [==============================] - 0s 60us/sample - loss: 0.2707 - val_loss: 1.2207\n",
            "Epoch 52/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2909 - val_loss: 0.4347\n",
            "Epoch 53/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2668 - val_loss: 0.4334\n",
            "Epoch 54/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2618 - val_loss: 0.2961\n",
            "3870/3870 [==============================] - 0s 25us/sample - loss: 0.3189\n",
            "7740/7740 [==============================] - 0s 26us/sample - loss: 0.2638\n",
            "[CV]  learning_rate=0.011696544506002878, n_hidden=3, n_neurons=35, total=  24.6s\n",
            "[CV] learning_rate=0.011696544506002878, n_hidden=3, n_neurons=35 ....\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/100\n",
            "7740/7740 [==============================] - 1s 79us/sample - loss: 0.8010 - val_loss: 0.4892\n",
            "Epoch 2/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.4604 - val_loss: 0.3907\n",
            "Epoch 3/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3973 - val_loss: 0.4431\n",
            "Epoch 4/100\n",
            "7740/7740 [==============================] - 0s 58us/sample - loss: 0.3722 - val_loss: 0.4880\n",
            "Epoch 5/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3599 - val_loss: 0.8178\n",
            "Epoch 6/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3542 - val_loss: 0.5536\n",
            "Epoch 7/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3473 - val_loss: 0.5726\n",
            "Epoch 8/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3405 - val_loss: 0.5665\n",
            "Epoch 9/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3349 - val_loss: 0.6424\n",
            "Epoch 10/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3335 - val_loss: 0.6588\n",
            "Epoch 11/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3282 - val_loss: 0.7353\n",
            "Epoch 12/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3242 - val_loss: 0.6900\n",
            "3870/3870 [==============================] - 0s 25us/sample - loss: 0.3488\n",
            "7740/7740 [==============================] - 0s 25us/sample - loss: 0.3207\n",
            "[CV]  learning_rate=0.011696544506002878, n_hidden=3, n_neurons=35, total=   6.0s\n",
            "[CV] learning_rate=0.011696544506002878, n_hidden=3, n_neurons=35 ....\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/100\n",
            "7740/7740 [==============================] - 1s 78us/sample - loss: 0.8751 - val_loss: 18.6589\n",
            "Epoch 2/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.9462 - val_loss: 2.4183\n",
            "Epoch 3/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.9008 - val_loss: 0.4897\n",
            "Epoch 4/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4720 - val_loss: 1.2783\n",
            "Epoch 5/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4741 - val_loss: 2.6113\n",
            "Epoch 6/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.4262 - val_loss: 3.9008\n",
            "Epoch 7/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4429 - val_loss: 34.7757\n",
            "Epoch 8/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.4027 - val_loss: 4.3036\n",
            "Epoch 9/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3895 - val_loss: 3.8143\n",
            "Epoch 10/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3732 - val_loss: 1.0334\n",
            "Epoch 11/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3705 - val_loss: 0.5665\n",
            "Epoch 12/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3588 - val_loss: 0.4122\n",
            "Epoch 13/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3512 - val_loss: 0.3574\n",
            "Epoch 14/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3473 - val_loss: 0.3424\n",
            "Epoch 15/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3434 - val_loss: 0.3615\n",
            "Epoch 16/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3415 - val_loss: 0.3327\n",
            "Epoch 17/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3391 - val_loss: 0.3482\n",
            "Epoch 18/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3338 - val_loss: 0.3192\n",
            "Epoch 19/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3323 - val_loss: 0.3220\n",
            "Epoch 20/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3258 - val_loss: 0.3155\n",
            "Epoch 21/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3260 - val_loss: 0.3389\n",
            "Epoch 22/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3214 - val_loss: 0.3215\n",
            "Epoch 23/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3193 - val_loss: 0.3375\n",
            "Epoch 24/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3156 - val_loss: 0.2966\n",
            "Epoch 25/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3124 - val_loss: 0.3072\n",
            "Epoch 26/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3104 - val_loss: 0.3078\n",
            "Epoch 27/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3079 - val_loss: 0.2968\n",
            "Epoch 28/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3059 - val_loss: 0.3206\n",
            "Epoch 29/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3034 - val_loss: 0.3226\n",
            "Epoch 30/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3026 - val_loss: 0.2865\n",
            "Epoch 31/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2990 - val_loss: 0.3261\n",
            "Epoch 32/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2980 - val_loss: 0.2869\n",
            "Epoch 33/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2964 - val_loss: 0.3033\n",
            "Epoch 34/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2968 - val_loss: 0.2972\n",
            "Epoch 35/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2938 - val_loss: 0.2921\n",
            "Epoch 36/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2907 - val_loss: 0.3055\n",
            "Epoch 37/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2901 - val_loss: 0.2849\n",
            "Epoch 38/100\n",
            "7740/7740 [==============================] - 0s 58us/sample - loss: 0.2865 - val_loss: 0.2948\n",
            "Epoch 39/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2856 - val_loss: 0.2998\n",
            "Epoch 40/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2858 - val_loss: 0.3048\n",
            "Epoch 41/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2831 - val_loss: 0.2787\n",
            "Epoch 42/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2825 - val_loss: 0.3031\n",
            "Epoch 43/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2820 - val_loss: 0.2811\n",
            "Epoch 44/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2796 - val_loss: 0.3017\n",
            "Epoch 45/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2789 - val_loss: 0.3086\n",
            "Epoch 46/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2791 - val_loss: 0.2893\n",
            "Epoch 47/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2771 - val_loss: 0.2893\n",
            "Epoch 48/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2764 - val_loss: 0.2889\n",
            "Epoch 49/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2775 - val_loss: 0.2781\n",
            "Epoch 50/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2756 - val_loss: 0.2814\n",
            "Epoch 51/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2736 - val_loss: 0.2838\n",
            "Epoch 52/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2745 - val_loss: 0.2873\n",
            "Epoch 53/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2719 - val_loss: 0.3097\n",
            "Epoch 54/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2729 - val_loss: 0.2870\n",
            "Epoch 55/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2717 - val_loss: 0.2990\n",
            "Epoch 56/100\n",
            "7740/7740 [==============================] - 0s 58us/sample - loss: 0.2717 - val_loss: 0.2805\n",
            "Epoch 57/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2708 - val_loss: 0.2888\n",
            "Epoch 58/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2684 - val_loss: 0.2779\n",
            "Epoch 59/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2689 - val_loss: 0.2818\n",
            "Epoch 60/100\n",
            "7740/7740 [==============================] - 0s 58us/sample - loss: 0.2690 - val_loss: 0.2743\n",
            "Epoch 61/100\n",
            "7740/7740 [==============================] - 0s 59us/sample - loss: 0.2677 - val_loss: 0.3273\n",
            "Epoch 62/100\n",
            "7740/7740 [==============================] - 0s 58us/sample - loss: 0.2671 - val_loss: 0.2831\n",
            "Epoch 63/100\n",
            "7740/7740 [==============================] - 0s 58us/sample - loss: 0.2664 - val_loss: 0.3209\n",
            "Epoch 64/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2672 - val_loss: 0.3045\n",
            "Epoch 65/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2651 - val_loss: 0.3462\n",
            "Epoch 66/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2662 - val_loss: 0.2769\n",
            "Epoch 67/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2639 - val_loss: 0.2781\n",
            "Epoch 68/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2628 - val_loss: 0.2961\n",
            "Epoch 69/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2632 - val_loss: 0.2890\n",
            "Epoch 70/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2608 - val_loss: 0.2899\n",
            "3870/3870 [==============================] - 0s 26us/sample - loss: 0.2999\n",
            "7740/7740 [==============================] - 0s 25us/sample - loss: 0.2615\n",
            "[CV]  learning_rate=0.011696544506002878, n_hidden=3, n_neurons=35, total=  31.4s\n",
            "[CV] learning_rate=0.013900679624986843, n_hidden=3, n_neurons=27 ....\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/100\n",
            "7740/7740 [==============================] - 1s 79us/sample - loss: 0.8292 - val_loss: 1.5926\n",
            "Epoch 2/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4481 - val_loss: 0.8813\n",
            "Epoch 3/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3940 - val_loss: 0.3545\n",
            "Epoch 4/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3710 - val_loss: 0.3515\n",
            "Epoch 5/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3601 - val_loss: 0.3477\n",
            "Epoch 6/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3512 - val_loss: 0.3472\n",
            "Epoch 7/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3496 - val_loss: 0.3338\n",
            "Epoch 8/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3457 - val_loss: 0.3453\n",
            "Epoch 9/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3392 - val_loss: 0.3692\n",
            "Epoch 10/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3377 - val_loss: 0.3322\n",
            "Epoch 11/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3301 - val_loss: 0.3161\n",
            "Epoch 12/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3274 - val_loss: 0.3263\n",
            "Epoch 13/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3296 - val_loss: 0.3152\n",
            "Epoch 14/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3258 - val_loss: 0.3301\n",
            "Epoch 15/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3218 - val_loss: 0.3464\n",
            "Epoch 16/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3211 - val_loss: 0.3059\n",
            "Epoch 17/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3167 - val_loss: 0.3529\n",
            "Epoch 18/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3122 - val_loss: 0.3112\n",
            "Epoch 19/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3146 - val_loss: 0.3058\n",
            "Epoch 20/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3078 - val_loss: 0.4648\n",
            "Epoch 21/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3115 - val_loss: 0.3512\n",
            "Epoch 22/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3105 - val_loss: 0.2929\n",
            "Epoch 23/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3054 - val_loss: 0.3072\n",
            "Epoch 24/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3058 - val_loss: 0.3296\n",
            "Epoch 25/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3014 - val_loss: 0.3258\n",
            "Epoch 26/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3003 - val_loss: 0.3150\n",
            "Epoch 27/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2972 - val_loss: 0.3144\n",
            "Epoch 28/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2963 - val_loss: 0.4051\n",
            "Epoch 29/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.2972 - val_loss: 0.3333\n",
            "Epoch 30/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.2918 - val_loss: 0.3090\n",
            "Epoch 31/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.2909 - val_loss: 0.3016\n",
            "Epoch 32/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2909 - val_loss: 0.2954\n",
            "3870/3870 [==============================] - 0s 25us/sample - loss: 0.3098\n",
            "7740/7740 [==============================] - 0s 25us/sample - loss: 0.2766\n",
            "[CV]  learning_rate=0.013900679624986843, n_hidden=3, n_neurons=27, total=  14.8s\n",
            "[CV] learning_rate=0.013900679624986843, n_hidden=3, n_neurons=27 ....\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/100\n",
            "7740/7740 [==============================] - 1s 79us/sample - loss: 0.6733 - val_loss: 0.4722\n",
            "Epoch 2/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.4101 - val_loss: 0.6391\n",
            "Epoch 3/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3823 - val_loss: 0.6468\n",
            "Epoch 4/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3709 - val_loss: 0.5809\n",
            "Epoch 5/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3581 - val_loss: 0.4992\n",
            "Epoch 6/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3524 - val_loss: 0.5538\n",
            "Epoch 7/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3481 - val_loss: 0.6087\n",
            "Epoch 8/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3453 - val_loss: 0.4286\n",
            "Epoch 9/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3397 - val_loss: 0.4154\n",
            "Epoch 10/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3321 - val_loss: 0.3769\n",
            "Epoch 11/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3318 - val_loss: 0.3769\n",
            "Epoch 12/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3292 - val_loss: 0.3568\n",
            "Epoch 13/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3274 - val_loss: 0.3900\n",
            "Epoch 14/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3233 - val_loss: 0.3744\n",
            "Epoch 15/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3224 - val_loss: 0.3589\n",
            "Epoch 16/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3149 - val_loss: 0.3226\n",
            "Epoch 17/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3133 - val_loss: 0.3414\n",
            "Epoch 18/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3129 - val_loss: 0.3137\n",
            "Epoch 19/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3099 - val_loss: 0.3453\n",
            "Epoch 20/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3093 - val_loss: 0.3135\n",
            "Epoch 21/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3055 - val_loss: 0.3021\n",
            "Epoch 22/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3037 - val_loss: 0.3033\n",
            "Epoch 23/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3037 - val_loss: 0.2997\n",
            "Epoch 24/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3032 - val_loss: 0.3086\n",
            "Epoch 25/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2991 - val_loss: 0.3058\n",
            "Epoch 26/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2975 - val_loss: 0.3274\n",
            "Epoch 27/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2953 - val_loss: 0.3004\n",
            "Epoch 28/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2936 - val_loss: 0.2854\n",
            "Epoch 29/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2889 - val_loss: 0.3138\n",
            "Epoch 30/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2902 - val_loss: 0.4046\n",
            "Epoch 31/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2891 - val_loss: 0.3774\n",
            "Epoch 32/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.2852 - val_loss: 0.3659\n",
            "Epoch 33/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.2853 - val_loss: 0.3041\n",
            "Epoch 34/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2839 - val_loss: 0.2884\n",
            "Epoch 35/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2833 - val_loss: 0.2784\n",
            "Epoch 36/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2794 - val_loss: 0.3235\n",
            "Epoch 37/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.2838 - val_loss: 0.4543\n",
            "Epoch 38/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2777 - val_loss: 0.4160\n",
            "Epoch 39/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.2768 - val_loss: 0.3792\n",
            "Epoch 40/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2778 - val_loss: 0.2864\n",
            "Epoch 41/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2759 - val_loss: 0.2791\n",
            "Epoch 42/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2723 - val_loss: 0.2813\n",
            "Epoch 43/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2735 - val_loss: 0.3338\n",
            "Epoch 44/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2704 - val_loss: 0.2921\n",
            "Epoch 45/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2714 - val_loss: 0.3320\n",
            "3870/3870 [==============================] - 0s 25us/sample - loss: 0.3222\n",
            "7740/7740 [==============================] - 0s 25us/sample - loss: 0.2693\n",
            "[CV]  learning_rate=0.013900679624986843, n_hidden=3, n_neurons=27, total=  20.1s\n",
            "[CV] learning_rate=0.013900679624986843, n_hidden=3, n_neurons=27 ....\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/100\n",
            "7740/7740 [==============================] - 1s 77us/sample - loss: 0.6876 - val_loss: 0.7188\n",
            "Epoch 2/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4206 - val_loss: 0.4482\n",
            "Epoch 3/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3963 - val_loss: 0.4052\n",
            "Epoch 4/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3825 - val_loss: 0.3617\n",
            "Epoch 5/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3726 - val_loss: 0.3419\n",
            "Epoch 6/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3663 - val_loss: 0.3492\n",
            "Epoch 7/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3619 - val_loss: 0.5143\n",
            "Epoch 8/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3585 - val_loss: 0.3777\n",
            "Epoch 9/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3514 - val_loss: 0.3902\n",
            "Epoch 10/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3492 - val_loss: 0.3748\n",
            "Epoch 11/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3427 - val_loss: 0.4215\n",
            "Epoch 12/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3388 - val_loss: 0.3500\n",
            "Epoch 13/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3374 - val_loss: 0.4526\n",
            "Epoch 14/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3331 - val_loss: 0.3169\n",
            "Epoch 15/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3315 - val_loss: 0.3324\n",
            "Epoch 16/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3303 - val_loss: 0.3072\n",
            "Epoch 17/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3275 - val_loss: 0.3510\n",
            "Epoch 18/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3237 - val_loss: 0.3663\n",
            "Epoch 19/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3230 - val_loss: 0.3077\n",
            "Epoch 20/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3193 - val_loss: 0.3818\n",
            "Epoch 21/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3174 - val_loss: 0.3072\n",
            "Epoch 22/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3143 - val_loss: 0.7166\n",
            "Epoch 23/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3244 - val_loss: 0.5171\n",
            "Epoch 24/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3174 - val_loss: 0.4320\n",
            "Epoch 25/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3123 - val_loss: 0.3112\n",
            "Epoch 26/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3128 - val_loss: 0.3381\n",
            "3870/3870 [==============================] - 0s 25us/sample - loss: 0.3307\n",
            "7740/7740 [==============================] - 0s 25us/sample - loss: 0.3188\n",
            "[CV]  learning_rate=0.013900679624986843, n_hidden=3, n_neurons=27, total=  11.7s\n",
            "[CV] learning_rate=0.018799462448897047, n_hidden=2, n_neurons=35 ....\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/100\n",
            "7740/7740 [==============================] - 1s 74us/sample - loss: 0.6989 - val_loss: 0.5622\n",
            "Epoch 2/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.4184 - val_loss: 1.6244\n",
            "Epoch 3/100\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3853 - val_loss: 0.6169\n",
            "Epoch 4/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3641 - val_loss: 0.7823\n",
            "Epoch 5/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3716 - val_loss: 0.4203\n",
            "Epoch 6/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3520 - val_loss: 1.7930\n",
            "Epoch 7/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3733 - val_loss: 0.9484\n",
            "Epoch 8/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3481 - val_loss: 2.6356\n",
            "Epoch 9/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3563 - val_loss: 1.5026\n",
            "Epoch 10/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3446 - val_loss: 0.3640\n",
            "Epoch 11/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3294 - val_loss: 0.3262\n",
            "Epoch 12/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3227 - val_loss: 0.3265\n",
            "Epoch 13/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3185 - val_loss: 0.3699\n",
            "Epoch 14/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3181 - val_loss: 0.3316\n",
            "Epoch 15/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3129 - val_loss: 0.2988\n",
            "Epoch 16/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3086 - val_loss: 0.3452\n",
            "Epoch 17/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3036 - val_loss: 0.3249\n",
            "Epoch 18/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3032 - val_loss: 0.4108\n",
            "Epoch 19/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3012 - val_loss: 0.3686\n",
            "Epoch 20/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.2997 - val_loss: 0.2956\n",
            "Epoch 21/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.2948 - val_loss: 0.2891\n",
            "Epoch 22/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.2936 - val_loss: 0.3071\n",
            "Epoch 23/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.2916 - val_loss: 0.2854\n",
            "Epoch 24/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.2930 - val_loss: 0.3165\n",
            "Epoch 25/100\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.2891 - val_loss: 0.2989\n",
            "Epoch 26/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.2875 - val_loss: 0.2998\n",
            "Epoch 27/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.2843 - val_loss: 0.3003\n",
            "Epoch 28/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.2869 - val_loss: 0.3094\n",
            "Epoch 29/100\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.2820 - val_loss: 0.3218\n",
            "Epoch 30/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.2821 - val_loss: 0.3056\n",
            "Epoch 31/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.2815 - val_loss: 0.3036\n",
            "Epoch 32/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.2797 - val_loss: 0.3043\n",
            "Epoch 33/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2775 - val_loss: 0.2847\n",
            "Epoch 34/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.2799 - val_loss: 0.3019\n",
            "Epoch 35/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.2771 - val_loss: 0.2885\n",
            "Epoch 36/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.2780 - val_loss: 0.3624\n",
            "Epoch 37/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.2734 - val_loss: 0.2873\n",
            "Epoch 38/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2747 - val_loss: 0.3110\n",
            "Epoch 39/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.2723 - val_loss: 0.2952\n",
            "Epoch 40/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.2706 - val_loss: 0.3212\n",
            "Epoch 41/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.2709 - val_loss: 0.2861\n",
            "Epoch 42/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2710 - val_loss: 0.3293\n",
            "Epoch 43/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2729 - val_loss: 0.2745\n",
            "Epoch 44/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2702 - val_loss: 0.3372\n",
            "Epoch 45/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.2679 - val_loss: 0.3065\n",
            "Epoch 46/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2704 - val_loss: 0.3448\n",
            "Epoch 47/100\n",
            "7740/7740 [==============================] - 0s 58us/sample - loss: 0.2689 - val_loss: 0.3537\n",
            "Epoch 48/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2660 - val_loss: 0.3760\n",
            "Epoch 49/100\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2661 - val_loss: 0.2838\n",
            "Epoch 50/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2639 - val_loss: 0.2749\n",
            "Epoch 51/100\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.2661 - val_loss: 0.2934\n",
            "Epoch 52/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.2650 - val_loss: 0.3786\n",
            "Epoch 53/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.2645 - val_loss: 0.9245\n",
            "3870/3870 [==============================] - 0s 23us/sample - loss: 0.3134\n",
            "7740/7740 [==============================] - 0s 24us/sample - loss: 0.2669\n",
            "[CV]  learning_rate=0.018799462448897047, n_hidden=2, n_neurons=35, total=  22.9s\n",
            "[CV] learning_rate=0.018799462448897047, n_hidden=2, n_neurons=35 ....\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/100\n",
            "7740/7740 [==============================] - 1s 73us/sample - loss: 0.6711 - val_loss: 0.3926\n",
            "Epoch 2/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3953 - val_loss: 0.5509\n",
            "Epoch 3/100\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3723 - val_loss: 0.8342\n",
            "Epoch 4/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3608 - val_loss: 1.0785\n",
            "Epoch 5/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3508 - val_loss: 0.9770\n",
            "Epoch 6/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3454 - val_loss: 0.4643\n",
            "Epoch 7/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3422 - val_loss: 0.6505\n",
            "Epoch 8/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3360 - val_loss: 0.4274\n",
            "Epoch 9/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3305 - val_loss: 0.3695\n",
            "Epoch 10/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3277 - val_loss: 0.7260\n",
            "Epoch 11/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3228 - val_loss: 0.3394\n",
            "Epoch 12/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3192 - val_loss: 0.5324\n",
            "Epoch 13/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3138 - val_loss: 0.4918\n",
            "Epoch 14/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3139 - val_loss: 0.3292\n",
            "Epoch 15/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3091 - val_loss: 0.5528\n",
            "Epoch 16/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3067 - val_loss: 1.2003\n",
            "Epoch 17/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3067 - val_loss: 0.3027\n",
            "Epoch 18/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3000 - val_loss: 0.9700\n",
            "Epoch 19/100\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.2974 - val_loss: 1.5790\n",
            "Epoch 20/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2955 - val_loss: 0.3116\n",
            "Epoch 21/100\n",
            "7740/7740 [==============================] - 0s 62us/sample - loss: 0.2965 - val_loss: 0.3404\n",
            "Epoch 22/100\n",
            "7740/7740 [==============================] - 0s 64us/sample - loss: 0.2914 - val_loss: 1.2913\n",
            "Epoch 23/100\n",
            "7740/7740 [==============================] - 1s 66us/sample - loss: 0.2911 - val_loss: 0.3033\n",
            "Epoch 24/100\n",
            "7740/7740 [==============================] - 1s 65us/sample - loss: 0.2876 - val_loss: 0.4122\n",
            "Epoch 25/100\n",
            "7740/7740 [==============================] - 0s 64us/sample - loss: 0.2872 - val_loss: 0.7181\n",
            "Epoch 26/100\n",
            "7740/7740 [==============================] - 0s 64us/sample - loss: 0.2849 - val_loss: 0.4278\n",
            "Epoch 27/100\n",
            "7740/7740 [==============================] - 0s 63us/sample - loss: 0.2847 - val_loss: 0.9742\n",
            "3870/3870 [==============================] - 0s 30us/sample - loss: 0.3258\n",
            "7740/7740 [==============================] - 0s 30us/sample - loss: 0.2726\n",
            "[CV]  learning_rate=0.018799462448897047, n_hidden=2, n_neurons=35, total=  12.6s\n",
            "[CV] learning_rate=0.018799462448897047, n_hidden=2, n_neurons=35 ....\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/100\n",
            "7740/7740 [==============================] - 1s 86us/sample - loss: 0.6597 - val_loss: 0.5069\n",
            "Epoch 2/100\n",
            "7740/7740 [==============================] - 0s 62us/sample - loss: 0.4304 - val_loss: 1.0330\n",
            "Epoch 3/100\n",
            "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4138 - val_loss: 0.6716\n",
            "Epoch 4/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3912 - val_loss: 3.1627\n",
            "Epoch 5/100\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.4019 - val_loss: 0.3290\n",
            "Epoch 6/100\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3606 - val_loss: 0.5573\n",
            "Epoch 7/100\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3553 - val_loss: 0.6948\n",
            "Epoch 8/100\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3535 - val_loss: 0.3987\n",
            "Epoch 9/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3404 - val_loss: 0.3577\n",
            "Epoch 10/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3368 - val_loss: 0.3340\n",
            "Epoch 11/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3287 - val_loss: 0.3359\n",
            "Epoch 12/100\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3275 - val_loss: 0.3084\n",
            "Epoch 13/100\n",
            "7740/7740 [==============================] - 0s 58us/sample - loss: 0.3197 - val_loss: 0.3249\n",
            "Epoch 14/100\n",
            "7740/7740 [==============================] - 0s 62us/sample - loss: 0.3195 - val_loss: 0.3200\n",
            "Epoch 15/100\n",
            "7740/7740 [==============================] - 0s 60us/sample - loss: 0.3157 - val_loss: 0.4447\n",
            "Epoch 16/100\n",
            "7740/7740 [==============================] - 0s 62us/sample - loss: 0.3151 - val_loss: 0.3515\n",
            "Epoch 17/100\n",
            "7740/7740 [==============================] - 0s 64us/sample - loss: 0.3133 - val_loss: 0.3235\n",
            "Epoch 18/100\n",
            "7740/7740 [==============================] - 0s 64us/sample - loss: 0.3083 - val_loss: 0.3573\n",
            "Epoch 19/100\n",
            "7740/7740 [==============================] - 1s 65us/sample - loss: 0.3073 - val_loss: 0.3281\n",
            "Epoch 20/100\n",
            "7740/7740 [==============================] - 1s 65us/sample - loss: 0.3047 - val_loss: 0.2989\n",
            "Epoch 21/100\n",
            "7740/7740 [==============================] - 0s 63us/sample - loss: 0.3042 - val_loss: 0.3180\n",
            "Epoch 22/100\n",
            "7740/7740 [==============================] - 0s 64us/sample - loss: 0.3071 - val_loss: 0.3230\n",
            "Epoch 23/100\n",
            "7740/7740 [==============================] - 0s 61us/sample - loss: 0.3065 - val_loss: 0.3500\n",
            "Epoch 24/100\n",
            "7740/7740 [==============================] - 0s 62us/sample - loss: 0.2995 - val_loss: 0.3261\n",
            "Epoch 25/100\n",
            "7740/7740 [==============================] - 0s 61us/sample - loss: 0.2975 - val_loss: 0.3063\n",
            "Epoch 26/100\n",
            "7740/7740 [==============================] - 0s 62us/sample - loss: 0.2958 - val_loss: 0.3234\n",
            "Epoch 27/100\n",
            "7740/7740 [==============================] - 0s 64us/sample - loss: 0.2935 - val_loss: 0.2856\n",
            "Epoch 28/100\n",
            "7740/7740 [==============================] - 0s 61us/sample - loss: 0.2929 - val_loss: 0.4114\n",
            "Epoch 29/100\n",
            "7740/7740 [==============================] - 0s 61us/sample - loss: 0.2941 - val_loss: 0.2859\n",
            "Epoch 30/100\n",
            "7740/7740 [==============================] - 0s 61us/sample - loss: 0.2903 - val_loss: 0.4069\n",
            "Epoch 31/100\n",
            "7740/7740 [==============================] - 0s 64us/sample - loss: 0.2864 - val_loss: 0.3028\n",
            "Epoch 32/100\n",
            "7740/7740 [==============================] - 0s 61us/sample - loss: 0.2852 - val_loss: 0.3033\n",
            "Epoch 33/100\n",
            "7740/7740 [==============================] - 0s 61us/sample - loss: 0.2826 - val_loss: 0.3369\n",
            "Epoch 34/100\n",
            "7740/7740 [==============================] - 0s 62us/sample - loss: 0.2843 - val_loss: 0.3093\n",
            "Epoch 35/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.2824 - val_loss: 0.2769\n",
            "Epoch 36/100\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 0.2866 - val_loss: 0.2912\n",
            "Epoch 37/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.2822 - val_loss: 0.3066\n",
            "Epoch 38/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.2816 - val_loss: 0.3115\n",
            "Epoch 39/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.2793 - val_loss: 0.3219\n",
            "Epoch 40/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.2791 - val_loss: 0.2845\n",
            "Epoch 41/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.2816 - val_loss: 0.3036\n",
            "Epoch 42/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.2767 - val_loss: 0.2954\n",
            "Epoch 43/100\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2766 - val_loss: 0.2999\n",
            "Epoch 44/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2729 - val_loss: 0.3001\n",
            "Epoch 45/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.2730 - val_loss: 0.2742\n",
            "Epoch 46/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.2735 - val_loss: 0.2847\n",
            "Epoch 47/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.2725 - val_loss: 0.2859\n",
            "Epoch 48/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.2708 - val_loss: 0.2868\n",
            "Epoch 49/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.2686 - val_loss: 0.3958\n",
            "Epoch 50/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.2711 - val_loss: 0.6530\n",
            "Epoch 51/100\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.2707 - val_loss: 0.3100\n",
            "Epoch 52/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.2705 - val_loss: 0.3110\n",
            "Epoch 53/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.2797 - val_loss: 0.3426\n",
            "Epoch 54/100\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2745 - val_loss: 0.2799\n",
            "Epoch 55/100\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 0.2688 - val_loss: 0.3995\n",
            "3870/3870 [==============================] - 0s 26us/sample - loss: 0.2849\n",
            "7740/7740 [==============================] - 0s 24us/sample - loss: 0.2596\n",
            "[CV]  learning_rate=0.018799462448897047, n_hidden=2, n_neurons=35, total=  25.2s\n",
            "[CV] learning_rate=0.021913134075387083, n_hidden=2, n_neurons=93 ....\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/100\n",
            "7740/7740 [==============================] - 1s 104us/sample - loss: 0.6469 - val_loss: 2.6595\n",
            "Epoch 2/100\n",
            "7740/7740 [==============================] - 1s 84us/sample - loss: 0.4251 - val_loss: 8.9569\n",
            "Epoch 3/100\n",
            "7740/7740 [==============================] - 1s 81us/sample - loss: 0.5000 - val_loss: 7.9103\n",
            "Epoch 4/100\n",
            "7740/7740 [==============================] - 1s 84us/sample - loss: 0.4943 - val_loss: 0.6539\n",
            "Epoch 5/100\n",
            "7740/7740 [==============================] - 1s 83us/sample - loss: 0.3467 - val_loss: 0.6614\n",
            "Epoch 6/100\n",
            "7740/7740 [==============================] - 1s 86us/sample - loss: 0.3467 - val_loss: 0.8742\n",
            "Epoch 7/100\n",
            "7740/7740 [==============================] - 1s 82us/sample - loss: 0.3344 - val_loss: 0.4656\n",
            "Epoch 8/100\n",
            "7740/7740 [==============================] - 1s 83us/sample - loss: 0.3192 - val_loss: 0.4215\n",
            "Epoch 9/100\n",
            "7740/7740 [==============================] - 1s 80us/sample - loss: 0.3147 - val_loss: 0.4164\n",
            "Epoch 10/100\n",
            "7740/7740 [==============================] - 1s 81us/sample - loss: 0.3062 - val_loss: 0.3001\n",
            "Epoch 11/100\n",
            "7740/7740 [==============================] - 1s 82us/sample - loss: 0.3024 - val_loss: 0.3050\n",
            "Epoch 12/100\n",
            "7740/7740 [==============================] - 1s 82us/sample - loss: 0.2964 - val_loss: 0.3022\n",
            "Epoch 13/100\n",
            "7740/7740 [==============================] - 1s 80us/sample - loss: 0.2892 - val_loss: 0.2751\n",
            "Epoch 14/100\n",
            "7740/7740 [==============================] - 1s 80us/sample - loss: 0.2909 - val_loss: 0.3121\n",
            "Epoch 15/100\n",
            "7740/7740 [==============================] - 1s 80us/sample - loss: 0.2872 - val_loss: 0.2925\n",
            "Epoch 16/100\n",
            "7740/7740 [==============================] - 1s 77us/sample - loss: 0.2877 - val_loss: 0.3592\n",
            "Epoch 17/100\n",
            "7740/7740 [==============================] - 1s 81us/sample - loss: 0.2831 - val_loss: 0.2814\n",
            "Epoch 18/100\n",
            "7740/7740 [==============================] - 1s 79us/sample - loss: 0.2780 - val_loss: 0.2925\n",
            "Epoch 19/100\n",
            "7740/7740 [==============================] - 1s 84us/sample - loss: 0.2757 - val_loss: 0.2776\n",
            "Epoch 20/100\n",
            "7740/7740 [==============================] - 1s 82us/sample - loss: 0.2687 - val_loss: 0.3142\n",
            "Epoch 21/100\n",
            "7740/7740 [==============================] - 1s 87us/sample - loss: 0.2703 - val_loss: 0.2798\n",
            "Epoch 22/100\n",
            "7740/7740 [==============================] - 1s 81us/sample - loss: 0.2703 - val_loss: 0.3033\n",
            "Epoch 23/100\n",
            "7740/7740 [==============================] - 1s 83us/sample - loss: 0.2649 - val_loss: 0.3242\n",
            "3870/3870 [==============================] - 0s 33us/sample - loss: 0.3090\n",
            "7740/7740 [==============================] - 0s 31us/sample - loss: 0.2606\n",
            "[CV]  learning_rate=0.021913134075387083, n_hidden=2, n_neurons=93, total=  15.4s\n",
            "[CV] learning_rate=0.021913134075387083, n_hidden=2, n_neurons=93 ....\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/100\n",
            "7740/7740 [==============================] - 1s 109us/sample - loss: 0.6072 - val_loss: 0.5874\n",
            "Epoch 2/100\n",
            "7740/7740 [==============================] - 1s 89us/sample - loss: 0.3900 - val_loss: 0.7366\n",
            "Epoch 3/100\n",
            "7740/7740 [==============================] - 1s 85us/sample - loss: 0.3663 - val_loss: 0.5702\n",
            "Epoch 4/100\n",
            "7740/7740 [==============================] - 1s 82us/sample - loss: 0.3575 - val_loss: 0.3858\n",
            "Epoch 5/100\n",
            "7740/7740 [==============================] - 1s 87us/sample - loss: 0.3473 - val_loss: 0.6435\n",
            "Epoch 6/100\n",
            "7740/7740 [==============================] - 1s 84us/sample - loss: 0.3394 - val_loss: 0.3179\n",
            "Epoch 7/100\n",
            "7740/7740 [==============================] - 1s 82us/sample - loss: 0.3298 - val_loss: 0.7137\n",
            "Epoch 8/100\n",
            "7740/7740 [==============================] - 1s 85us/sample - loss: 0.3274 - val_loss: 0.4322\n",
            "Epoch 9/100\n",
            "7740/7740 [==============================] - 1s 82us/sample - loss: 0.3185 - val_loss: 0.3551\n",
            "Epoch 10/100\n",
            "7740/7740 [==============================] - 1s 81us/sample - loss: 0.3181 - val_loss: 0.3366\n",
            "Epoch 11/100\n",
            "7740/7740 [==============================] - 1s 88us/sample - loss: 0.3121 - val_loss: 0.6015\n",
            "Epoch 12/100\n",
            "7740/7740 [==============================] - 1s 84us/sample - loss: 0.3087 - val_loss: 0.2940\n",
            "Epoch 13/100\n",
            "7740/7740 [==============================] - 1s 87us/sample - loss: 0.3022 - val_loss: 0.3414\n",
            "Epoch 14/100\n",
            "7740/7740 [==============================] - 1s 84us/sample - loss: 0.2987 - val_loss: 0.3174\n",
            "Epoch 15/100\n",
            "7740/7740 [==============================] - 1s 81us/sample - loss: 0.2958 - val_loss: 0.2893\n",
            "Epoch 16/100\n",
            "7740/7740 [==============================] - 1s 82us/sample - loss: 0.2958 - val_loss: 0.2897\n",
            "Epoch 17/100\n",
            "7740/7740 [==============================] - 1s 84us/sample - loss: 0.2912 - val_loss: 0.7236\n",
            "Epoch 18/100\n",
            "7740/7740 [==============================] - 1s 80us/sample - loss: 0.2853 - val_loss: 0.9619\n",
            "Epoch 19/100\n",
            "7740/7740 [==============================] - 1s 78us/sample - loss: 0.2913 - val_loss: 1.1779\n",
            "Epoch 20/100\n",
            "7740/7740 [==============================] - 1s 85us/sample - loss: 0.2838 - val_loss: 0.5353\n",
            "Epoch 21/100\n",
            "7740/7740 [==============================] - 1s 80us/sample - loss: 0.2853 - val_loss: 0.8009\n",
            "Epoch 22/100\n",
            "7740/7740 [==============================] - 1s 80us/sample - loss: 0.2818 - val_loss: 0.5778\n",
            "Epoch 23/100\n",
            "7740/7740 [==============================] - 1s 79us/sample - loss: 0.2774 - val_loss: 0.3491\n",
            "Epoch 24/100\n",
            "7740/7740 [==============================] - 1s 81us/sample - loss: 0.2785 - val_loss: 0.4377\n",
            "Epoch 25/100\n",
            "7740/7740 [==============================] - 1s 79us/sample - loss: 0.2752 - val_loss: 0.4121\n",
            "3870/3870 [==============================] - 0s 33us/sample - loss: 0.3179\n",
            "7740/7740 [==============================] - 0s 30us/sample - loss: 0.2718\n",
            "[CV]  learning_rate=0.021913134075387083, n_hidden=2, n_neurons=93, total=  17.2s\n",
            "[CV] learning_rate=0.021913134075387083, n_hidden=2, n_neurons=93 ....\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "Epoch 1/100\n",
            "7740/7740 [==============================] - 1s 107us/sample - loss: 0.9672 - val_loss: 2.7182\n",
            "Epoch 2/100\n",
            "7740/7740 [==============================] - 1s 83us/sample - loss: 0.7130 - val_loss: 0.5650\n",
            "Epoch 3/100\n",
            "7740/7740 [==============================] - 1s 88us/sample - loss: 0.4735 - val_loss: 24.6039\n",
            "Epoch 4/100\n",
            "7740/7740 [==============================] - 1s 87us/sample - loss: 0.4430 - val_loss: 11.1749\n",
            "Epoch 5/100\n",
            "7740/7740 [==============================] - 1s 80us/sample - loss: nan - val_loss: nan\n",
            "Epoch 6/100\n",
            "7740/7740 [==============================] - 1s 83us/sample - loss: nan - val_loss: nan\n",
            "Epoch 7/100\n",
            "7740/7740 [==============================] - 1s 85us/sample - loss: nan - val_loss: nan\n",
            "Epoch 8/100\n",
            "7740/7740 [==============================] - 1s 84us/sample - loss: nan - val_loss: nan\n",
            "Epoch 9/100\n",
            "7740/7740 [==============================] - 1s 82us/sample - loss: nan - val_loss: nan\n",
            "Epoch 10/100\n",
            "7740/7740 [==============================] - 1s 89us/sample - loss: nan - val_loss: nan\n",
            "Epoch 11/100\n",
            "7740/7740 [==============================] - 1s 81us/sample - loss: nan - val_loss: nan\n",
            "Epoch 12/100\n",
            "7740/7740 [==============================] - 1s 81us/sample - loss: nan - val_loss: nan\n",
            "3870/3870 [==============================] - 0s 33us/sample - loss: nan\n",
            "7740/7740 [==============================] - 0s 31us/sample - loss: nan\n",
            "[CV]  learning_rate=0.021913134075387083, n_hidden=2, n_neurons=93, total=   8.6s\n",
            "Train on 11610 samples, validate on 3870 samples\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed: 11.2min finished\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
            "  DeprecationWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "11610/11610 [==============================] - 1s 65us/sample - loss: 0.5753 - val_loss: 2.2489\n",
            "Epoch 2/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.4066 - val_loss: 3.5797\n",
            "Epoch 3/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.3965 - val_loss: 0.4617\n",
            "Epoch 4/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3628 - val_loss: 0.3249\n",
            "Epoch 5/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.3472 - val_loss: 0.3803\n",
            "Epoch 6/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.3426 - val_loss: 0.3176\n",
            "Epoch 7/100\n",
            "11610/11610 [==============================] - 1s 50us/sample - loss: 0.3404 - val_loss: 0.3594\n",
            "Epoch 8/100\n",
            "11610/11610 [==============================] - 1s 50us/sample - loss: 0.3306 - val_loss: 0.3028\n",
            "Epoch 9/100\n",
            "11610/11610 [==============================] - 1s 49us/sample - loss: 0.3247 - val_loss: 0.3712\n",
            "Epoch 10/100\n",
            "11610/11610 [==============================] - 1s 49us/sample - loss: 0.3197 - val_loss: 0.3124\n",
            "Epoch 11/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.3134 - val_loss: 0.3167\n",
            "Epoch 12/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.3101 - val_loss: 0.3139\n",
            "Epoch 13/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.3085 - val_loss: 0.2912\n",
            "Epoch 14/100\n",
            "11610/11610 [==============================] - 1s 49us/sample - loss: 0.3052 - val_loss: 0.3382\n",
            "Epoch 15/100\n",
            "11610/11610 [==============================] - 1s 49us/sample - loss: 0.3042 - val_loss: 0.3108\n",
            "Epoch 16/100\n",
            "11610/11610 [==============================] - 1s 50us/sample - loss: 0.2984 - val_loss: 0.2882\n",
            "Epoch 17/100\n",
            "11610/11610 [==============================] - 1s 50us/sample - loss: 0.2983 - val_loss: 0.2931\n",
            "Epoch 18/100\n",
            "11610/11610 [==============================] - 1s 50us/sample - loss: 0.2963 - val_loss: 0.2874\n",
            "Epoch 19/100\n",
            "11610/11610 [==============================] - 1s 49us/sample - loss: 0.2936 - val_loss: 0.2815\n",
            "Epoch 20/100\n",
            "11610/11610 [==============================] - 1s 49us/sample - loss: 0.2922 - val_loss: 0.2799\n",
            "Epoch 21/100\n",
            "11610/11610 [==============================] - 1s 49us/sample - loss: 0.2882 - val_loss: 0.2902\n",
            "Epoch 22/100\n",
            "11610/11610 [==============================] - 1s 50us/sample - loss: 0.2876 - val_loss: 0.3331\n",
            "Epoch 23/100\n",
            "11610/11610 [==============================] - 1s 51us/sample - loss: 0.2865 - val_loss: 0.2740\n",
            "Epoch 24/100\n",
            "11610/11610 [==============================] - 1s 50us/sample - loss: 0.2857 - val_loss: 0.2707\n",
            "Epoch 25/100\n",
            "11610/11610 [==============================] - 1s 50us/sample - loss: 0.2837 - val_loss: 0.2946\n",
            "Epoch 26/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.2841 - val_loss: 0.2900\n",
            "Epoch 27/100\n",
            "11610/11610 [==============================] - 1s 50us/sample - loss: 0.2815 - val_loss: 0.3962\n",
            "Epoch 28/100\n",
            "11610/11610 [==============================] - 1s 49us/sample - loss: 0.2827 - val_loss: 0.2821\n",
            "Epoch 29/100\n",
            "11610/11610 [==============================] - 1s 50us/sample - loss: 0.2807 - val_loss: 0.3251\n",
            "Epoch 30/100\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.2767 - val_loss: 0.3321\n",
            "Epoch 31/100\n",
            "11610/11610 [==============================] - 1s 50us/sample - loss: 0.2768 - val_loss: 0.2909\n",
            "Epoch 32/100\n",
            "11610/11610 [==============================] - 1s 49us/sample - loss: 0.2754 - val_loss: 0.2872\n",
            "Epoch 33/100\n",
            "11610/11610 [==============================] - 1s 49us/sample - loss: 0.2719 - val_loss: 0.2930\n",
            "Epoch 34/100\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.2734 - val_loss: 0.2719\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomizedSearchCV(cv=3, error_score='raise-deprecating',\n",
              "          estimator=<tensorflow.python.keras.wrappers.scikit_learn.KerasRegressor object at 0x7f0f6a020198>,\n",
              "          fit_params=None, iid='warn', n_iter=10, n_jobs=None,\n",
              "          param_distributions={'n_hidden': [0, 1, 2, 3], 'n_neurons': array([ 1,  2, ..., 98, 99]), 'learning_rate': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f0f73dea898>},\n",
              "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
              "          return_train_score='warn', scoring=None, verbose=2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "metadata": {
        "id": "9c4d-Mr-RPVQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "24041d55-7e17-46b0-8986-7605173b8eed"
      },
      "cell_type": "code",
      "source": [
        "rnd_search_cv.best_params_"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'learning_rate': 0.018799462448897047, 'n_hidden': 2, 'n_neurons': 35}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "metadata": {
        "id": "d7K37LWWRPVR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "6264913a-f21d-45ec-a58e-9a485152719f"
      },
      "cell_type": "code",
      "source": [
        "rnd_search_cv.best_score_"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.30800893278495695"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "metadata": {
        "id": "RLUYrjIHRPVR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "adb36d6c-1915-465f-b240-9a61c83a8601"
      },
      "cell_type": "code",
      "source": [
        "rnd_search_cv.best_estimator_"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.wrappers.scikit_learn.KerasRegressor at 0x7f0f742224e0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "metadata": {
        "id": "wccJZgORRPVU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 6.5)\n",
        "Evaluate the best model found on the test set. You can either use the best estimator's `score()` method, or get its underlying Keras model *via* its `model` attribute, and call this model's `evaluate()` method. Note that the estimator returns the negative mean square error (it's a score, not a loss, so higher is better)."
      ]
    },
    {
      "metadata": {
        "id": "CCfIIQYsRPVU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "b67ed8bb-cf04-478a-f2ee-35004265fce5"
      },
      "cell_type": "code",
      "source": [
        "rnd_search_cv.score(X_test_scaled, y_test)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5160/5160 [==============================] - 0s 29us/sample - loss: 0.2830\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.28301668686922204"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "metadata": {
        "id": "u4bsVUNFRPVV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "8b106aab-1ae8-41b1-a1a7-325b1c7255fd"
      },
      "cell_type": "code",
      "source": [
        "model = rnd_search_cv.best_estimator_.model\n",
        "model.evaluate(X_test_scaled, y_test)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5160/5160 [==============================] - 0s 28us/sample - loss: 0.2830\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.28301668686922204"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "metadata": {
        "id": "AsmW8IYARPVV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 6.6)\n",
        "Finally, save the best Keras model found. **Tip**: it is available via the best estimator's `model` attribute, and just need to call its `save()` method."
      ]
    },
    {
      "metadata": {
        "id": "AYlt1P3LRPVV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.save(\"my_fine_tuned_housing_model.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "P9e9wQIfRPVW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![Exercise](https://c1.staticflickr.com/9/8101/8553474140_c50cf08708_b.jpg)"
      ]
    },
    {
      "metadata": {
        "id": "c4kqwt-JRPVW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Exercise 7 – The functional API"
      ]
    },
    {
      "metadata": {
        "id": "sZvGXFcsRPVW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Not all neural network models are simply sequential. Some may have complex topologies. Some may have multiple inputs and/or multiple outputs. For example, a Wide & Deep neural network (see [paper](https://ai.google/research/pubs/pub45413)) connects all or part of the inputs directly to the output layer, as shown on the following diagram:"
      ]
    },
    {
      "metadata": {
        "id": "V_S9BInRRPVW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://github.com/ageron/tf2_course/blob/master/images/wide_and_deep_net.png?raw=1\" title=\"Wide and deep net\" width=300 />"
      ]
    },
    {
      "metadata": {
        "id": "ILFVy2lSRPVW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 7.1)\n",
        "Use Keras' functional API to implement a Wide & Deep network to tackle the California housing problem.\n",
        "\n",
        "**Tips**:\n",
        "* You need to create a `keras.layers.Input` layer to represent the inputs. Don't forget to specify the input `shape`.\n",
        "* Create the `Dense` layers, and connect them by using them like functions. For example, `hidden1 = keras.layers.Dense(30, activation=\"relu\")(input)` and `hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)`\n",
        "* Use the `keras.layers.concatenate()` function to concatenate the input layer and the second hidden layer's output.\n",
        "* Create a `keras.models.Model` and specify its `inputs` and `outputs` (e.g., `inputs=[input]`).\n",
        "* Then use this model just like a `Sequential` model: you need to compile it, display its summary, train it, evaluate it and use it to make predictions."
      ]
    },
    {
      "metadata": {
        "id": "hUoA2ez1RPVW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9a2GqzfKRPVX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dKDTLCd2RPVX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pZChwbg_RPVY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 7.2)\n",
        "After the Sequential API and the Functional API, let's try the Subclassing API:\n",
        "* Create a subclass of the `keras.models.Model` class.\n",
        "* Create all the layers you need in the constructor (e.g., `self.hidden1 = keras.layers.Dense(...)`).\n",
        "* Use the layers to process the `input` in the `call()` method, and return the output.\n",
        "* Note that you do not need to create a `keras.layers.Input` in this case.\n",
        "* Also note that `self.output` is used by Keras, so you should use another name for the output layer (e.g., `self.output_layer`).\n",
        "\n",
        "**When should you use the Subclassing API?**\n",
        "* Both the Sequential API and the Functional API are declarative: you first declare the list of layers you need and how they are connected, and only then can you feed your model with actual data. The models that these APIs build are just static graphs of layers. This has many advantages (easy inspection, debugging, saving, loading, sharing, etc.), and they cover the vast majority of use cases, but if you need to build a very dynamic model (e.g., with loops or conditional branching), or if you want to experiment with new ideas using an imperative programming style, then the Subclassing API is for you. You can pretty much do any computation you want in the `call()` method, possibly with loops and conditions, using Keras layers of even low-level TensorFlow operations.\n",
        "* However, this extra flexibility comes at the cost of less transparency. Since the model is defined within the `call()` method, Keras cannot fully inspect it. All it sees is the list of model attributes (which include the layers you define in the constructor), so when you display the model summary you just see a list of unconnected layers. Consequently, you cannot save or load the model without writing extra code. So this API is best used only when you really need the extra flexibility."
      ]
    },
    {
      "metadata": {
        "id": "TWNE3avjRPVZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MyModel(keras.models.Model):\n",
        "    def __init__(self):\n",
        "        super(MyModel, self).__init__()\n",
        "        # create layers here\n",
        "\n",
        "    def call(self, input):\n",
        "        # write any code here, using layers or even low-level TF code\n",
        "        return output\n",
        "\n",
        "model = MyModel()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DIcNUEoZRPVZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0iZVEzjXRPVa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MthBD4bnRPVa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uUXS2YMcRPVa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 7.3)\n",
        "Now suppose you want to send only features 0 to 4 directly to the output, and only features 2 to 7 through the hidden layers, as shown on the following diagram. Use the functional API to build, train and evaluate this model.\n",
        "\n",
        "**Tips**:\n",
        "* You need to create two `keras.layers.Input` (`input_A` and `input_B`)\n",
        "* Build the model using the functional API, as above, but when you build the `keras.models.Model`, remember to set `inputs=[input_A, input_B]`\n",
        "* When calling `fit()`, `evaluate()` and  `predict()`, instead of passing `X_train_scaled`, pass `(X_train_scaled_A, X_train_scaled_B)` (two NumPy arrays containing only the appropriate features copied from `X_train_scaled`)."
      ]
    },
    {
      "metadata": {
        "id": "bwVHgk4hRPVb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://github.com/ageron/tf2_course/blob/master/images/multiple_inputs.png?raw=1\" title=\"Multiple inputs\" width=300 />"
      ]
    },
    {
      "metadata": {
        "id": "yB9YaiQKRPVb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1cfwyQYSRPVb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WmdGU_nbRPVb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NTdf8BRwRPVc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 7.4)\n",
        "Build the multi-input and multi-output neural net represented in the following diagram.\n",
        "\n",
        "<img src=\"https://github.com/ageron/tf2_course/blob/master/images/multiple_inputs_and_outputs.png?raw=1\" title=\"Multiple inputs and outputs\" width=400 />\n",
        "\n",
        "**Why?**\n",
        "\n",
        "There are many use cases in which having multiple outputs can be useful:\n",
        "* Your task may require multiple outputs, for example, you may want to locate and classify the main object in a picture. This is both a regression task (finding the coordinates of the object's center, as well as its width and height) and a classification task.\n",
        "* Similarly, you may have multiple independent tasks to perform based on the same data. Sure, you could train one neural network per task, but in many cases you will get better results on all tasks by training a single neural network with one output per task. This is because the neural network can learn features in the data that are useful across tasks.\n",
        "* Another use case is as a regularization technique (i.e., a training constraint whose objective is to reduce overfitting and thus improve the model's ability to generalize). For example, you may want to add some auxiliary outputs in a neural network architecture (as shown in the diagram) to ensure that that the underlying part of the network learns something useful on its own, without relying on the rest of the network.\n",
        "\n",
        "**Tips**:\n",
        "* Building the model is pretty straightforward using the functional API. Just make sure you specify both outputs when creating the `keras.models.Model`, for example `outputs=[output, aux_output]`.\n",
        "* Each output has its own loss function. In this scenario, they will be identical, so you can either specify `loss=\"mse\"` (this loss will apply to both outputs) or `loss=[\"mse\", \"mse\"]`, which does the same thing.\n",
        "* The final loss used to train the whole network is just a weighted sum of all loss functions. In this scenario, you want most to give a much smaller weight to the auxiliary output, so when compiling the model, you must specify `loss_weights=[0.9, 0.1]`.\n",
        "* When calling `fit()` or `evaluate()`, you need to pass the labels for all outputs. In this scenario the labels will be the same for the main output and for the auxiliary output, so make sure to pass `(y_train, y_train)` instead of `y_train`.\n",
        "* The `predict()` method will return both the main output and the auxiliary output."
      ]
    },
    {
      "metadata": {
        "id": "7Dq7UdxlRPVc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kHDSlR0SRPVc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aYXnhDdvRPVd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E9nG3_bARPVe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![Exercise solution](https://camo.githubusercontent.com/250388fde3fac9135ead9471733ee28e049f7a37/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f302f30362f46696c6f735f736567756e646f5f6c6f676f5f253238666c69707065642532392e6a7067)"
      ]
    },
    {
      "metadata": {
        "id": "gHHA0Z1SRPVe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Exercise 7 – Solution"
      ]
    },
    {
      "metadata": {
        "id": "fMFQ195BRPVe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 7.1)\n",
        "Use Keras' functional API to implement a Wide & Deep network to tackle the California housing problem."
      ]
    },
    {
      "metadata": {
        "id": "6ucumLSYRPVe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "input = keras.layers.Input(shape=X_train.shape[1:])\n",
        "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input)\n",
        "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
        "concat = keras.layers.concatenate([input, hidden2])\n",
        "output = keras.layers.Dense(1)(concat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yMHVXShSRPVf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = keras.models.Model(inputs=[input], outputs=[output])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bj0nYn-dRPVf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile(loss=\"mean_squared_error\", optimizer=\"sgd\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jbJuPB93RPVg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "5102f5f8-205b-4d84-abdc-6f024c8678b8"
      },
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 8)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_159 (Dense)               (None, 30)           270         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_160 (Dense)               (None, 30)           930         dense_159[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 38)           0           input_1[0][0]                    \n",
            "                                                                 dense_160[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_161 (Dense)               (None, 1)            39          concatenate[0][0]                \n",
            "==================================================================================================\n",
            "Total params: 1,239\n",
            "Trainable params: 1,239\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tpQmSUN0RPVg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "outputId": "10d7b91b-6698-4ad6-bb04-8a81ccbe10f4"
      },
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train_scaled, y_train, epochs=10,\n",
        "                    validation_data=(X_valid_scaled, y_valid))"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 11610 samples, validate on 3870 samples\n",
            "Epoch 1/10\n",
            "11610/11610 [==============================] - 1s 61us/sample - loss: 1.7020 - val_loss: 1.2252\n",
            "Epoch 2/10\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.6714 - val_loss: 0.6308\n",
            "Epoch 3/10\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.6209 - val_loss: 0.5736\n",
            "Epoch 4/10\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.5896 - val_loss: 0.5538\n",
            "Epoch 5/10\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.5658 - val_loss: 0.5257\n",
            "Epoch 6/10\n",
            "11610/11610 [==============================] - 1s 45us/sample - loss: 0.5462 - val_loss: 0.5314\n",
            "Epoch 7/10\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.5281 - val_loss: 0.5033\n",
            "Epoch 8/10\n",
            "11610/11610 [==============================] - 1s 45us/sample - loss: 0.5139 - val_loss: 0.4958\n",
            "Epoch 9/10\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.5004 - val_loss: 0.5170\n",
            "Epoch 10/10\n",
            "11610/11610 [==============================] - 1s 45us/sample - loss: 0.4890 - val_loss: 0.5095\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "k5uIya_9RPVg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "322b6af2-b5f0-4623-c94a-fba882d0e1c1"
      },
      "cell_type": "code",
      "source": [
        "model.evaluate(X_test_scaled, y_test)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5160/5160 [==============================] - 0s 23us/sample - loss: 0.4795\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4794762220493583"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "metadata": {
        "id": "4zrY5HK-RPVh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "98c1001c-9e08-40a2-d1ec-5d8617432518"
      },
      "cell_type": "code",
      "source": [
        "model.predict(X_test_scaled)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.4621063],\n",
              "       [1.393166 ],\n",
              "       [3.097611 ],\n",
              "       ...,\n",
              "       [1.2330668],\n",
              "       [2.6873953],\n",
              "       [3.6915073]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "metadata": {
        "id": "MNI_cZCiRPVh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 7.2)\n",
        "After the Sequential API and the Functional API, let's try the Subclassing API:\n",
        "* Create a subclass of the `keras.models.Model` class.\n",
        "* Create all the layers you need in the constructor (e.g., `self.hidden1 = keras.layers.Dense(...)`).\n",
        "* Use the layers to process the `input` in the `call()` method, and return the output.\n",
        "* Note that you do not need to create a `keras.layers.Input` in this case.\n",
        "* Also note that `self.output` is used by Keras, so you should use another name for the output layer (e.g., `self.output_layer`)."
      ]
    },
    {
      "metadata": {
        "id": "6Pw89ns-RPVh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MyModel(keras.models.Model):\n",
        "    def __init__(self):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.hidden1 = keras.layers.Dense(30, activation=\"relu\")\n",
        "        self.hidden2 = keras.layers.Dense(30, activation=\"relu\")\n",
        "        self.output_ = keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, input):\n",
        "        hidden1 = self.hidden1(input)\n",
        "        hidden2 = self.hidden2(hidden1)\n",
        "        concat = keras.layers.concatenate([input, hidden2])\n",
        "        output = self.output_(concat)\n",
        "        return output\n",
        "\n",
        "model = MyModel()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lpOkVtb8RPVi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile(loss=\"mse\", optimizer=\"sgd\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6TaMebr3RPVi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "outputId": "4b2f3f8b-570d-4c50-aaa4-4544ffff2aa3"
      },
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train_scaled, y_train, epochs=10,\n",
        "                    validation_data=(X_valid_scaled, y_valid))"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 11610 samples, validate on 3870 samples\n",
            "Epoch 1/10\n",
            "11610/11610 [==============================] - 1s 60us/sample - loss: 1.6204 - val_loss: 0.8130\n",
            "Epoch 2/10\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.7397 - val_loss: 0.6680\n",
            "Epoch 3/10\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.6684 - val_loss: 0.6123\n",
            "Epoch 4/10\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.6236 - val_loss: 0.5814\n",
            "Epoch 5/10\n",
            "11610/11610 [==============================] - 1s 45us/sample - loss: 0.5874 - val_loss: 0.5531\n",
            "Epoch 6/10\n",
            "11610/11610 [==============================] - 1s 45us/sample - loss: 0.5621 - val_loss: 0.5155\n",
            "Epoch 7/10\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.5392 - val_loss: 0.4974\n",
            "Epoch 8/10\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.5209 - val_loss: 0.4764\n",
            "Epoch 9/10\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.5048 - val_loss: 0.4640\n",
            "Epoch 10/10\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.4908 - val_loss: 0.4648\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "djqzC1t8RPVj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "fbf86fed-8f68-4ef2-9a5c-3463e6c8ecef"
      },
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_162 (Dense)            multiple                  270       \n",
            "_________________________________________________________________\n",
            "dense_163 (Dense)            multiple                  930       \n",
            "_________________________________________________________________\n",
            "dense_164 (Dense)            multiple                  39        \n",
            "=================================================================\n",
            "Total params: 1,239\n",
            "Trainable params: 1,239\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "x6CRjtsLRPVj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "5729f568-283e-4522-de61-d5db19f37297"
      },
      "cell_type": "code",
      "source": [
        "model.evaluate(X_test_scaled, y_test)"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5160/5160 [==============================] - 0s 25us/sample - loss: 0.4786\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4785669109618017"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "metadata": {
        "id": "aW0krivsRPVj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "1657492d-8f8e-47b0-dede-e1f49f7216f6"
      },
      "cell_type": "code",
      "source": [
        "model.predict(X_test_scaled)"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.6162932],\n",
              "       [1.400727 ],\n",
              "       [2.9422164],\n",
              "       ...,\n",
              "       [1.4964975],\n",
              "       [2.4508696],\n",
              "       [3.7177792]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "metadata": {
        "id": "bG44K4tIRPVk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 7.3)\n",
        "Now suppose you want to send only features 0 to 4 directly to the output, and only features 2 to 7 through the hidden layers, as shown on the diagram. Use the functional API to build, train and evaluate this model."
      ]
    },
    {
      "metadata": {
        "id": "kDQuTRF4RPVk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "input_A = keras.layers.Input(shape=[5])\n",
        "input_B = keras.layers.Input(shape=[6])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "myaasXV9RPVk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_B)\n",
        "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
        "concat = keras.layers.concatenate([input_A, hidden2])\n",
        "output = keras.layers.Dense(1)(concat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LVyxD7c6RPVk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = keras.models.Model(inputs=[input_A, input_B], outputs=[output])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MeYx3KvTRPVl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile(loss=\"mean_squared_error\", optimizer=\"sgd\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tgCc0CQ2RPVl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "outputId": "89d654c3-240c-4dad-a431-227328e8623d"
      },
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            [(None, 6)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_165 (Dense)               (None, 30)           210         input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 5)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_166 (Dense)               (None, 30)           930         dense_165[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 35)           0           input_2[0][0]                    \n",
            "                                                                 dense_166[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_167 (Dense)               (None, 1)            36          concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 1,176\n",
            "Trainable params: 1,176\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bj_xVOJ3RPVm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train_scaled_A = X_train_scaled[:, :5]\n",
        "X_train_scaled_B = X_train_scaled[:, 2:]\n",
        "X_valid_scaled_A = X_valid_scaled[:, :5]\n",
        "X_valid_scaled_B = X_valid_scaled[:, 2:]\n",
        "X_test_scaled_A = X_test_scaled[:, :5]\n",
        "X_test_scaled_B = X_test_scaled[:, 2:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ptp5IsGKRPVo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "outputId": "5cc817f0-2ce8-4e31-b99b-5e055f3655b3"
      },
      "cell_type": "code",
      "source": [
        "history = model.fit([X_train_scaled_A, X_train_scaled_B], y_train, epochs=10,\n",
        "                    validation_data=([X_valid_scaled_A, X_valid_scaled_B], y_valid))"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 11610 samples, validate on 3870 samples\n",
            "Epoch 1/10\n",
            "11610/11610 [==============================] - 1s 69us/sample - loss: 2.0103 - val_loss: 11.4812\n",
            "Epoch 2/10\n",
            "11610/11610 [==============================] - 1s 52us/sample - loss: 0.8658 - val_loss: 2.4738\n",
            "Epoch 3/10\n",
            "11610/11610 [==============================] - 1s 51us/sample - loss: 0.7285 - val_loss: 0.7231\n",
            "Epoch 4/10\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.6551 - val_loss: 0.6059\n",
            "Epoch 5/10\n",
            "11610/11610 [==============================] - 1s 49us/sample - loss: 0.6102 - val_loss: 0.6245\n",
            "Epoch 6/10\n",
            "11610/11610 [==============================] - 1s 49us/sample - loss: 0.5770 - val_loss: 0.5621\n",
            "Epoch 7/10\n",
            "11610/11610 [==============================] - 1s 49us/sample - loss: 0.5501 - val_loss: 0.5486\n",
            "Epoch 8/10\n",
            "11610/11610 [==============================] - 1s 51us/sample - loss: 0.5292 - val_loss: 0.4910\n",
            "Epoch 9/10\n",
            "11610/11610 [==============================] - 1s 48us/sample - loss: 0.5117 - val_loss: 0.4889\n",
            "Epoch 10/10\n",
            "11610/11610 [==============================] - 1s 49us/sample - loss: 0.4985 - val_loss: 0.4552\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "X226h84dRPVq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "ef517171-ecea-4850-d1d6-4d9fc6803f12"
      },
      "cell_type": "code",
      "source": [
        "model.evaluate([X_test_scaled_A, X_test_scaled_B], y_test)"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5160/5160 [==============================] - 0s 28us/sample - loss: 0.4844\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4844350025635357"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "metadata": {
        "id": "wFviVXMQRPVq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "95df0b36-2b7c-4412-a61d-4400f6089741"
      },
      "cell_type": "code",
      "source": [
        "model.predict([X_test_scaled_A, X_test_scaled_B])"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.31621903],\n",
              "       [1.6015828 ],\n",
              "       [2.6473088 ],\n",
              "       ...,\n",
              "       [1.531363  ],\n",
              "       [2.5520012 ],\n",
              "       [3.5317016 ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "metadata": {
        "id": "a_Ae7zSYRPVr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 7.4)\n",
        "Build the multi-input and multi-output neural net represented in the diagram."
      ]
    },
    {
      "metadata": {
        "id": "84BBTxF8RPVr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "input_A = keras.layers.Input(shape=X_train_scaled_A.shape[1:])\n",
        "input_B = keras.layers.Input(shape=X_train_scaled_B.shape[1:])\n",
        "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_B)\n",
        "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
        "concat = keras.layers.concatenate([input_A, hidden2])\n",
        "output = keras.layers.Dense(1)(concat)\n",
        "aux_output = keras.layers.Dense(1)(hidden2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Pm07brixRPVr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = keras.models.Model(inputs=[input_A, input_B],\n",
        "                           outputs=[output, aux_output])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z9CtxpeYRPVr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile(loss=\"mean_squared_error\", loss_weights=[0.9, 0.1],\n",
        "              optimizer=\"sgd\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yUYgCgA8RPVs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "cc9aa3e7-98c9-4a5e-fadb-cf4584f5acd3"
      },
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_5 (InputLayer)            [(None, 6)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_168 (Dense)               (None, 30)           210         input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_4 (InputLayer)            [(None, 5)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_169 (Dense)               (None, 30)           930         dense_168[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 35)           0           input_4[0][0]                    \n",
            "                                                                 dense_169[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_170 (Dense)               (None, 1)            36          concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_171 (Dense)               (None, 1)            31          dense_169[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 1,207\n",
            "Trainable params: 1,207\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1pUh2SpMRPVs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "outputId": "05466903-386f-4764-8e07-64fe5b9313fb"
      },
      "cell_type": "code",
      "source": [
        "history = model.fit([X_train_scaled_A, X_train_scaled_B], [y_train, y_train], epochs=10,\n",
        "                    validation_data=([X_valid_scaled_A, X_valid_scaled_B], [y_valid, y_valid]))"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 11610 samples, validate on 3870 samples\n",
            "Epoch 1/10\n",
            "11610/11610 [==============================] - 1s 84us/sample - loss: 2.8102 - dense_170_loss: 2.5132 - dense_171_loss: 5.4836 - val_loss: 1.7919 - val_dense_170_loss: 1.4671 - val_dense_171_loss: 4.7142\n",
            "Epoch 2/10\n",
            "11610/11610 [==============================] - 1s 69us/sample - loss: 1.1719 - dense_170_loss: 0.9336 - dense_171_loss: 3.3172 - val_loss: 1.0355 - val_dense_170_loss: 0.8526 - val_dense_171_loss: 2.6813\n",
            "Epoch 3/10\n",
            "11610/11610 [==============================] - 1s 59us/sample - loss: 0.8751 - dense_170_loss: 0.7239 - dense_171_loss: 2.2361 - val_loss: 0.7753 - val_dense_170_loss: 0.6500 - val_dense_171_loss: 1.9032\n",
            "Epoch 4/10\n",
            "11610/11610 [==============================] - 1s 56us/sample - loss: 0.7591 - dense_170_loss: 0.6480 - dense_171_loss: 1.7591 - val_loss: 0.6914 - val_dense_170_loss: 0.5927 - val_dense_171_loss: 1.5801\n",
            "Epoch 5/10\n",
            "11610/11610 [==============================] - 1s 62us/sample - loss: 0.6987 - dense_170_loss: 0.6054 - dense_171_loss: 1.5377 - val_loss: 0.6507 - val_dense_170_loss: 0.5628 - val_dense_171_loss: 1.4418\n",
            "Epoch 6/10\n",
            "11610/11610 [==============================] - 1s 64us/sample - loss: 0.6591 - dense_170_loss: 0.5741 - dense_171_loss: 1.4244 - val_loss: 0.6175 - val_dense_170_loss: 0.5347 - val_dense_171_loss: 1.3626\n",
            "Epoch 7/10\n",
            "11610/11610 [==============================] - 1s 65us/sample - loss: 0.6296 - dense_170_loss: 0.5490 - dense_171_loss: 1.3552 - val_loss: 0.5920 - val_dense_170_loss: 0.5122 - val_dense_171_loss: 1.3105\n",
            "Epoch 8/10\n",
            "11610/11610 [==============================] - 1s 64us/sample - loss: 0.6060 - dense_170_loss: 0.5282 - dense_171_loss: 1.3063 - val_loss: 0.5692 - val_dense_170_loss: 0.4911 - val_dense_171_loss: 1.2721\n",
            "Epoch 9/10\n",
            "11610/11610 [==============================] - 1s 65us/sample - loss: 0.5863 - dense_170_loss: 0.5110 - dense_171_loss: 1.2642 - val_loss: 0.5527 - val_dense_170_loss: 0.4755 - val_dense_171_loss: 1.2480\n",
            "Epoch 10/10\n",
            "11610/11610 [==============================] - 1s 64us/sample - loss: 0.5700 - dense_170_loss: 0.4967 - dense_171_loss: 1.2292 - val_loss: 0.5348 - val_dense_170_loss: 0.4584 - val_dense_171_loss: 1.2232\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QsFiB4p5RPVt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "8025b2c1-d9d5-494a-813f-f8e7d5423459"
      },
      "cell_type": "code",
      "source": [
        "model.evaluate([X_test_scaled_A, X_test_scaled_B], [y_test, y_test])"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5160/5160 [==============================] - 0s 37us/sample - loss: 0.5567 - dense_170_loss: 0.4850 - dense_171_loss: 1.2019\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5567135950391606, 0.48502448, 1.2019154]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "metadata": {
        "id": "NZWk56B9RPVu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_pred, y_pred_aux = model.predict([X_test_scaled_A, X_test_scaled_B])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "13Bf_A41RPVu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "6caff8dd-450b-49e5-a044-f3d2f742881e"
      },
      "cell_type": "code",
      "source": [
        "y_pred"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.45073482],\n",
              "       [1.5798678 ],\n",
              "       [2.9344106 ],\n",
              "       ...,\n",
              "       [1.223527  ],\n",
              "       [2.6384761 ],\n",
              "       [3.49195   ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "YWFo4rJoRPVv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "dc157f42-3821-490a-96d4-cae1b20be0d3"
      },
      "cell_type": "code",
      "source": [
        "y_pred_aux"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.2922733],\n",
              "       [1.845499 ],\n",
              "       [2.4428554],\n",
              "       ...,\n",
              "       [1.6922522],\n",
              "       [1.7551965],\n",
              "       [2.0618   ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "metadata": {
        "id": "t-ngJfrrRPVv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![Exercise](https://c1.staticflickr.com/9/8101/8553474140_c50cf08708_b.jpg)"
      ]
    },
    {
      "metadata": {
        "id": "-Mnbyys4RPVw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Exercise 8 – Deep Nets"
      ]
    },
    {
      "metadata": {
        "id": "rhTEAF2jRPVw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's go back to Fashion MNIST and build deep nets to tackle it. We need to load it, split it and scale it."
      ]
    },
    {
      "metadata": {
        "id": "Blo-KYbVRPVw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\n",
        "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
        "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e69gCSVkRPVx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train.astype(np.float32).reshape(-1, 1)).reshape(-1, 28, 28)\n",
        "X_valid_scaled = scaler.transform(X_valid.astype(np.float32).reshape(-1, 1)).reshape(-1, 28, 28)\n",
        "X_test_scaled = scaler.transform(X_test.astype(np.float32).reshape(-1, 1)).reshape(-1, 28, 28)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0qIsJPchRPVz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 8.1)\n",
        "Build a sequential model with 20 hidden dense layers, with 100 neurons each, using the ReLU activation function, plus the output layer (10 neurons, softmax activation function). Try to train it for 10 epochs on Fashion MNIST and plot the learning curves. Notice that progress is very slow."
      ]
    },
    {
      "metadata": {
        "id": "B_vAhZZ2RPVz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J8M8sj_oRPVz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GDR8HRl4RPVz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RnBYQTinRPV0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 8.2)\n",
        "Update the model to add a `BatchNormalization` layer after every hidden layer. Notice that performance progresses much faster per epoch, although computations are much more intensive. Display the model summary and notice all the non-trainable parameters (the scale $\\gamma$ and offset $\\beta$ parameters)."
      ]
    },
    {
      "metadata": {
        "id": "dn1yyxJsRPV0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zzcekjZbRPV0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CwyzlYgCRPV0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "05BPVXNSRPV1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 8.3)\n",
        "Try moving the BN layers before the hidden layers' activation functions. Does this affect the model's performance?"
      ]
    },
    {
      "metadata": {
        "id": "P7cbh0FtRPV1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vDumW1CvRPV2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ypeM9PoMRPV2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9aXdqod3RPV3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 8.4)\n",
        "Remove all the BN layers, and just use the SELU activation function instead (always use SELU with LeCun Normal weight initialization). Notice that you get better performance than with BN but training is much faster. Isn't it marvelous? :-)"
      ]
    },
    {
      "metadata": {
        "id": "aV4hsJyORPV3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bwhuUD_pRPV3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pJOdKHFORPV3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "axwsQ3dPRPV4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 8.5)\n",
        "Try training for 10 additional epochs, and notice that the model starts overfitting. Try adding a Dropout layer (with a 50% dropout rate) just before the output layer. Does it reduce overfitting? What about the final validation accuracy?\n",
        "\n",
        "**Warning**: you should not use regular Dropout, as it breaks the self-normalizing property of the SELU activation function. Instead, use AlphaDropout, which is designed to work with SELU."
      ]
    },
    {
      "metadata": {
        "id": "dy6CDuALRPV4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "65ICtemsRPV5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hkZT5aeYRPV5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Vgb5Bzo5RPV6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![Exercise solution](https://camo.githubusercontent.com/250388fde3fac9135ead9471733ee28e049f7a37/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f302f30362f46696c6f735f736567756e646f5f6c6f676f5f253238666c69707065642532392e6a7067)"
      ]
    },
    {
      "metadata": {
        "id": "sNU-eAdlRPV6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Exercise 8 – Solution"
      ]
    },
    {
      "metadata": {
        "id": "tu9qE9Q7RPV6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 8.1)\n",
        "Build a sequential model with 20 hidden dense layers, with 100 neurons each, using the ReLU activation function, plus the output layer (10 neurons, softmax activation function). Try to train it for 10 epochs on Fashion MNIST and plot the learning curves. Notice that progress is very slow."
      ]
    },
    {
      "metadata": {
        "id": "sxiyq7Y1RPV6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        },
        "outputId": "d5bacae2-9ec8-4028-bff6-b22a3f583de6"
      },
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
        "for _ in range(20):\n",
        "    model.add(keras.layers.Dense(100, activation=\"relu\"))\n",
        "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\",\n",
        "              metrics=[\"accuracy\"])\n",
        "history = model.fit(X_train_scaled, y_train, epochs=10,\n",
        "                    validation_data=(X_valid_scaled, y_valid))\n",
        "plot_learning_curves(history)"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-110-a26a9a4847f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m               metrics=[\"accuracy\"])\n\u001b[1;32m      8\u001b[0m history = model.fit(X_train_scaled, y_train, epochs=10,\n\u001b[0;32m----> 9\u001b[0;31m                     validation_data=(X_valid_scaled, y_valid))\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mplot_learning_curves\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    805\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m         \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m         shuffle=shuffle)\n\u001b[0m\u001b[1;32m    808\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m     \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2509\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2510\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2511\u001b[0;31m           exception_prefix='input')\n\u001b[0m\u001b[1;32m   2512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2513\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    337\u001b[0m                            \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m                            \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m                            'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m           \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected flatten_4_input to have 3 dimensions, but got array with shape (11610, 8)"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "XIyRv0_zRPV6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 8.2)\n",
        "Update the model to add a `BatchNormalization` layer after every hidden layer. Notice that performance progresses much faster per epoch, although computations are much more intensive. Display the model summary and notice all the non-trainable parameters (the scale $\\gamma$ and offset $\\beta$ parameters)."
      ]
    },
    {
      "metadata": {
        "id": "9wZ7mJgNRPV6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        },
        "outputId": "85c00b48-2af6-434d-d3db-0f6bf90b9cf8"
      },
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
        "for _ in range(20):\n",
        "    model.add(keras.layers.Dense(100, activation=\"relu\"))\n",
        "    model.add(keras.layers.BatchNormalization())\n",
        "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\",\n",
        "              metrics=[\"accuracy\"])\n",
        "history = model.fit(X_train_scaled, y_train, epochs=10,\n",
        "                    validation_data=(X_valid_scaled, y_valid))\n",
        "plot_learning_curves(history)"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-111-33ba80a387d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m               metrics=[\"accuracy\"])\n\u001b[1;32m      9\u001b[0m history = model.fit(X_train_scaled, y_train, epochs=10,\n\u001b[0;32m---> 10\u001b[0;31m                     validation_data=(X_valid_scaled, y_valid))\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mplot_learning_curves\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    805\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m         \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m         shuffle=shuffle)\n\u001b[0m\u001b[1;32m    808\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m     \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2509\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2510\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2511\u001b[0;31m           exception_prefix='input')\n\u001b[0m\u001b[1;32m   2512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2513\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    337\u001b[0m                            \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m                            \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m                            'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m           \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected flatten_5_input to have 3 dimensions, but got array with shape (11610, 8)"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "zGm7KSSeRPV7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1512
        },
        "outputId": "405e3514-8aa8-41a5-f494-6fb0a03421f0"
      },
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_50\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_5 (Flatten)          (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_193 (Dense)            (None, 100)               78500     \n",
            "_________________________________________________________________\n",
            "batch_normalization_v2 (Batc (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "dense_194 (Dense)            (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "batch_normalization_v2_1 (Ba (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "dense_195 (Dense)            (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "batch_normalization_v2_2 (Ba (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "dense_196 (Dense)            (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "batch_normalization_v2_3 (Ba (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "dense_197 (Dense)            (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "batch_normalization_v2_4 (Ba (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "dense_198 (Dense)            (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "batch_normalization_v2_5 (Ba (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "dense_199 (Dense)            (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "batch_normalization_v2_6 (Ba (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "dense_200 (Dense)            (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "batch_normalization_v2_7 (Ba (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "dense_201 (Dense)            (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "batch_normalization_v2_8 (Ba (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "dense_202 (Dense)            (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "batch_normalization_v2_9 (Ba (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "dense_203 (Dense)            (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "batch_normalization_v2_10 (B (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "dense_204 (Dense)            (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "batch_normalization_v2_11 (B (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "dense_205 (Dense)            (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "batch_normalization_v2_12 (B (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "dense_206 (Dense)            (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "batch_normalization_v2_13 (B (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "dense_207 (Dense)            (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "batch_normalization_v2_14 (B (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "dense_208 (Dense)            (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "batch_normalization_v2_15 (B (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "dense_209 (Dense)            (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "batch_normalization_v2_16 (B (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "dense_210 (Dense)            (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "batch_normalization_v2_17 (B (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "dense_211 (Dense)            (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "batch_normalization_v2_18 (B (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "dense_212 (Dense)            (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "batch_normalization_v2_19 (B (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "dense_213 (Dense)            (None, 10)                1010      \n",
            "=================================================================\n",
            "Total params: 279,410\n",
            "Trainable params: 275,410\n",
            "Non-trainable params: 4,000\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8excpNofRPV7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 8.3)\n",
        "Try moving the BN layers before the hidden layers' activation functions. Does this affect the model's performance?"
      ]
    },
    {
      "metadata": {
        "id": "RakvXiIaRPV7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        },
        "outputId": "ea4c0e37-0d31-4fa5-8ecf-986af71abbfd"
      },
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
        "for _ in range(20):\n",
        "    model.add(keras.layers.Dense(100))\n",
        "    model.add(keras.layers.BatchNormalization())\n",
        "    model.add(keras.layers.Activation(\"relu\"))\n",
        "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\",\n",
        "              metrics=[\"accuracy\"])\n",
        "history = model.fit(X_train_scaled, y_train, epochs=10,\n",
        "                    validation_data=(X_valid_scaled, y_valid))\n",
        "plot_learning_curves(history)"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-113-12d3a4207889>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m               metrics=[\"accuracy\"])\n\u001b[1;32m     10\u001b[0m history = model.fit(X_train_scaled, y_train, epochs=10,\n\u001b[0;32m---> 11\u001b[0;31m                     validation_data=(X_valid_scaled, y_valid))\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mplot_learning_curves\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    805\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m         \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m         shuffle=shuffle)\n\u001b[0m\u001b[1;32m    808\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m     \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2509\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2510\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2511\u001b[0;31m           exception_prefix='input')\n\u001b[0m\u001b[1;32m   2512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2513\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    337\u001b[0m                            \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m                            \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m                            'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m           \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected flatten_6_input to have 3 dimensions, but got array with shape (11610, 8)"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "aIVzBXYjRPV8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 8.4)\n",
        "Remove all the BN layers, and just use the SELU activation function instead (always use SELU with LeCun Normal weight initialization). Notice that you get better performance than with BN but training is much faster. Isn't it marvelous? :-)"
      ]
    },
    {
      "metadata": {
        "id": "5_oeh7gvRPV8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        },
        "outputId": "ea817544-8047-4382-a1aa-c843ce288874"
      },
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
        "for _ in range(20):\n",
        "    model.add(keras.layers.Dense(100, activation=\"selu\",\n",
        "                                 kernel_initializer=\"lecun_normal\"))\n",
        "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\",\n",
        "              metrics=[\"accuracy\"])\n",
        "history = model.fit(X_train_scaled, y_train, epochs=10,\n",
        "                    validation_data=(X_valid_scaled, y_valid))\n",
        "plot_learning_curves(history)"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-114-a2181a0a8fc5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m               metrics=[\"accuracy\"])\n\u001b[1;32m      9\u001b[0m history = model.fit(X_train_scaled, y_train, epochs=10,\n\u001b[0;32m---> 10\u001b[0;31m                     validation_data=(X_valid_scaled, y_valid))\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mplot_learning_curves\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    805\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m         \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m         shuffle=shuffle)\n\u001b[0m\u001b[1;32m    808\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m     \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2509\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2510\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2511\u001b[0;31m           exception_prefix='input')\n\u001b[0m\u001b[1;32m   2512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2513\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    337\u001b[0m                            \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m                            \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m                            'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m           \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected flatten_7_input to have 3 dimensions, but got array with shape (11610, 8)"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "5FvV9Un-RPV8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 8.5)\n",
        "Try training for 10 additional epochs, and notice that the model starts overfitting. Try adding a Dropout layer (with a 50% dropout rate) just before the output layer. Does it reduce overfitting? What about the final validation accuracy?"
      ]
    },
    {
      "metadata": {
        "id": "HIaTJxItRPV9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 558
        },
        "outputId": "cac97d8e-d1d6-4538-b30a-f61b7c88623c"
      },
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train_scaled, y_train, epochs=10,\n",
        "                    validation_data=(X_valid_scaled, y_valid))\n",
        "plot_learning_curves(history)"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-115-ff6a6cf5f26d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m history = model.fit(X_train_scaled, y_train, epochs=10,\n\u001b[0;32m----> 2\u001b[0;31m                     validation_data=(X_valid_scaled, y_valid))\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplot_learning_curves\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    805\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m         \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m         shuffle=shuffle)\n\u001b[0m\u001b[1;32m    808\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m     \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2509\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2510\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2511\u001b[0;31m           exception_prefix='input')\n\u001b[0m\u001b[1;32m   2512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2513\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    337\u001b[0m                            \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m                            \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m                            'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m           \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected flatten_7_input to have 3 dimensions, but got array with shape (11610, 8)"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "Ul9SW0pyRPV9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        },
        "outputId": "7701ff2f-f2d1-46d3-ac2c-15186f5ab966"
      },
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
        "for _ in range(20):\n",
        "    model.add(keras.layers.Dense(100, activation=\"selu\",\n",
        "                                 kernel_initializer=\"lecun_normal\"))\n",
        "model.add(keras.layers.AlphaDropout(rate=0.5))\n",
        "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\",\n",
        "              metrics=[\"accuracy\"])\n",
        "history = model.fit(X_train_scaled, y_train, epochs=20,\n",
        "                    validation_data=(X_valid_scaled, y_valid))\n",
        "plot_learning_curves(history)"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-116-d2cc78990886>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m               metrics=[\"accuracy\"])\n\u001b[1;32m     10\u001b[0m history = model.fit(X_train_scaled, y_train, epochs=20,\n\u001b[0;32m---> 11\u001b[0;31m                     validation_data=(X_valid_scaled, y_valid))\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mplot_learning_curves\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    805\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m         \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m         shuffle=shuffle)\n\u001b[0m\u001b[1;32m    808\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m     \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2509\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2510\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2511\u001b[0;31m           exception_prefix='input')\n\u001b[0m\u001b[1;32m   2512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2513\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    337\u001b[0m                            \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m                            \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m                            'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m           \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected flatten_8_input to have 3 dimensions, but got array with shape (11610, 8)"
          ]
        }
      ]
    }
  ]
}