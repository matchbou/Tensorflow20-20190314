{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Keras Quasi-Recurrent Neural Network-201904100008aa.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matchbou/Tensorflow20-20190314/blob/master/Keras_Quasi_Recurrent_Neural_Network_201904100008aa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "9ZHoroEJoiSJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#https://www.kaggle.com/bkkaggle/keras-quasi-recurrent-neural-network\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vatDfITeoj9U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d88a295b-e4ee-4f0f-9d04-30f2d8bc57e2"
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "\n",
        "from keras.preprocessing import sequence\n",
        "from keras.utils import np_utils\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Embedding, SpatialDropout1D\n",
        "from keras.layers import LSTM, SimpleRNN, GRU\n",
        "from keras.regularizers import l2\n",
        "from keras.constraints import maxnorm\n",
        "from keras.datasets import imdb\n",
        "\n",
        "from keras import optimizers\n",
        "\n",
        "from keras import backend as K\n",
        "from keras import activations, initializers, regularizers, constraints\n",
        "from keras.layers import Layer, InputSpec\n",
        "\n",
        "from keras.utils.conv_utils import conv_output_length\n",
        "\n",
        "def _dropout(x, level, noise_shape=None, seed=None):\n",
        "    x = K.dropout(x, level, noise_shape, seed)\n",
        "    x *= (1. - level) # compensate for the scaling by the dropout\n",
        "    return x\n",
        "\n",
        "class QRNN(Layer):\n",
        "    '''Quasi RNN\n",
        "    # Arguments\n",
        "        units: dimension of the internal projections and the final output.\n",
        "    # References\n",
        "        - [Quasi-recurrent Neural Networks](http://arxiv.org/abs/1611.01576)\n",
        "    '''\n",
        "    def __init__(self, units, window_size=2, stride=1,\n",
        "                 return_sequences=False, go_backwards=False, \n",
        "                 stateful=False, unroll=False, activation='tanh',\n",
        "                 kernel_initializer='uniform', bias_initializer='zero',\n",
        "                 kernel_regularizer=None, bias_regularizer=None,\n",
        "                 activity_regularizer=None,\n",
        "                 kernel_constraint=None, bias_constraint=None, \n",
        "                 dropout=0, use_bias=True, input_dim=None, input_length=None,\n",
        "                 **kwargs):\n",
        "        self.return_sequences = return_sequences\n",
        "        self.go_backwards = go_backwards\n",
        "        self.stateful = stateful\n",
        "        self.unroll = unroll\n",
        "\n",
        "        self.units = units \n",
        "        self.window_size = window_size\n",
        "        self.strides = (stride, 1)\n",
        "\n",
        "        self.use_bias = use_bias\n",
        "        self.activation = activations.get(activation)\n",
        "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
        "        self.bias_initializer = initializers.get(bias_initializer)\n",
        "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
        "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
        "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
        "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
        "        self.bias_constraint = constraints.get(bias_constraint)\n",
        "\n",
        "        self.dropout = dropout\n",
        "        self.supports_masking = True\n",
        "        self.input_spec = [InputSpec(ndim=3)]\n",
        "        self.input_dim = input_dim\n",
        "        self.input_length = input_length\n",
        "        if self.input_dim:\n",
        "            kwargs['input_shape'] = (self.input_length, self.input_dim)\n",
        "        super(QRNN, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        if isinstance(input_shape, list):\n",
        "            input_shape = input_shape[0]\n",
        "\n",
        "        batch_size = input_shape[0] if self.stateful else None\n",
        "        self.input_dim = input_shape[2]\n",
        "        self.input_spec = InputSpec(shape=(batch_size, None, self.input_dim))\n",
        "        self.state_spec = InputSpec(shape=(batch_size, self.units))\n",
        "\n",
        "        self.states = [None]\n",
        "        if self.stateful:\n",
        "            self.reset_states()\n",
        "\n",
        "        kernel_shape = (self.window_size, 1, self.input_dim, self.units * 3)\n",
        "        self.kernel = self.add_weight(name='kernel',\n",
        "                                      shape=kernel_shape,\n",
        "                                      initializer=self.kernel_initializer,\n",
        "                                      regularizer=self.kernel_regularizer,\n",
        "                                      constraint=self.kernel_constraint)\n",
        "        if self.use_bias:\n",
        "            self.bias = self.add_weight(name='bias', \n",
        "                                        shape=(self.units * 3,),\n",
        "                                        initializer=self.bias_initializer,\n",
        "                                        regularizer=self.bias_regularizer,\n",
        "                                        constraint=self.bias_constraint)\n",
        "\n",
        "        self.built = True\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        if isinstance(input_shape, list):\n",
        "            input_shape = input_shape[0]\n",
        "\n",
        "        length = input_shape[1]\n",
        "        if length:\n",
        "            length = conv_output_length(length + self.window_size - 1,\n",
        "                                        self.window_size, 'valid',\n",
        "                                        self.strides[0])\n",
        "        if self.return_sequences:\n",
        "            return (input_shape[0], length, self.units)\n",
        "        else:\n",
        "            return (input_shape[0], self.units)\n",
        "\n",
        "    def compute_mask(self, inputs, mask):\n",
        "        if self.return_sequences:\n",
        "            return mask\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    def get_initial_states(self, inputs):\n",
        "        # build an all-zero tensor of shape (samples, units)\n",
        "        initial_state = K.zeros_like(inputs)  # (samples, timesteps, input_dim)\n",
        "        initial_state = K.sum(initial_state, axis=(1, 2))  # (samples,)\n",
        "        initial_state = K.expand_dims(initial_state)  # (samples, 1)\n",
        "        initial_state = K.tile(initial_state, [1, self.units])  # (samples, units)\n",
        "        initial_states = [initial_state for _ in range(len(self.states))]\n",
        "        return initial_states\n",
        "\n",
        "    def reset_states(self, states=None):\n",
        "        if not self.stateful:\n",
        "            raise AttributeError('Layer must be stateful.')\n",
        "        if not self.input_spec:\n",
        "            raise RuntimeError('Layer has never been called '\n",
        "                               'and thus has no states.')\n",
        "\n",
        "        batch_size = self.input_spec.shape[0]\n",
        "        if not batch_size:\n",
        "            raise ValueError('If a QRNN is stateful, it needs to know '\n",
        "                             'its batch size. Specify the batch size '\n",
        "                             'of your input tensors: \\n'\n",
        "                             '- If using a Sequential model, '\n",
        "                             'specify the batch size by passing '\n",
        "                             'a `batch_input_shape` '\n",
        "                             'argument to your first layer.\\n'\n",
        "                             '- If using the functional API, specify '\n",
        "                             'the time dimension by passing a '\n",
        "                             '`batch_shape` argument to your Input layer.')\n",
        "\n",
        "        if self.states[0] is None:\n",
        "            self.states = [K.zeros((batch_size, self.units))\n",
        "                           for _ in self.states]\n",
        "        elif states is None:\n",
        "            for state in self.states:\n",
        "                K.set_value(state, np.zeros((batch_size, self.units)))\n",
        "        else:\n",
        "            if not isinstance(states, (list, tuple)):\n",
        "                states = [states]\n",
        "            if len(states) != len(self.states):\n",
        "                raise ValueError('Layer ' + self.name + ' expects ' +\n",
        "                                 str(len(self.states)) + ' states, '\n",
        "                                 'but it received ' + str(len(states)) +\n",
        "                                 'state values. Input received: ' +\n",
        "                                 str(states))\n",
        "            for index, (value, state) in enumerate(zip(states, self.states)):\n",
        "                if value.shape != (batch_size, self.units):\n",
        "                    raise ValueError('State ' + str(index) +\n",
        "                                     ' is incompatible with layer ' +\n",
        "                                     self.name + ': expected shape=' +\n",
        "                                     str((batch_size, self.units)) +\n",
        "                                     ', found shape=' + str(value.shape))\n",
        "                K.set_value(state, value)\n",
        "\n",
        "    def __call__(self, inputs, initial_state=None, **kwargs):\n",
        "        # If `initial_state` is specified,\n",
        "        # and if it a Keras tensor,\n",
        "        # then add it to the inputs and temporarily\n",
        "        # modify the input spec to include the state.\n",
        "        if initial_state is not None:\n",
        "            if hasattr(initial_state, '_keras_history'):\n",
        "                # Compute the full input spec, including state\n",
        "                input_spec = self.input_spec\n",
        "                state_spec = self.state_spec\n",
        "                if not isinstance(state_spec, list):\n",
        "                    state_spec = [state_spec]\n",
        "                self.input_spec = [input_spec] + state_spec\n",
        "\n",
        "                # Compute the full inputs, including state\n",
        "                if not isinstance(initial_state, (list, tuple)):\n",
        "                    initial_state = [initial_state]\n",
        "                inputs = [inputs] + list(initial_state)\n",
        "\n",
        "                # Perform the call\n",
        "                output = super(QRNN, self).__call__(inputs, **kwargs)\n",
        "\n",
        "                # Restore original input spec\n",
        "                self.input_spec = input_spec\n",
        "                return output\n",
        "            else:\n",
        "                kwargs['initial_state'] = initial_state\n",
        "        return super(QRNN, self).__call__(inputs, **kwargs)\n",
        "\n",
        "    def call(self, inputs, mask=None, initial_state=None, training=None):\n",
        "        # input shape: `(samples, time (padded with zeros), input_dim)`\n",
        "        # note that the .build() method of subclasses MUST define\n",
        "        # self.input_spec and self.state_spec with complete input shapes.\n",
        "        if isinstance(inputs, list):\n",
        "            initial_states = inputs[1:]\n",
        "            inputs = inputs[0]\n",
        "        elif initial_state is not None:\n",
        "            pass\n",
        "        elif self.stateful:\n",
        "            initial_states = self.states\n",
        "        else:\n",
        "            initial_states = self.get_initial_states(inputs)\n",
        "\n",
        "        if len(initial_states) != len(self.states):\n",
        "            raise ValueError('Layer has ' + str(len(self.states)) +\n",
        "                             ' states but was passed ' +\n",
        "                             str(len(initial_states)) +\n",
        "                             ' initial states.')\n",
        "        input_shape = K.int_shape(inputs)\n",
        "        if self.unroll and input_shape[1] is None:\n",
        "            raise ValueError('Cannot unroll a RNN if the '\n",
        "                             'time dimension is undefined. \\n'\n",
        "                             '- If using a Sequential model, '\n",
        "                             'specify the time dimension by passing '\n",
        "                             'an `input_shape` or `batch_input_shape` '\n",
        "                             'argument to your first layer. If your '\n",
        "                             'first layer is an Embedding, you can '\n",
        "                             'also use the `input_length` argument.\\n'\n",
        "                             '- If using the functional API, specify '\n",
        "                             'the time dimension by passing a `shape` '\n",
        "                             'or `batch_shape` argument to your Input layer.')\n",
        "        constants = self.get_constants(inputs, training=None)\n",
        "        preprocessed_input = self.preprocess_input(inputs, training=None)\n",
        "\n",
        "        last_output, outputs, states = K.rnn(self.step, preprocessed_input,\n",
        "                                            initial_states,\n",
        "                                            go_backwards=self.go_backwards,\n",
        "                                            mask=mask,\n",
        "                                            constants=constants,\n",
        "                                            unroll=self.unroll,\n",
        "                                            input_length=input_shape[1])\n",
        "        if self.stateful:\n",
        "            updates = []\n",
        "            for i in range(len(states)):\n",
        "                updates.append((self.states[i], states[i]))\n",
        "            self.add_update(updates, inputs)\n",
        "\n",
        "        # Properly set learning phase\n",
        "        if 0 < self.dropout < 1:\n",
        "            last_output._uses_learning_phase = True\n",
        "            outputs._uses_learning_phase = True\n",
        "\n",
        "        if self.return_sequences:\n",
        "            return outputs\n",
        "        else:\n",
        "            return last_output\n",
        "\n",
        "    def preprocess_input(self, inputs, training=None):\n",
        "        if self.window_size > 1:\n",
        "            inputs = K.temporal_padding(inputs, (self.window_size-1, 0))\n",
        "        inputs = K.expand_dims(inputs, 2)  # add a dummy dimension\n",
        "\n",
        "        output = K.conv2d(inputs, self.kernel, strides=self.strides,\n",
        "                          padding='valid',\n",
        "                          data_format='channels_last')\n",
        "        output = K.squeeze(output, 2)  # remove the dummy dimension\n",
        "        if self.use_bias:\n",
        "            output = K.bias_add(output, self.bias, data_format='channels_last')\n",
        "\n",
        "        if self.dropout is not None and 0. < self.dropout < 1.:\n",
        "            z = output[:, :, :self.units]\n",
        "            f = output[:, :, self.units:2 * self.units]\n",
        "            o = output[:, :, 2 * self.units:]\n",
        "            f = K.in_train_phase(1 - _dropout(1 - f, self.dropout), f, training=training)\n",
        "            return K.concatenate([z, f, o], -1)\n",
        "        else:\n",
        "            return output\n",
        "\n",
        "    def step(self, inputs, states):\n",
        "        prev_output = states[0]\n",
        "\n",
        "        z = inputs[:, :self.units]\n",
        "        f = inputs[:, self.units:2 * self.units]\n",
        "        o = inputs[:, 2 * self.units:]\n",
        "\n",
        "        z = self.activation(z)\n",
        "        f = f if self.dropout is not None and 0. < self.dropout < 1. else K.sigmoid(f)\n",
        "        o = K.sigmoid(o)\n",
        "\n",
        "        output = f * prev_output + (1 - f) * z\n",
        "        output = o * output\n",
        "\n",
        "        return output, [output]\n",
        "\n",
        "    def get_constants(self, inputs, training=None):\n",
        "        return []\n",
        " \n",
        "    def get_config(self):\n",
        "        config = {'units': self.units,\n",
        "                  'window_size': self.window_size,\n",
        "                  'stride': self.strides[0],\n",
        "                  'return_sequences': self.return_sequences,\n",
        "                  'go_backwards': self.go_backwards,\n",
        "                  'stateful': self.stateful,\n",
        "                  'unroll': self.unroll,\n",
        "                  'use_bias': self.use_bias,\n",
        "                  'dropout': self.dropout,\n",
        "                  'activation': activations.serialize(self.activation),\n",
        "                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n",
        "                  'bias_initializer': initializers.serialize(self.bias_initializer),\n",
        "                  'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n",
        "                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n",
        "                  'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n",
        "                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n",
        "                  'bias_constraint': constraints.serialize(self.bias_constraint),\n",
        "                  'input_dim': self.input_dim,\n",
        "                  'input_length': self.input_length}\n",
        "        base_config = super(QRNN, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "wWF0BmQboyks",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "outputId": "a7ade5c2-ecdd-4a00-e80d-c1e1b86eda25"
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "\n",
        "max_features = 20000\n",
        "maxlen = 80  # cut texts after this number of words (among top max_features most common words)\n",
        "batch_size = 32 \n",
        "\n",
        "print('Build model...')\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, 128))\n",
        "model.add(SpatialDropout1D(0.2))\n",
        "model.add(QRNN(128, window_size=3, dropout=0.2, \n",
        "               kernel_regularizer=l2(1e-4), bias_regularizer=l2(1e-4), \n",
        "               kernel_constraint=maxnorm(10), bias_constraint=maxnorm(10)))\n",
        "model.add(Dense(1))\n",
        "model.add(Activation('sigmoid'))\n",
        "\n",
        "# try using different optimizers and different optimizer configs\n",
        "opt = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print('Loading data...')\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "print(len(X_train), 'train sequences')\n",
        "print(len(X_test), 'test sequences')\n",
        "\n",
        "print('Pad sequences (samples x time)')\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
        "print('X_train shape:', X_train.shape)\n",
        "print('X_test shape:', X_test.shape)\n",
        "\n",
        "print('Train...')\n",
        "model.fit(X_train, y_train, batch_size=batch_size, epochs=7,\n",
        "          validation_data=(X_test, y_test))\n",
        "score, acc = model.evaluate(X_test, y_test,\n",
        "                            batch_size=batch_size)\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Build model...\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "Loading data...\n",
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 1s 0us/step\n",
            "25000 train sequences\n",
            "25000 test sequences\n",
            "Pad sequences (samples x time)\n",
            "X_train shape: (25000, 80)\n",
            "X_test shape: (25000, 80)\n",
            "Train...\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/7\n",
            "25000/25000 [==============================] - 61s 2ms/step - loss: 0.6246 - acc: 0.6385 - val_loss: 0.6077 - val_acc: 0.6577\n",
            "Epoch 2/7\n",
            " 6208/25000 [======>.......................] - ETA: 36s - loss: 0.5020 - acc: 0.7677"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}