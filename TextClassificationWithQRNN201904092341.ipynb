{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextClassificationWithQRNN201904092341.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matchbou/Tensorflow20-20190314/blob/master/TextClassificationWithQRNN201904092341.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "N1qSy2QLQ1Uz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#https://www.kaggle.com/kmader/text-classification-with-qrnn/data\n",
        "#Text Classification with QRNN\n",
        "\n",
        "#Refference \n",
        "#https://qiita.com/katsu1110/items/a8d508a1b6f07bd3a243\n",
        "#Google Colab上でKaggleのデータをロード、モデル訓練、提出の全てを行う\n",
        "#https://www.st-hakky-blog.com/entry/2018/05/19/133656\n",
        "#Kaggle APIのforbidden対策\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jpi_g4dLQ5C_",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 96
        },
        "outputId": "754337b7-029d-4746-aca4-7ded2be0dd21"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ad8143e2-2c9f-4dfc-9964-33f928d98557\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-ad8143e2-2c9f-4dfc-9964-33f928d98557\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle (1).json\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"matchbou\",\"key\":\"a19feac7cab97defa8efb7b6d539ead0\"}'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "metadata": {
        "id": "lG26RB9JRFMg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iWfI39TXRK-U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "a1023a2c-62b0-4d58-af00-5bd5e27b8213"
      },
      "cell_type": "code",
      "source": [
        "!pip install kaggle"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.6/dist-packages (1.5.3)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.22)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2019.3.9)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.5.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.18.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.28.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (3.0.2)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (2.6)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: text-unidecode==1.2 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle) (1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eHb14yFvRO0Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!chmod 600 /root/.kaggle/kaggle.json\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dsLQKMSjSkEu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!kaggle competitions download -c jigsaw-toxic-comment-classification-challenge"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YHlLzFhMStth",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir ./input"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2o0rTnNvTXAo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!unzip train.csv.zip -d ./input"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sGYuIvsHTZXv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!unzip test.csv.zip -d ./input"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m4pEWdIITeqy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir ./notebook "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rehNdORjRPnn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "24688188-3e5b-4f1d-e6a4-4d2c8b2cad28"
      },
      "cell_type": "code",
      "source": [
        "!ls -l"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 53916\n",
            "drwxr-xr-x 2 root root     4096 Apr  9 13:17 input\n",
            "-rw-r--r-- 1 root root       64 Apr  9 13:04 kaggle.json\n",
            "drwxr-xr-x 2 root root     4096 Apr  9 13:18 notebook\n",
            "drwxr-xr-x 1 root root     4096 Apr  4 20:20 sample_data\n",
            "-rw-r--r-- 1 root root  1459715 Apr  9 13:09 sample_submission.csv.zip\n",
            "-rw-r--r-- 1 root root 24577258 Apr  9 13:09 test.csv.zip\n",
            "-rw-r--r-- 1 root root  1527605 Apr  9 13:09 test_labels.csv.zip\n",
            "-rw-r--r-- 1 root root 27619914 Apr  9 13:09 train.csv.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "U62KLQTyR6rt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6FsXiIAfSWwK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "os.chdir('notebook')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nndeVccmSbXy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "09af6260-99d2-469b-c542-6acbe638e3e3"
      },
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/notebook\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OQd1wgp_SeOi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "30132073-d93e-4263-85c4-9d31bc01f297"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Embedding, Input\n",
        "from keras.layers import Conv1D, GlobalMaxPool1D, Dropout, concatenate\n",
        "from keras.preprocessing import text, sequence\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "NrxQmNscS2XZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras import backend as K\n",
        "from keras import activations, initializers, regularizers, constraints\n",
        "from keras.layers import Layer, InputSpec\n",
        "from keras.utils.conv_utils import conv_output_length\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o2sHUYppS39Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def _dropout(x, level, noise_shape=None, seed=None):\n",
        "    x = K.dropout(x, level, noise_shape, seed)\n",
        "    x *= (1. - level) # compensate for the scaling by the dropout\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pAqiA7VyTDW2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class QRNN(Layer):\n",
        "    '''Quasi RNN\n",
        "    # Arguments\n",
        "        units: dimension of the internal projections and the final output.\n",
        "    # References\n",
        "        - [Quasi-recurrent Neural Networks](http://arxiv.org/abs/1611.01576)\n",
        "    '''\n",
        "    def __init__(self, units, window_size=2, stride=1,\n",
        "                 return_sequences=False, go_backwards=False, \n",
        "                 stateful=False, unroll=False, activation='tanh',\n",
        "                 kernel_initializer='uniform', bias_initializer='zero',\n",
        "                 kernel_regularizer=None, bias_regularizer=None,\n",
        "                 activity_regularizer=None,\n",
        "                 kernel_constraint=None, bias_constraint=None, \n",
        "                 dropout=0, use_bias=True, input_dim=None, input_length=None,\n",
        "                 **kwargs):\n",
        "        self.return_sequences = return_sequences\n",
        "        self.go_backwards = go_backwards\n",
        "        self.stateful = stateful\n",
        "        self.unroll = unroll\n",
        "\n",
        "        self.units = units \n",
        "        self.window_size = window_size\n",
        "        self.strides = (stride, 1)\n",
        "\n",
        "        self.use_bias = use_bias\n",
        "        self.activation = activations.get(activation)\n",
        "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
        "        self.bias_initializer = initializers.get(bias_initializer)\n",
        "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
        "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
        "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
        "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
        "        self.bias_constraint = constraints.get(bias_constraint)\n",
        "\n",
        "        self.dropout = dropout\n",
        "        self.supports_masking = True\n",
        "        self.input_spec = [InputSpec(ndim=3)]\n",
        "        self.input_dim = input_dim\n",
        "        self.input_length = input_length\n",
        "        if self.input_dim:\n",
        "            kwargs['input_shape'] = (self.input_length, self.input_dim)\n",
        "        super(QRNN, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        if isinstance(input_shape, list):\n",
        "            input_shape = input_shape[0]\n",
        "\n",
        "        batch_size = input_shape[0] if self.stateful else None\n",
        "        self.input_dim = input_shape[2]\n",
        "        self.input_spec = InputSpec(shape=(batch_size, None, self.input_dim))\n",
        "        self.state_spec = InputSpec(shape=(batch_size, self.units))\n",
        "\n",
        "        self.states = [None]\n",
        "        if self.stateful:\n",
        "            self.reset_states()\n",
        "\n",
        "        kernel_shape = (self.window_size, 1, self.input_dim, self.units * 3)\n",
        "        self.kernel = self.add_weight(name='kernel',\n",
        "                                      shape=kernel_shape,\n",
        "                                      initializer=self.kernel_initializer,\n",
        "                                      regularizer=self.kernel_regularizer,\n",
        "                                      constraint=self.kernel_constraint)\n",
        "        if self.use_bias:\n",
        "            self.bias = self.add_weight(name='bias', \n",
        "                                        shape=(self.units * 3,),\n",
        "                                        initializer=self.bias_initializer,\n",
        "                                        regularizer=self.bias_regularizer,\n",
        "                                        constraint=self.bias_constraint)\n",
        "\n",
        "        self.built = True\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        if isinstance(input_shape, list):\n",
        "            input_shape = input_shape[0]\n",
        "\n",
        "        length = input_shape[1]\n",
        "        if length:\n",
        "            length = conv_output_length(length + self.window_size - 1,\n",
        "                                        self.window_size, 'valid',\n",
        "                                        self.strides[0])\n",
        "        if self.return_sequences:\n",
        "            return (input_shape[0], length, self.units)\n",
        "        else:\n",
        "            return (input_shape[0], self.units)\n",
        "\n",
        "    def compute_mask(self, inputs, mask):\n",
        "        if self.return_sequences:\n",
        "            return mask\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    def get_initial_states(self, inputs):\n",
        "        # build an all-zero tensor of shape (samples, units)\n",
        "        initial_state = K.zeros_like(inputs)  # (samples, timesteps, input_dim)\n",
        "        initial_state = K.sum(initial_state, axis=(1, 2))  # (samples,)\n",
        "        initial_state = K.expand_dims(initial_state)  # (samples, 1)\n",
        "        initial_state = K.tile(initial_state, [1, self.units])  # (samples, units)\n",
        "        initial_states = [initial_state for _ in range(len(self.states))]\n",
        "        return initial_states\n",
        "\n",
        "    def reset_states(self, states=None):\n",
        "        if not self.stateful:\n",
        "            raise AttributeError('Layer must be stateful.')\n",
        "        if not self.input_spec:\n",
        "            raise RuntimeError('Layer has never been called '\n",
        "                               'and thus has no states.')\n",
        "\n",
        "        batch_size = self.input_spec.shape[0]\n",
        "        if not batch_size:\n",
        "            raise ValueError('If a QRNN is stateful, it needs to know '\n",
        "                             'its batch size. Specify the batch size '\n",
        "                             'of your input tensors: \\n'\n",
        "                             '- If using a Sequential model, '\n",
        "                             'specify the batch size by passing '\n",
        "                             'a `batch_input_shape` '\n",
        "                             'argument to your first layer.\\n'\n",
        "                             '- If using the functional API, specify '\n",
        "                             'the time dimension by passing a '\n",
        "                             '`batch_shape` argument to your Input layer.')\n",
        "\n",
        "        if self.states[0] is None:\n",
        "            self.states = [K.zeros((batch_size, self.units))\n",
        "                           for _ in self.states]\n",
        "        elif states is None:\n",
        "            for state in self.states:\n",
        "                K.set_value(state, np.zeros((batch_size, self.units)))\n",
        "        else:\n",
        "            if not isinstance(states, (list, tuple)):\n",
        "                states = [states]\n",
        "            if len(states) != len(self.states):\n",
        "                raise ValueError('Layer ' + self.name + ' expects ' +\n",
        "                                 str(len(self.states)) + ' states, '\n",
        "                                 'but it received ' + str(len(states)) +\n",
        "                                 'state values. Input received: ' +\n",
        "                                 str(states))\n",
        "            for index, (value, state) in enumerate(zip(states, self.states)):\n",
        "                if value.shape != (batch_size, self.units):\n",
        "                    raise ValueError('State ' + str(index) +\n",
        "                                     ' is incompatible with layer ' +\n",
        "                                     self.name + ': expected shape=' +\n",
        "                                     str((batch_size, self.units)) +\n",
        "                                     ', found shape=' + str(value.shape))\n",
        "                K.set_value(state, value)\n",
        "\n",
        "    def __call__(self, inputs, initial_state=None, **kwargs):\n",
        "        # If `initial_state` is specified,\n",
        "        # and if it a Keras tensor,\n",
        "        # then add it to the inputs and temporarily\n",
        "        # modify the input spec to include the state.\n",
        "        if initial_state is not None:\n",
        "            if hasattr(initial_state, '_keras_history'):\n",
        "                # Compute the full input spec, including state\n",
        "                input_spec = self.input_spec\n",
        "                state_spec = self.state_spec\n",
        "                if not isinstance(state_spec, list):\n",
        "                    state_spec = [state_spec]\n",
        "                self.input_spec = [input_spec] + state_spec\n",
        "\n",
        "                # Compute the full inputs, including state\n",
        "                if not isinstance(initial_state, (list, tuple)):\n",
        "                    initial_state = [initial_state]\n",
        "                inputs = [inputs] + list(initial_state)\n",
        "\n",
        "                # Perform the call\n",
        "                output = super(QRNN, self).__call__(inputs, **kwargs)\n",
        "\n",
        "                # Restore original input spec\n",
        "                self.input_spec = input_spec\n",
        "                return output\n",
        "            else:\n",
        "                kwargs['initial_state'] = initial_state\n",
        "        return super(QRNN, self).__call__(inputs, **kwargs)\n",
        "\n",
        "    def call(self, inputs, mask=None, initial_state=None, training=None):\n",
        "        # input shape: `(samples, time (padded with zeros), input_dim)`\n",
        "        # note that the .build() method of subclasses MUST define\n",
        "        # self.input_spec and self.state_spec with complete input shapes.\n",
        "        if isinstance(inputs, list):\n",
        "            initial_states = inputs[1:]\n",
        "            inputs = inputs[0]\n",
        "        elif initial_state is not None:\n",
        "            pass\n",
        "        elif self.stateful:\n",
        "            initial_states = self.states\n",
        "        else:\n",
        "            initial_states = self.get_initial_states(inputs)\n",
        "\n",
        "        if len(initial_states) != len(self.states):\n",
        "            raise ValueError('Layer has ' + str(len(self.states)) +\n",
        "                             ' states but was passed ' +\n",
        "                             str(len(initial_states)) +\n",
        "                             ' initial states.')\n",
        "        input_shape = K.int_shape(inputs)\n",
        "        if self.unroll and input_shape[1] is None:\n",
        "            raise ValueError('Cannot unroll a RNN if the '\n",
        "                             'time dimension is undefined. \\n'\n",
        "                             '- If using a Sequential model, '\n",
        "                             'specify the time dimension by passing '\n",
        "                             'an `input_shape` or `batch_input_shape` '\n",
        "                             'argument to your first layer. If your '\n",
        "                             'first layer is an Embedding, you can '\n",
        "                             'also use the `input_length` argument.\\n'\n",
        "                             '- If using the functional API, specify '\n",
        "                             'the time dimension by passing a `shape` '\n",
        "                             'or `batch_shape` argument to your Input layer.')\n",
        "        constants = self.get_constants(inputs, training=None)\n",
        "        preprocessed_input = self.preprocess_input(inputs, training=None)\n",
        "\n",
        "        last_output, outputs, states = K.rnn(self.step, preprocessed_input,\n",
        "                                            initial_states,\n",
        "                                            go_backwards=self.go_backwards,\n",
        "                                            mask=mask,\n",
        "                                            constants=constants,\n",
        "                                            unroll=self.unroll,\n",
        "                                            input_length=input_shape[1])\n",
        "        if self.stateful:\n",
        "            updates = []\n",
        "            for i in range(len(states)):\n",
        "                updates.append((self.states[i], states[i]))\n",
        "            self.add_update(updates, inputs)\n",
        "\n",
        "        # Properly set learning phase\n",
        "        if 0 < self.dropout < 1:\n",
        "            last_output._uses_learning_phase = True\n",
        "            outputs._uses_learning_phase = True\n",
        "\n",
        "        if self.return_sequences:\n",
        "            return outputs\n",
        "        else:\n",
        "            return last_output\n",
        "\n",
        "    def preprocess_input(self, inputs, training=None):\n",
        "        if self.window_size > 1:\n",
        "            inputs = K.temporal_padding(inputs, (self.window_size-1, 0))\n",
        "        inputs = K.expand_dims(inputs, 2)  # add a dummy dimension\n",
        "\n",
        "        output = K.conv2d(inputs, self.kernel, strides=self.strides,\n",
        "                          padding='valid',\n",
        "                          data_format='channels_last')\n",
        "        output = K.squeeze(output, 2)  # remove the dummy dimension\n",
        "        if self.use_bias:\n",
        "            output = K.bias_add(output, self.bias, data_format='channels_last')\n",
        "\n",
        "        if self.dropout is not None and 0. < self.dropout < 1.:\n",
        "            z = output[:, :, :self.units]\n",
        "            f = output[:, :, self.units:2 * self.units]\n",
        "            o = output[:, :, 2 * self.units:]\n",
        "            f = K.in_train_phase(1 - _dropout(1 - f, self.dropout), f, training=training)\n",
        "            return K.concatenate([z, f, o], -1)\n",
        "        else:\n",
        "            return output\n",
        "\n",
        "    def step(self, inputs, states):\n",
        "        prev_output = states[0]\n",
        "\n",
        "        z = inputs[:, :self.units]\n",
        "        f = inputs[:, self.units:2 * self.units]\n",
        "        o = inputs[:, 2 * self.units:]\n",
        "\n",
        "        z = self.activation(z)\n",
        "        f = f if self.dropout is not None and 0. < self.dropout < 1. else K.sigmoid(f)\n",
        "        o = K.sigmoid(o)\n",
        "\n",
        "        output = f * prev_output + (1 - f) * z\n",
        "        output = o * output\n",
        "\n",
        "        return output, [output]\n",
        "\n",
        "    def get_constants(self, inputs, training=None):\n",
        "        return []\n",
        " \n",
        "    def get_config(self):\n",
        "        config = {'units': self.units,\n",
        "                  'window_size': self.window_size,\n",
        "                  'stride': self.strides[0],\n",
        "                  'return_sequences': self.return_sequences,\n",
        "                  'go_backwards': self.go_backwards,\n",
        "                  'stateful': self.stateful,\n",
        "                  'unroll': self.unroll,\n",
        "                  'use_bias': self.use_bias,\n",
        "                  'dropout': self.dropout,\n",
        "                  'activation': activations.serialize(self.activation),\n",
        "                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n",
        "                  'bias_initializer': initializers.serialize(self.bias_initializer),\n",
        "                  'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n",
        "                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n",
        "                  'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n",
        "                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n",
        "                  'bias_constraint': constraints.serialize(self.bias_constraint),\n",
        "                  'input_dim': self.input_dim,\n",
        "                  'input_length': self.input_length}\n",
        "        base_config = super(QRNN, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ohyx0aDPTMlC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# define network parameters\n",
        "max_features = 20000\n",
        "maxlen = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pyTRGs94TwRU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train = pd.read_csv(\"../input/train.csv\")\n",
        "test = pd.read_csv(\"../input/test.csv\")\n",
        "train = train.sample(frac=1)\n",
        "\n",
        "list_sentences_train = train[\"comment_text\"].fillna(\"Invalid\").values\n",
        "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
        "y = train[list_classes].values\n",
        "list_sentences_test = test[\"comment_text\"].fillna(\"Invalid\").values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PBRQdUssTz5F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tokenizer = text.Tokenizer(num_words=max_features)\n",
        "tokenizer.fit_on_texts(list(list_sentences_train))\n",
        "# train data\n",
        "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
        "X_t = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
        "# test data\n",
        "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
        "X_te = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cyQAa3S0UCfI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 818
        },
        "outputId": "123ed5b4-af6e-4adf-d0f1-ffd1b4a769aa"
      },
      "cell_type": "code",
      "source": [
        "def build_model(conv_layers = 2, max_dilation_rate = 3):\n",
        "    embed_size = 128\n",
        "    inp = Input(shape=(maxlen, ))\n",
        "    x = Embedding(max_features, embed_size)(inp)\n",
        "    x = Dropout(0.25)(x)\n",
        "    x = Conv1D(2*embed_size, \n",
        "                   kernel_size = 3)(x)\n",
        "    prefilt_x = Conv1D(2*embed_size, \n",
        "                   kernel_size = 3)(x)\n",
        "    out_conv = []\n",
        "    x = prefilt_x\n",
        "    # strides rate we use here for skip\n",
        "    for strides in [1, 1, 2]:\n",
        "        x = QRNN(128*2**(strides), \n",
        "                 return_sequences = True, \n",
        "                 stride = strides,\n",
        "                dropout = 0.2)(x)\n",
        "    x_f = QRNN(512)(x)  \n",
        "    x_b = QRNN(512, go_backwards=True)(x)\n",
        "    x = concatenate([x_f, x_b])\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(64, activation=\"relu\")(x)\n",
        "    x = Dropout(0.1)(x)\n",
        "    x = Dense(6, activation=\"sigmoid\")(x)\n",
        "    model = Model(inputs=inp, outputs=x)\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['binary_accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "model = build_model()\n",
        "model.summary()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 100)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, 100, 128)     2560000     input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 100, 128)     0           embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 98, 256)      98560       dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 96, 256)      196864      conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "qrnn_1 (QRNN)                   (None, 96, 256)      393984      conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "qrnn_2 (QRNN)                   (None, 96, 256)      393984      qrnn_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "qrnn_3 (QRNN)                   (None, 48, 512)      787968      qrnn_2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "qrnn_4 (QRNN)                   (None, 512)          1574400     qrnn_3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "qrnn_5 (QRNN)                   (None, 512)          1574400     qrnn_3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 1024)         0           qrnn_4[0][0]                     \n",
            "                                                                 qrnn_5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 1024)         0           concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 64)           65600       dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 64)           0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 6)            390         dropout_3[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 7,646,150\n",
            "Trainable params: 7,646,150\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Y1QYIrPCULsK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532
        },
        "outputId": "17bbd43c-1c0e-4ac3-bccc-3d0bc4fea851"
      },
      "cell_type": "code",
      "source": [
        "#batch_size = 1024 # big beefy GPUs like large batches\n",
        "#epochs = 12\n",
        "batch_size = 512 # big beefy GPUs like large batches\n",
        "epochs = 6\n",
        "\n",
        "file_path=\"weights.hdf5\"\n",
        "checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5)\n",
        "\n",
        "callbacks_list = [checkpoint, early] #early\n",
        "model.fit(X_t, y, \n",
        "          batch_size=batch_size, \n",
        "          epochs=epochs, \n",
        "          shuffle = True,\n",
        "          validation_split=0.25, \n",
        "          callbacks=callbacks_list)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 119678 samples, validate on 39893 samples\n",
            "Epoch 1/6\n",
            "119678/119678 [==============================] - 287s 2ms/step - loss: 0.6295 - binary_accuracy: 0.9608 - val_loss: 0.5930 - val_binary_accuracy: 0.9640\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.59297, saving model to weights.hdf5\n",
            "Epoch 2/6\n",
            "119678/119678 [==============================] - 277s 2ms/step - loss: 0.5511 - binary_accuracy: 0.9631 - val_loss: 0.5111 - val_binary_accuracy: 0.9640\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.59297 to 0.51114, saving model to weights.hdf5\n",
            "Epoch 3/6\n",
            "119678/119678 [==============================] - 277s 2ms/step - loss: 0.4775 - binary_accuracy: 0.9631 - val_loss: 0.4447 - val_binary_accuracy: 0.9640\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.51114 to 0.44473, saving model to weights.hdf5\n",
            "Epoch 4/6\n",
            "119678/119678 [==============================] - 278s 2ms/step - loss: 0.4177 - binary_accuracy: 0.9631 - val_loss: 0.3909 - val_binary_accuracy: 0.9640\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.44473 to 0.39085, saving model to weights.hdf5\n",
            "Epoch 5/6\n",
            "119678/119678 [==============================] - 277s 2ms/step - loss: 0.3692 - binary_accuracy: 0.9631 - val_loss: 0.3471 - val_binary_accuracy: 0.9640\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.39085 to 0.34708, saving model to weights.hdf5\n",
            "Epoch 6/6\n",
            "119678/119678 [==============================] - 278s 2ms/step - loss: 0.3297 - binary_accuracy: 0.9631 - val_loss: 0.3114 - val_binary_accuracy: 0.9640\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.34708 to 0.31144, saving model to weights.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f14154d09b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "metadata": {
        "id": "VZV_bRjmUa34",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "364c6b08-66fe-4e94-9379-9c5c5cbaee5d"
      },
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown, display\n",
        "dmd = lambda x: display(Markdown(x))\n",
        "def show_sentence(sent_idx):\n",
        "    dmd('# Input Sentence:\\n `{}`'.format(list_sentences_train[sent_idx]))\n",
        "    c_pred = model.predict(X_t[sent_idx:sent_idx+1])[0]\n",
        "    dmd('## Positive Categories')\n",
        "    for k, v, p in zip(list_classes, y[sent_idx], c_pred):\n",
        "        if v>0:\n",
        "            dmd('- {}, Prediction: {:2.2f}%'.format(k, 100*v, 100*p))\n",
        "    dmd('## Negative Categories')\n",
        "    for k, v, p in zip(list_classes, y[sent_idx], c_pred):\n",
        "        if v<1:\n",
        "            dmd('- {}, Prediction: {:2.2f}%'.format(k, 100*p))\n",
        "show_sentence(0)\n",
        "show_sentence(50)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "# Input Sentence:\n `You were giving him a week's timeframe for a reply. In that situation, it is reasonable to go ahead with the edit without waiting for you. I agree that as a general courtesy, one should not edit while the issue is being discussed. But this is not necessary. Edits can also be made while the discussion goes on. This is particularly true when one party seems only interested in preventing others in getting anything into an article. If one party is rejecting the other users proposals without having any credible reasons, or in a hypocritical way, the edit can go ahead.-`",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "## Positive Categories",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "## Negative Categories",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "- toxic, Prediction: 24.40%",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "- severe_toxic, Prediction: 23.36%",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "- obscene, Prediction: 23.83%",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "- threat, Prediction: 23.27%",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "- insult, Prediction: 23.77%",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "- identity_hate, Prediction: 23.34%",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "# Input Sentence:\n `\"  Also, if you want to verify for yourself what they say, try using the search strings visible in the previews (despite the fact that the pages are not) in Google.  Somehow, going to the site from google allows you to see the pages.  I had to do that to verify the Age of Reason quote.  My search string in that case was, \"\"Giulio Cesare Vanini strangle age of reason\"\" which produced the source in question as result #1.\"`",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "## Positive Categories",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "## Negative Categories",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "- toxic, Prediction: 24.40%",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "- severe_toxic, Prediction: 23.36%",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "- obscene, Prediction: 23.83%",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "- threat, Prediction: 23.27%",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "- insult, Prediction: 23.77%",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "- identity_hate, Prediction: 23.34%",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "IwAk1AZvUcYt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Input Sentence:\n",
        "\n",
        "Sure, that couldn't hurt. I've actually tested it on Mac/Firefox and Mac/Safari (albeit an earlier version of Firefox) and it's worked for me. So I think it's less likely a straight-up browser/OS issue, and more likely an interaction with either the monobook.js or perhaps another piece of Javascript code that may be running on your client. But that's just a guess. If it behaves differently for you with Safari, please let me know. '''''' Talk"
      ]
    },
    {
      "metadata": {
        "id": "rRG8sH1FUgr9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3672cbe1-eb56-433c-b6fb-e6609d1bb361"
      },
      "cell_type": "code",
      "source": [
        "#Make Predictions\n",
        "#Load the model and make predictions on the test dataset\n",
        "\n",
        "model.load_weights(file_path)\n",
        "y_test = model.predict(X_te, verbose = True, batch_size = 1024)\n",
        "sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\n",
        "sample_submission[list_classes] = y_test\n",
        "sample_submission.to_csv(\"predictions.csv\", \n",
        "                         index=False)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "153164/153164 [==============================] - 105s 685us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "t8hn0ZCLiElK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d79b4fe2-a5f9-4cf4-cd90-c452f82c1fc2"
      },
      "cell_type": "code",
      "source": [
        "!ls -a\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".  ..  predictions.csv\tweights.hdf5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "D4xDoJVRicIM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('predictions.csv') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Sc69D-7-iw-D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "outputId": "6d7bea60-8720-4949-b65c-93549ade260c"
      },
      "cell_type": "code",
      "source": [
        "files.download('weights.hdf5')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------\n",
            "Exception happened during processing of request from ('::ffff:127.0.0.1', 48624, 0, 0)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/socketserver.py\", line 317, in _handle_request_noblock\n",
            "    self.process_request(request, client_address)\n",
            "  File \"/usr/lib/python3.6/socketserver.py\", line 348, in process_request\n",
            "    self.finish_request(request, client_address)\n",
            "  File \"/usr/lib/python3.6/socketserver.py\", line 361, in finish_request\n",
            "    self.RequestHandlerClass(request, client_address, self)\n",
            "  File \"/usr/lib/python3.6/socketserver.py\", line 721, in __init__\n",
            "    self.handle()\n",
            "  File \"/usr/lib/python3.6/http/server.py\", line 418, in handle\n",
            "    self.handle_one_request()\n",
            "  File \"/usr/lib/python3.6/http/server.py\", line 406, in handle_one_request\n",
            "    method()\n",
            "  File \"/usr/lib/python3.6/http/server.py\", line 639, in do_GET\n",
            "    self.copyfile(f, self.wfile)\n",
            "  File \"/usr/lib/python3.6/http/server.py\", line 800, in copyfile\n",
            "    shutil.copyfileobj(source, outputfile)\n",
            "  File \"/usr/lib/python3.6/shutil.py\", line 82, in copyfileobj\n",
            "    fdst.write(buf)\n",
            "  File \"/usr/lib/python3.6/socketserver.py\", line 800, in write\n",
            "    self._sock.sendall(b)\n",
            "ConnectionResetError: [Errno 104] Connection reset by peer\n",
            "----------------------------------------\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "q5HV0oeOi5JP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}